---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
      pdf-separate-fragments: true
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(gridExtra)
library(qrcode)
```

<h1>Lecture 10.2: Kaplan-Meier estimator & logrank tests</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>November 13, 2025</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=100 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/F25/lec-10-2/slides-10-2>

:::

::: {.absolute bottom=50 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/F25/lec-10-2/slides-10-2"))
```
:::

---

## Roadmap

- Recap from last class

- Estimating $S(t)$ without a probability distribution (non-parametrically)

   - When we don't have censoring
   
   - When we do have censoring
   
      - Strategy 1: Uniform censoring adjustment with life tables
   
      - Strategy 2: Kaplan-Meier estimator

- Confidence intervals for $S(t)$
   
- Survival curves

- Logrank tests: testing difference between survival curves

---

## Recap: Time-to-event data

:::: {.columns}
::: {.column width="50%"}
Last time we introduced the idea of [time-to-event data]{.alert}, which is characterized by:

- Whether an individual experience an [event]{.alert}

- If yes, the [time]{.alert} that event happened

- If no, how long were they observed for

- The presence of [censoring]{.alert}

<br>

We also discussed how to measure time to an event:

- Time origin: time to event from when the "research question" clock starts

- Entry to study: time to event from when someone first enters your study
:::

::: {.column width="50%"}
![](timeOrigin.png){width=100%}
:::
::::

---

## Recap: Describing time-to-event data

We also talked about the ways we can describe survival time

- The [survival function]{.alert} $S(t) = P(T>t)$: the probability that a event happens after time $t$

- The [hazard function]{.alert} $\lambda(t) = \lambda(t) = \lim_{\delta \rightarrow 0} \frac{P(t\le T < t + \delta | T \ge t)}{\delta}$: the instantaneous rate of an event happening

- The [cumulative hazard function]{.alert} $\Lambda(t) = \int_0^t \lambda(u) du$: the rate of event occurrence over a period of time

![](surv-fnc.png){width=800}

---

## Recap: Describing time-to-event data

:::: {.columns}

::: {.column width="50%"}
We noted that we could assume that the survival time $T$ follows a [probability distribution]{.alert}, and use that assumption to calculate $S(t)$, $\lambda(t)$, and $\Lambda(t)$

- We specifically looked at the exponential and Weibull distributions
:::

::: {.column width="25%"}

![](exponential.png){width=700}

:::

::: {.column width="25%"}

![](weibull.png){width=500}

:::


::::

However, if our assumption about the distribution is [incorrect]{.alert}, this can lead to a [biased and inconsistent]{.alert} estimate of $S(t)$

- This is bad because $S(t)$ is the main things we're trying to estimate in survival analysis!

Because of this, we want to try to estimate $S(t)$ in a [non-parametric]{.alert} way...

---

## Estimating $S(t)$

Using this data, how would we estimate $S(t) = P(T>t)$ at t=1 and t=2?

:::: {.columns}
::: {.column width="50%"}
![](estimating-S.png){width=1000px}
:::

::: {.column width="50%"}

$P(T>1) = \frac{\text{# alive at 1 year}}{\text{# "at risk"}}=$[$8/10=0.8$]{.fragment fragment-index=1}

::: {.fragment fragment-index=1}
What about $P(T>2)$?

- We could use the "full" sample and look at $\frac{\text{# with no event at 2 year}}{\text{# "at risk" since time 0}}$
:::

::: {.fragment fragment-index=1}
- Or we could use a "reduced" sample that only looks at $\frac{\text{# alive past 2 year}}{\text{# with complete data at 2 years}}$
:::

::: {.fragment fragment-index=2}
|  | P(T>1) | P(T>2) |  |
|---------|-----|----|----------|
| "Full" sample | 8/10 = 0.8 | 6/10 = 0.6 | (too high) |
| "Reduced" sample | 8/10 = 0.8  | 4/8 = 0.5 | (too variable) |
:::
:::
::::

::: {.fragment fragment-index=2 .absolute width=50% bottom=100 left=-10}
The "full" sample takes anyone who was censored and [carries their last observation forward]{.alert}

The "reduced" sample [ignores]{.alert} anyone who dropped out
:::

---

## Estimating $S(t)$
:::: {.columns}
::: {.column width="50%"}
![](estimating-S.png){width=1000px}
:::

::: {.column width="50%"}
Alternatively, we could also use conditional probability!

$$\begin{align*}P(T>2) &= P(T>2 | T>1) \times P(T>1)\\
&=\frac{\text{# alive past 2 years}}{\text{# observed > 2 years, alive at 1 year}}\\
&~~~~~~~\times \frac{\text{# alive at 1 year}}{\text{# observed at 1 year}}\\
&= \frac{4}{6} \times \frac{8}{10}\\
&= 0.53\end{align*}$$
:::
::::

::: {.absolute width=75% bottom=75 left=0}
Compared to:

|  | P(T>1) | P(T>2) |  |
|---------|-----|----|----------|
| "Full" sample | 8/10 = 0.8 | 6/10 = 0.6 | (too high) |
| "Reduced" sample | 8/10 = 0.8  | 4/8 = 0.5 | (too variable) |
:::

---

## Life table estimates

But what happens when we have [censoring]{.alert}?

Consider some data from the 6-MP leukemia trial we discussed in lecture 10.1

- The essential data are the 21 ordered times (in months) to relapse or [censoring]{.underline} (time with "+" next to it): 6, 6, 6, 7, 10, 13, 16, 22, 23, 6+, 9+, 10+, 11+, 17+, 19+, 20+, 25+, 32+, 32+, 34+, 35+

:::: {.columns}
::: {.column width="50%"}
- We can organize these times into intervals:

| Interval | Beginning total | # lost | # relapsed |
|---------|----------|---------|---------|
| (0 - 5] | 21 | 0 | 0 |
| (5 - 10] | 21 | 3 | 5 |
| (10 - 15] | 13 | 1 | 1 |
| (15 - 20] | 11 | 3 | 1 |
| (20 - 25] | 7 | 1 | 2 |
| (25 - 30] | 4 | 0 | 0 |
| (30 - 35] | 4 | 4 | 0 |
:::

::: {.column width="50%"}
::: {.fragment}
How do we account for censoring when estimating $S(t)$?

- Could adjust the beginning totals for censoring

   - Assume censoring occurs [uniformly]{.alert} throughout the interval $\Rightarrow$ censored participants are at risk for half the interval (on average)

- [Adjusted total at risk]{.alert} in an interval is then:
$$\begin{align*}\text{Adju. total} &= (\text{Uncens. total}) + (\text{# lost})/2\\
& = (\text{Beg. total}) - (\text{# lost}) + (\text{# lost})/2\\
& = (\text{Beg. total}) - (\text{# lost})/2\end{align*}$$
:::
:::
::::

---

## Life table estimates

:::: {.columns}
::: {.column width="50%"}
This gets us:

| Interval | Beginning total | # lost | Adju. total | # relapsed |
|---------|----------|---------|---------|
| (0 - 5] | 21 | 0 | 21 | 0 |
| (5 - 10] | 21 | 3 | 19.5 | 5 |
| (10 - 15] | 13 | 1 | 12.5 | 1 |
| (15 - 20] | 11 | 3 | 9.5 | 1 |
| (20 - 25] | 7 | 1 | 6.5 | 2 |
| (25 - 30] | 4 | 0 | 4 | 0 |
| (30 - 35] | 4 | 4 | 2 | 0 |
:::

::: {.column width="50%"}
Then we can compute the survival proportion for each interval:
$$(0 - 5]: \frac{21-0}{21} = 1$$
$$(5 - 10]: \frac{19.5-5}{19.5} = 14.5/19.5 = 0.74 \Rightarrow $$ 75% of those not relapsed at the end of month 5 have not relapsed by the end of the 10th month

::: {.fragment}
$$(10 - 15]: \frac{12.5-1}{12.5} = 11.5/12.5 = 0.92 \Rightarrow $$ 92% of those not relapsed at the end of month 10 have not relapsed by the end of the 15th month
:::
:::
::::

---

## Life table estimates

Then we can use conditional probability to compute the surviving proportions

$$P(T > 10) = P(T>5) \times \frac{P(T>10)}{P(T>5)} = 1 \times 0.74 = 0.74$$

$$P(T > 15) = P(T>5) \times \frac{P(T>10)}{P(T>5)} \times \frac{P(T>15)}{P(T>10)} = 1 \times 0.74 \times 0.92 = 0.68$$

---

## Life table estimates

Which gets us:

| Interval | Beginning total | # lost | Adju. total | # relapsed | Cond. interval survival | Marg. survival|
|-------|------|-----|-----|-----|------|------|
| (0 - 5] | 21 | 0 | 21 | 0 | 1 | 1 |
| (5 - 10] | 21 | 3 | 19.5 | 5 | 0.7436 | 0.7436 |
| (10 - 15] | 13 | 1 | 12.5 | 1 | 0.92 | 0.6841 |
| (15 - 20] | 11 | 3 | 9.5 | 1 | 0.8947 | 0.6121 |
| (20 - 25] | 7 | 1 | 6.5 | 2 | 0.6923 | 0.4236 |
| (25 - 30] | 4 | 0 | 4 | 0 | 1 | 0.4236 |
| (30 - 35] | 4 | 4 | 2 | 0 | 1 | 0.4236 |


This method requires us to come up with the intervals [on our own]{.underline}

- Some of these intervals have no events or censoring, some have multiple

- Who's to say what's the correct interval? ... or is there a better way?

---

## Kaplan-Meier estimator of $S(t)$

The [Kaplan-Meier estimator]{.alert} of $S(t)$ considers the probability of survival a (very small) interval of time, given that an individual is at risk at the beginning of the interval

<br>

- Say we have $D$ total event times in a sample, and we can put them in order: $t_1 < t_2 < \dots < t_D$

- $d_i$: total number of [events]{.underline} at time $t_i$

- $s_i$: total number who [haven't failed]{.underline} by time $t_i$

- $Y_i$: total number [at risk]{.underline} at time $t_i$ (under observation, not failed)

<br>

Total number of events at time $t_i$ is equal to total number at risk minus those who haven't failed
$$d_i = Y_i - s_i$$

---

## KM estimator 

Instead of looking at neatly-spaced calendar time intervals (like we did before), the KM estimator calculates a conditional survival probability for each time interval in which [only 1 event occurs]{.underline}, and then multiplies them together:

$$\begin{align*}\widehat{S}_{KM}(t) &= P(T>t)\\
&= P(T>t | T > t_k) \times P(T>t_k | T>t_{k-1})\times \dots \times P(T>t_1)\\
&= \prod_{t_i \le t}\left[1-\frac{d_i}{Y_i}\right]\end{align*}$$

- We can just focus on the intervals where an event occurs because if the interval contains no event, the conditional surivival probability is approximately 1

---

## Example: 6-MP

Back to our 6-MP example, with event and censoring times: 6, 6, 6, 7, 10, 13, 16, 22, 23, 6+, 9+, 10+, 11+, 17+, 19+, 20+, 25+, 32+, 32+, 34+, 35+

| $t_i$ | $d_i$ | $Y_i$ | $\widehat{S}_{KM}(t)$ | 
|-------|------|-----|-----|
| 6 | 3 | 21 | [$1-\frac{3}{21}=0.857$]{.fragment fragment-index=1} |
| 7 | 1 | 17 | [$0.857(1-\frac{1}{17})=0.807$]{.fragment fragment-index=1} |
| 10 | 1 | 15 | [$0.807(1-\frac{1}{15})=0.753$]{.fragment fragment-index=1} |
| 13 | 1 | 12 | [$0.753(1-\frac{1}{12})=0.690$]{.fragment fragment-index=1} |
| 16 | 1 | 11 | [$0.690(1-\frac{1}{11})=0.628$]{.fragment fragment-index=1} |
| 22 | 1 | 7 | [$0.628(1-\frac{1}{7})=0.538$]{.fragment fragment-index=1} |
| 23 | 1 | 6 | [$0.538(1-\frac{1}{16})=0.448$]{.fragment fragment-index=1} |

---

## Example: 6-MP

:::: {.columns}

::: {.column width="50%"}

If we were to wite our KM estimates in the style of a life table, this would look like:

| $t$ | $\widehat{S}_{KM}(t)$ | 
|-------|------|
| [0 - 6) | 0.857 |
| [6 - 7) | 0.807 |
| [7 - 10) | 0.753 |
| [10 - 13) | 0.690 |
| [13 - 16) | 0.628 |
| [16 - 22) | 0.538 |
| [22 - 23) | 0.448 |

:::

::: {.column width="10%"}
:::

::: {.column width="40%"}

<br>

Compared to before:

| Interval | Marg. survival|
|-------|------|
| (0 - 5]  | 1 |
| (5 - 10] | 0.7436 |
| (10 - 15] | 0.6841 |
| (15 - 20] | 0.6121 |
| (20 - 25] | 0.4236 |
| (25 - 30] | 0.4236 |
| (30 - 35] | 0.4236 |

:::

::::

<br>

We've estimated the survival function, but maybe we'd also like some confidence intervals around this probability...

---

## KM confidence intervals

$d_i$: total number of [events]{.underline} at time $t_i$; $Y_i$: total number [at risk]{.underline} at time $t_i$ (under observation, not failed)

:::: {.columns}
::: {.column width="60%"}
We can also estimate the variance and standard error of the KM estimator by:

$$\widehat{Var}\left[\widehat{S}_{KM}(t)\right] = \widehat{S}_{KM}(t)^2 \sum_{t_i \le t}\frac{d_i}{Y_i(Y_i - d_i)}$$

$$\widehat{SE}\left[\widehat{S}_{KM}(t)\right] = \sqrt{\widehat{Var}\left[\widehat{S}_{KM}(t)\right]} = \widehat{S}_{KM}(t) \sqrt{\sum_{t_i \le t}\frac{d_i}{Y_i(Y_i - d_i)}}$$
:::

::: {.column width="40%"}
This means we can construct $(1-\alpha)\times 100$% confidence intervals for $S(t)$:
$$\widehat{S}_{KM}(t) \pm Z_{1-\alpha/2} \times \widehat{SE}_{KM}(t)$$
:::
::::

<br>

This formulation of a confidence interval for $S(t)$ could produce upper and lower bounds outside of [0,1]

- $S(t)$ is a probability, so this is a problem!

---

## KM confidence intervals

One way to avoid this is by using the [complementary log-log transformation]{.alert}

- Recall that the cumulative hazard function $\Lambda(t)=-\log S(t)$

- So, the complementary log-log transformation gives us:

$$\log\left[-\log S(t)\right] = \log\left[\Lambda(t)\right]$$

<br>

Next we'll build a confidence interval for $\log\left[\widehat\Lambda(t)\right]$ first, and then we'll back-transform it to get the interval for $\widehat{S}(t)$

---

## KM confidence intervals

[Complementary log-log transformation]{.alert}: $\log\left[-\log S(t)\right] = \log\left[\Lambda(t)\right]$

<br>

The variance for $\log\left[\widehat{\Lambda}(t)\right]$ is:
$$Var\left[\log\left[\widehat{\Lambda}(t)\right]\right] = \frac{Var\left[\widehat S(t)\right]}{\left(\widehat S(t) \log\left[\widehat S(t)\right]\right)^2}$$

<br>

This means the confidence interval for $\log\left[\widehat{\Lambda}(t)\right]$ is:
$$\widehat \Lambda(t) \pm Z_{1-\alpha/2} \times \sqrt{ \frac{Var\left[\widehat S(t)\right]}{\left(\widehat S(t) \log\left[\widehat S(t)\right]\right)^2} } \Rightarrow\widehat \Lambda(t) \pm Z_{1-\alpha/2} \times \widehat{SE}_{\Lambda}(t)$$


---

## KM confidence intervals

[Complementary log-log transformation]{.alert}: $\log\left[-\log S(t)\right] = \log\left[\Lambda(t)\right]$

<br>

This means the confidence interval for $\log\left[\widehat{\Lambda}(t)\right]$ is:
$$\widehat \Lambda(t) \pm Z_{1-\alpha/2} \times \sqrt{ \frac{Var\left[\widehat S_{KM}(t)\right]}{\left(\widehat S_{KM}(t) \log\left[\widehat S_{KM}(t)\right]\right)^2} } \Rightarrow\widehat \Lambda(t) \pm Z_{1-\alpha/2} \times \widehat{SE}_{\Lambda}(t)$$

<br>

Back-transforming this to $\widehat S_{KM}(t)$ gets us the confidence interval:
$$\left(\widehat S_{KM}(t)^{\exp\left\{Z_{1-\alpha/2} \times \widehat{SE}_{\Lambda}(t)\right\}}, ~ \widehat S_{KM}(t)^{\exp\left\{-Z_{1-\alpha/2} \times \widehat{SE}_{\Lambda}(t)\right\}}\right)$$

---

## Nelson-Aalen estimator of $\Lambda(t)$

We can also use the KM estimator $\widehat{S}_{KM}(t)$ to get an estimate of the cumulative hazard function:
$$\widehat{\Lambda}_{KM}(t) = -\log\left[\widehat{S}_{KM}(t)\right]$$

- But this estimator can perform poorly when the sample size is small

<br>

An alternative estimator of $\Lambda(t)$ is what we call the [Nelson-Aalen estimator]{.alert}:

$$\tilde{\Lambda}_{NA}(t) = \sum_{t_i \le t}\frac{d_i}{Y_i}$$
---

## NA estimator

An alternative estimator of $\Lambda(t)$ is the [Nelson-Aalen estimator]{.alert}:

$$\tilde{\Lambda}_{NA}(t) = \sum_{t_i \le t}\frac{d_i}{Y_i}$$

:::: {.columns}
::: {.column width="60%"}
The variance and standard error of the NA estimator are given by:
$$\widehat{Var}[\tilde{\Lambda}_{NA}(t)] = \sum_{t_i \le t}\frac{d_i}{Y_i^2}$$

$$\widehat{SE}[\tilde{\Lambda}_{NA}(t)] =  \sqrt{\widehat{Var}[\tilde{\Lambda}_{NA}(t)]} = \sqrt{\sum_{t_i \le t}\frac{d_i}{Y_i^2}}$$
:::

::: {.column width="40%"}
This means we can construct $(1-\alpha)\times 100$% confidence intervals for $\Lambda(t)$:
$$\tilde{\Lambda}_{NA}(t) \pm Z_{1-\alpha/2} \times \widehat{SE}_{NA}(t)$$
:::
::::

---

## NA estimator of $\Lambda(t)$ and $S(t)$

$$\tilde{\Lambda}_{NA}(t) = \sum_{t_i \le t}\frac{d_i}{Y_i}$$

Using the NA estimator of $\Lambda(t)$, we can also construct an alternative estimator of the survival function, which we call the [Breslow estimator]{.alert}:
$$\tilde{S}_{B}(t) = e^{-\tilde{\Lambda}_{NA}(t)}$$

---

## Example: 6-MP

$$\widehat \Lambda_{KM}(t) = -\log[\widehat S_{KM}(t)], ~~ \tilde{\Lambda}_{NA}(t) = \sum_{t_i \le t}\frac{d_i}{Y_i}, ~~ \tilde S_{B}(t) = e^{-\tilde{\Lambda}_{NA}(t)}$$

Recall the 6-MP event and censoring times: 6, 6, 6, 7, 10, 13, 16, 22, 23, 6+, 9+, 10+, 11+, 17+, 19+, 20+, 25+, 32+, 32+, 34+, 35+

| $t_i$ | $d_i$ | $Y_i$ | $\widehat{S}_{KM}(t)$ | $\widehat\Lambda_{KM}(t)$ | $\tilde\Lambda_{NA}(t)$ | $\tilde S_{B}(t)$ |
|-------|------|-----|-----|------|-----|-----|
| 6 | 3 | 21 | 0.857 | [$-\log(0.857)=0.154$]{.fragment fragment-index=1} | [$\frac{3}{21}=0.1428$]{.fragment fragment-index=1} | [$e^{0.1428}=0.887$]{.fragment fragment-index=1} |
| 7 | 1 | 17 | 0.807 |[$-\log(0.807)=0.214$]{.fragment fragment-index=2} | [$0.1428 + \frac{1}{17}=0.2017$]{.fragment fragment-index=2} | [$e^{0.2017}=0.8173$]{.fragment fragment-index=2} |
| 10 | 1 | 15 | 0.753 | [$-\log(0.753)=0.284$]{.fragment fragment-index=3} | [$0.2017 + \frac{1}{15}=0.2683$]{.fragment fragment-index=3} | [$e^{0.2683}=0.7647$]{.fragment fragment-index=3} |
| 13 | 1 | 12 | 0.690 | [$-\log(0.690)=0.371$]{.fragment fragment-index=3} | [$0.2683 + \frac{1}{12}=0.3517$]{.fragment fragment-index=3} | [$e^{0.3517}=0.7035$]{.fragment fragment-index=3} |
| 16 | 1 | 11 | 0.628 | [$-\log(0.628)=0.465$]{.fragment fragment-index=3} | [$0.3517 + \frac{1}{11}=0.4426$]{.fragment fragment-index=3} | [$e^{0.4426}=0.6424$]{.fragment fragment-index=3} |
| 22 | 1 | 7 | 0.538 | [$-\log(0.538)=0.620$]{.fragment fragment-index=3} | [$0.4426 + \frac{1}{7}=0.5854$]{.fragment fragment-index=3} | [$e^{0.5854}=0.5569$]{.fragment fragment-index=3} |
| 23 | 1 | 6 |0.448 | [$-\log(0.448)=0.803$]{.fragment fragment-index=3} | [$0.5854 + \frac{1}{6}=0.7521$]{.fragment fragment-index=3} | [$e^{0.7521}=0.4714$]{.fragment fragment-index=3} |

---

## Example: 6-MP

Comparing the KM and NA estimates in a life table would look like:

|        | Survival Functions || Cumulative Hazards |
|-------|------|-------|------|------|
| $\boldsymbol{t}$ | $\boldsymbol{\widehat{S}_{KM}(t)}$ | $\boldsymbol{\tilde S_{B}(t)}$ | $\boldsymbol{\widehat\Lambda_{KM}(t)}$ | $\boldsymbol{\tilde\Lambda_{NA}(t)}$ |
| [0 - 6) | 0.857 | 0.887 | 0.154 | 0.143 |
| [6 - 7) | 0.807 | 0.817 | 0.214 | 0.202 |
| [7 - 10) | 0.753 | 0.765 | 0.284 | 0.268 |
| [10 - 13) | 0.690 | 0.704 | 0.371 | 0.352 |
| [13 - 16) | 0.628 | 0.342 | 0.465 | 0.443 |
| [16 - 22) | 0.538 | 0.557 | 0.620 | 0.585 |
| [22 - 23) | 0.448 | 0.471 | 0.803 | 0.752 |

---

## Survival curves

We've estimated all these survival probabilities but what do we... do with them?

- We could do like we've been doing and put them in a table and track how the probabilities change with time

- ... but that can get complicated quickly, and it's not very easy to compare how the probabilities change across multiple groups

[Solution: plot the probabilities as curves!]{.alert}

---

## Example: 6-MP

An example of a survival curve for the 6-MP data, using a KM estimator for the survival function
```{r 6mp data, echo=F}
library(KMsurv)
library(survival)
data(drug6mp)

```
```{r survival curve, out.width="90%"}
km.6mp <- survfit( Surv(t2, relapse) ~ 1, data=drug6mp )
plot(km.6mp, mark.time=T, xlab="Months since study start", ylab="Survival")
```

---

## Comparing survival curves

If we would like to formally compare the survival functions for multiple groups, we can perform a statistical test

$$\begin{align*}&H_0: S_1(t) = S_2(t) \text{ for all } t\\
&H_1: S_1(t) \ne S_2(t) \text{ for at least one } t\end{align*}$$

<br>

Let $t_1 < t_2 < \dots < t_D$ denote the distinct event times in the overall sample

- At time $t_i$, we observe:

   - $d_{1i}$ events from the $Y_{1i}$ subjects at risk in the first group
   
   - $d_{2i}$ events from the $Y_{2i}$ subjects at risk in the second group
   
   - $d_i = d_{1i} + d_{2i}$ events from the $U_i = Y_{1i} + Y_{2i}$ subjects at risk in the two groups combined

---

## Comparing survival curves

We can then make a contingency table for events and non-events at time $t_i$:

![](logrank-table.png)


Under the null hypothesis $H_0$, every subject at risk at time $t_i$ has the [same probability]{.underline} of experiencing an event at time $t_i$ regardless of group

- Under $H_0$, the expected number of subjects in group 1 who experience an event at time $t_i$ is $E_{1i} = E[d_{1i}] = d_i \frac{Y_{1i}}{Y_i}$

   - The variance is then $V_{1i} = Var[d_{1i}] = d_i \times \frac{Y_{1i}}{Y_i}\times\frac{Y_{2i}}{Y_i}\times\frac{Y_i - d_i}{Y_i - 1}$


---

## Logrank tests

We can use $d_{1i}$, $E_{1i}$, $V_{1i}$, and a weight function $W(t_i)$ to construct a weighted test statistic:

$$Z = \frac{\sum_{i=1}^D W(t_i)(d_{1i} - E_{1i})}{\sqrt{\sum_{i=1}^D W(t_i)^2 V_{1i}}}$$

If we choose $W(t_i) = 1$ for all $t$, we get the [Mantel-Haenszel]{.alert} or [logrank]{.alert} test statistic:

$$Z = \frac{\sum_{i=1}^D (d_{1i} - E_{1i})}{\sqrt{\sum_{i=1}^D V_{1i}}}$$

- We can compare this test statistic to a standard Normal distribution to determine whether we should reject $H_0$

- If we reject, this means that we have evidence that people in group 1 [do not]{.underline} have the same probability of experiencing an event at time $t_i$ as the people in group 2!

---

## Proportional hazards

:::: {.columns}
::: {.column width="50%"}
The logrank test is [most powerful]{.alert} when the alternative looks at a hazard for group 2 that is [different but proportional]{.alert} to the group 1 hazard: $\lambda_2(t) = \alpha \lambda_1(t)$
:::

::: {.column width="50%"}
![](prop-hazards.png){width=1000px}

:::
::::

---

## Proportional hazards

:::: {.columns}
::: {.column width="50%"}
The logrank test is [most powerful]{.alert} when the alternative looks at a hazard for group 2 that is [different but proportional]{.alert} to the group 1 hazard: $\lambda_2(t) = \alpha \lambda_1(t)$

- Can also rewrite this in terms of survival functions: $S_2(t) = S_1(t)^\alpha$

<br>

- Or in terms of the log cumulative hazards: $\log[\Lambda_2(t)] = \log[\alpha] + \log[\Lambda_1(t)]$

<br>

This means that the logrank test will be most powerful if the log cumulative hazards are (roughly) [parallel]{.alert}
:::

::: {.column width="50%"}
![](parallel-log-hazards.png){width=1000px}
:::
::::

---

## Example: 6-MP

```{r cancer long, echo=F}
drug6mp_long <- drug6mp %>% 
   pivot_longer(c("t1", "t2"), names_to="sixmp", values_to="time") %>% 
   mutate(sixmp = ifelse(sixmp=="t2", 1, 0),
          relapse = ifelse(sixmp==0, 1, relapse))
```

```{r cancer curves, out.width="80%"}
km.6mp2 <- survfit( Surv(time, relapse) ~ sixmp, data=drug6mp_long )
plot(km.6mp2, mark.time=T, xlab="Months since study start", ylab="Survival", lty=1:2)
legend( 1, .6, lty=1:2, legend=c("Placebo", "6-MP"), bty="n" )
```

::: {.absolute width=20% right=0 top=300}
Curves look pretty proportional... Let's do a logrank test to see if these curves are (significantly) different
:::

---

## Example: 6-MP

```{r cancer logrank}
survdiff( Surv(time, relapse) ~ sixmp, data=drug6mp_long )
```

A test statistic of 16.8 with a p-value of 0.00004... probably pretty different!

- Can conclude that 6-MP treatment group has higher relapse-free survival than placebo (higher probability that the treatment group's relapse time is greater that some time $t$ compared to the placebo group)

---

## Weighted logrank tests

While the logrank test will still work work under [non-proportional hazards]{.underline}, we might want to choose a different weight function $W(t_i)$ that would allow us to weight differences in the observed and expected number of events ($d_{1i} - E_{1i}$) differently over time

<br>

- [Gehan-Breslow]{.alert} weights: $W(t_i) = Y_i$ applies more weight to early failure times

- [Generalized Wilcoxon]{.alert} weights: $W(t_i) = \widehat{S}_{KM}(t_k-) = \prod_{t_i \le t}\left[1-\frac{d_i}{Y_i + 1}\right]$ also applies greater weight to early failure times

- [Fleming & Harrington]{.alert} weights: $W_{p,q}(t_i) = \left[\widehat{S}_{KM}(t_{i-1})\right]^p\left[1-\widehat{S}_{KM}(t_{i-1})\right]^q$

   - $p=q=0$ gives the unweighted logrank statistic
   
   - $p=1$, $q=0$ gives the generalized Wilcoxon logrank statistic
   
---

## Take home messages

- Can be hard to know that we're right about our assumed probability distribution for survival time

<br>

- In that case, it's good to estimate the survival function $S(t)$ using non-parametric methods

   - The Kaplan-Meier estimator $\widehat S_{KM}(t)$ is usually the default
   
   - The Nelson Aalen estimator of the cumulative hazard $\tilde\Lambda_{NA}(t)$ and it's equivalent Breslow estimator of the surivival function $\widehat S_B(t)$ work well with small sample sizes

<br>

- We can plot the survival function as a curve over time, and we can compare whether curves for different groups differ by using a logrank test