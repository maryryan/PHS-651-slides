---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1200
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
      pdf-separate-fragments: true
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```

<h1>Lecture 3.1: Marginal models for correlated data</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>September 23, 2025</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=225 left=300 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/F25/lec-3-1/slides-3-1>

:::

::: {.absolute bottom=120 left=50 width=250}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/F25/lec-3-1/slides-3-1"))
```
:::

---

## Recap: (generalized) linear mixed effect models

One way to account for clustering in analysis is with a [(generalized) linear mixed
effects]{.alert} model
$$g(Y_{ij}) = \color{red}{\beta_0 + U_{ij}\beta_1} + \color{blue}{Z_ib_{0i}}+ \epsilon_{ij}$$
$$g(\vec{Y}_i) = \color{red}{\boldsymbol{X}_i\vec{\beta}} + \color{blue}{\vec{Z}_ib_{0i}}+\vec{\epsilon}_i$$
(*Transformed outcome for individual j in cluster i, modeled as a function of the exposure status for individual j in cluster i, with cluster-level random effects*)

- $g(\cdot)$ is a [link function]{.alert} that transforms the outcome

-   $\color{red}{\beta_0 + u_{ij}\beta_1}$ (or $\color{red}{\boldsymbol{X}_i\vec{\beta}}$) are the [fixed effects]{.alert} modeling the mean outcome response as a function of variables

-   $\color{blue}{Z_i b_{0i}}$ ($\color{blue}{\vec{Z}_ib_{0i}}$) are the [random effects]{.alert} accounting for
    random between-cluster variation

-   $\epsilon_{ij}$ ($\vec{\epsilon}_i$) is the regular between-individual variation

- We assume the outcome observations in cluster *i*, $\vec{Y}_i$, follow a [multivariate probability distribution]{.alert}

---

## Recap: (generalized) linear mixed effect models

One way to account for clustering in analysis is with a [(generalized) linear mixed
effects]{.alert} model
$$g(Y_{ij}) = \color{red}{\beta_0 + U_{ij}\beta_1} + \color{blue}{Z_ib_{0i}}+ \epsilon_{ij}$$
$$g(\vec{Y}_i) = \color{red}{\boldsymbol{X}_i\vec{\beta}} + \color{blue}{\vec{Z}_ib_{0i}}+\vec{\epsilon}_i$$

::: {.columns}

::: {.column width="60%"}
![*Figure modified from Harrison et al. (2018). "A brief introduction to mixed effects modelling and multi-model inference in ecology." Ecology.*](random-intercept.png){width="70%"}
:::

::: {.column width="40%"}
$$\mu_{\text{group}} = \beta_0$$
$$\alpha_1 = \beta_0 + b_{0i}$$
:::

::::


---

## Recap: (generalized) linear mixed effect models

One way to account for clustering in analysis is with a [(generalized) linear mixed
effects]{.alert} model
$$g(Y_{ij}) = \color{red}{\beta_0 + U_{ij}\beta_1} + \color{blue}{Z_ib_{0i}}+ \epsilon_{ij}$$
$$g(\vec{Y}_i) = \color{red}{\boldsymbol{X}_i\vec{\beta}} + \color{blue}{\vec{Z}_ib_{0i}}+\vec{\epsilon}_i$$

The [fixed effects]{.alert} are shared across everyone in the sample,
while the [random effects]{.alert} are unique to a particular cluster (however we define cluster)

. . .

<br>
Interpretations of $\beta_1$ in an LME (identity link) are technically [conditional]{.alert} because they are comparing 2 individuals with the same cluster membership ($E[Y_{ij} | \boldsymbol{X}_i, b_{0i}]$)

- It is easy to separate out the random effects, though, and get the [marginal]{.alert} interpretation

---

## Recap: (generalized) linear mixed effect models

One way to account for clustering in analysis is with a [(generalized) linear mixed
effects]{.alert} model
$$g(Y_{ij}) = \color{red}{\beta_0 + U_{ij}\beta_1} + \color{blue}{Z_ib_{0i}}+ \epsilon_{ij}$$
$$g(\vec{Y}_i) = \color{red}{\boldsymbol{X}_i\vec{\beta}} + \color{blue}{\vec{Z}_ib_{0i}}+\vec{\epsilon}_i$$

The [fixed effects]{.alert} are shared across everyone in the sample,
while the [random effects]{.alert} are unique to a particular cluster (however we define cluster)

<br>
Interpretations of $\beta_1$ in a GLMM are [strictly conditional]{.alert} because
they are comparing 2 individuals with the same cluster membership, or have the same random effect value ($g\left(E[Y_{ij} | \boldsymbol{X}_i, b_{0i}]\right)$)

-   Often referred to as having individual-specific or cluster-specific
    interpretation

-   Possible to extract marginal effects from conditional models, but
    often complicated and prone to bias
    

---

## Recap: Marginal effects in GLMMs

In order to get the marginal effects, we would need to inverse-transform the outcome (undo the logit function):
$$E[Y_{ij}|\boldsymbol{X}_i, b_{0i}] = \frac{\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}{1+\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}~~~~~~~ E[Y_{ij}|\boldsymbol{X}_i, b_{0i}] = \frac{\exp(\beta_0 + U_{ij}\beta_1 + Z_ib_{0i})}{1+\exp(\beta_0 + U_{ij}\beta_1 + Z_ib_{0i})}$$

and then take the marginal expectation of that:

:::: {.columns}
::: {.column width="50%"}
$$\begin{align*}E[\vec{Y}_i|\boldsymbol{X}_i] &= E\left\{E[\vec{Y}_i|\boldsymbol{X}_i, b_{0i}]\right\}\\
&=E\left[\frac{\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}{1+\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}\right]\end{align*}$$
:::

::: {.column width="50%"}
$$\begin{align*}E[Y_{ij}|\boldsymbol{X}_i] &= E\left\{E[Y_{ij}|\boldsymbol{X}_i, b_{0i}]\right\}\\
&=E\left[\frac{\exp(\beta_0 + U_{ij}\beta_1 + Z_ib_{0i})}{1+\exp(\beta_0 + U_{ij}\beta_1 + Z_ib_{0i})}\right]\end{align*}$$
:::
::::


- Clearly, $E[\vec{Y}_i|\boldsymbol{X}_i]$ [is not the same as]{.alert} $g(E[\vec{Y}_i|\boldsymbol{X}_i])=\frac{\exp(\boldsymbol{X}_i\vec{\beta})}{1+\exp(\boldsymbol{X}_i\vec{\beta})}$

This phenomenon is known as [non-collapsibility]{.alert}

---

## Recap: (generalized) linear mixed effect models

One way to account for clustering in analysis is with a [(generalized) linear mixed
effects]{.alert} model
$$g(Y_{ij}) = \color{red}{\beta_0 + U_{ij}\beta_1} + \color{blue}{Z_ib_{0i}}+ \epsilon_{ij}$$
$$g(\vec{Y}_i) = \color{red}{\boldsymbol{X}_i\vec{\beta}} + \color{blue}{\vec{Z}_ib_{0i}}+\vec{\epsilon}_i$$

The [fixed effects]{.alert} are shared across everyone in the sample,
while the [random effects]{.alert} are unique to a particular cluster (however we define cluster)

<br>
Interpretations of $\beta_1$ in a GLMM are [strictly conditional]{.alert} because
they are comparing 2 individuals with the same cluster membership, or have the same random effect value ($g\left(E[Y_{ij} | \boldsymbol{X}_i, b_{0i}]\right)$)

-   Often referred to as having individual-specific or cluster-specific
    interpretation

-   Possible to extract marginal effects from conditional models, but
    often complicated and prone to bias

[What if we want to look at the marginal effect of an exposure -- one that's *averaged across*
all clusters?]{.alert}

------------------------------------------------------------------------

## Marginal models

Basic idea: we want to account for clustering of data, but make
inference about [population averages]{.alert} (marginal effects)

-   We're often very interested in these in public and population health!

- Not to be confused with the average treatment effect (ATE) -- this is a different concept

-   Mean response is conditional *only* on the adjustment covariates we add
    to the mean model

. . .

<br>
Rigid distributional assumptions got in the way of this in LMEs/GLMMs

- Adding random effects were how we were able to have non-0 covariance ($Cov[Y_{ij},Y_{ik}]\ne 0$) while still allowing for our outcome to follow a proper probability distribution

- This allowed us to assume some things about our data and not have to estimate them

- So for marginal models we want to use [fewer distributional assumptions]{.alert}

<!-- . . . -->

<!-- - How can we account for correlation among outcome observations/non-0 covariance without the rigidity of distribution assumptions? -->

---

## R-side and G-side variance

$$g(\vec{Y}_i) = \boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_ib_{0i} + \vec{\epsilon}_i$$

- Recall that with LMEs and GLMMs, we modeled our [systematic variation]{.underline} (random effects and within-cluster individual variation) as:
$$\vec{b}_0 \sim N(\vec{0}, \boldsymbol{G}),~~~ \vec{\epsilon}_i \sim N(\vec{0}, \boldsymbol{R}_i=\sigma_\epsilon^2\boldsymbol{I})$$

- When using random effects (like in LMEs and GLMMs), some call this modeling with [G-side variance]{.alert}

:::: {.columns}

::: {.column width="50%"}
The $\boldsymbol{G}$ matrix with random intercepts looked like:
$$\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}$$
:::

::: {.column width="50%"}
The $\boldsymbol{R}_i = \sigma^2_{\epsilon}\boldsymbol{I}$ matrix for within-cluster individual variation looked like:
$$\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \ddots & \dots & 0 \\
\vdots & \dots & \ddots & \vdots\\
0 & \dots & 0 & \sigma^2
\end{bmatrix}$$
:::

::::

---

## R-side and G-side variance

$$g(\vec{Y}_i) = \boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_ib_{0i} + \vec{\epsilon}_i$$

- Recall that with LMEs and GLMMs, we modeled our [systematic variation]{.underline} (random effects and within-cluster individual variation) as:
$$\vec{b}_0 \sim N(\vec{0}, \boldsymbol{G}),~~~ \vec{\epsilon}_i \sim N(\vec{0}, \boldsymbol{R}_i=\sigma_\epsilon^2\boldsymbol{I})$$

- When using random effects (like in LMEs and GLMMs), some call this modeling with [G-side variance]{.alert}

   - These random effects $\vec{\gamma}$ were difficult to marginalize out of the expectation when we used a non-identity link function
   
   - What if instead we had *no* random effects ($\boldsymbol{G}=0$), and modeled non-zero covariance using $\vec{\epsilon}_i$ and a $\boldsymbol{R}_i \ne \sigma^2_\epsilon \boldsymbol{I}$?

---

## R-side and G-side variance

$$g(\vec{Y}_i) = \boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_ib_{0i} + \vec{\epsilon}_i$$

- Recall that with LMEs and GLMMs, we modeled our [systematic variation]{.underline} (random effects and within-cluster individual variation) as:
$$\vec{b}_0 \sim N(\vec{0}, \boldsymbol{G}),~~~ \vec{\epsilon}_i \sim N(\vec{0}, \boldsymbol{R}_i=\sigma_\epsilon^2\boldsymbol{I})$$

- When using random effects (like in LMEs and GLMMs), some call this modeling with G-side variance

- When using $\boldsymbol{G}=0$ (no random intercepts), we call this modeling with [R-side variance]{.alert}

   - [Marginal modeling]{.alert} approaches tend to exclusively model with [R-side variance]{.underline}
   
   - Downside: this means we're not modeling the outcome using a proper probability distribution (more on this soon...)
   

<!--    - Those assumptions focused on specifying [within-cluster and between cluster variability]{.alert}, and helped show what the covariance matrix for a cluster should look like and how we could estimate model parameters -->

<!--    - What if we focused more on the [within-cluster correlation]{.alert}, and use an [empirically estimated]{.alert} covariance structure? -->

------------------------------------------------------------------------

## Marginal model components

There are 2 main components of a marginal model

1. The [marginal expectation]{.alert} of the outcome $E[Y_{ij}|\vec{X}_{ij}]=\mu_{ij}$:
- $\mu_{ij}$ should only depend on the covariates in the model through the link function $g(\cdot)$: $~g(\mu_{ij}) = \vec{X}_{ij}\vec{\beta}$

2. The covariance of the outcome $Cov[Y_{ij}, Y_{ik}|\boldsymbol{X}_i]$, which can be broken down into 2 more components:

:::: {.columns}
::: {.column width="50%"}
::: {.fragment}
i. The [marginal variance]{.alert} of the outcome:

$$Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$$

- Should depend on the marginal mean through the variance function $v(\cdot)$ and a scale parameter $\varphi$
:::
:::

::: {.column width="50%"}
::: {.fragment}
ii. The [within-cluster association]{.alert} of the outcomes ($Corr[Y_{ij},Y_{ik}]$)

   - Should depend on the means and a correlation parameter(s)
:::
:::
::::

::: {.fragment .absolute bottom=200 right=200 width="30%"}
[Component 2 is kind of like the systematic variation in GLMMs we saw in lecture 2.1!]{.alert}
:::


---

## Marginal model components example: continuous outcomes

For example, in a model with clustered continuous responses $Y_{ij}$ and only one exposure $U_{ij}$, we might have:

1.  $E[Y_{ij}|\boldsymbol{X}_i] = \mu_{ij} = \beta_0 + U_{ij}\beta_1$

- We specify this based on our domain knowledge, hypotheses we want to test, model-building strategies, etc
    
- To account for confounding, we would add additional variables to the right hand side so that we [correctly model the mean]{.alert}

. . .

2.1.  $Var[Y_{ij}|\boldsymbol{X}] = \varphi_{i} = \sigma^2$

- Clusters share a common variance, which we assume is independent of $\mu_{ij}$

- Could also assume difference variances for each cluster, $\sigma^2_i$!

. . .

2.2.  $Corr[Y_{ij}, Y_{ik}] = \alpha$

- Outcomes in same cluster are all correlated by $\alpha$

---

## Marginal model components example: continuous outcomes

For example, in a model with clustered continuous responses $Y_{ij}$ and only one exposure $U_{ij}$, we might have:

1.  $E[Y_{ij}|\boldsymbol{X}_i] = \mu_{ij} = \beta_0 + U_{ij}\beta_1$

2.1.  $Var[Y_{ij}|\boldsymbol{X}] = \varphi_{i} = \sigma^2$

2.2.  $Corr[Y_{ij}, Y_{ik}] = \alpha$

- This means we can assume a [correlation structure]{.alert} for one cluster that looks like:

$$\begin{bmatrix}
1 & \alpha & \dots & \alpha \\
\alpha & \ddots & \dots & \alpha \\
\vdots & \dots & \ddots & \vdots\\
\alpha & \dots & \alpha & 1
\end{bmatrix}$$

---

## Marginal model components example: continuous outcomes

If we combined 2.1 and 2.2, we get the covariance between two outcomes in the same cluster:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]\\
&= \sqrt{\sigma^2}~ \alpha ~\sqrt{\sigma^2}\\
&=\sigma^2\alpha\end{align*}$$

. . .

:::: {.columns}
::: {.column width="50%"}
which creates us a [covariance matrix]{.alert} for cluster $i$, $V_i$, that looks like:

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha & \dots & \sigma^2\alpha \\
\sigma^2\alpha & \ddots & \dots & \sigma^2\alpha \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha & \dots & \sigma^2\alpha & \sigma^2
\end{bmatrix}$$

We call $V_i$ the ["working" covariance]{.alert} matrix
:::

::: {.column width="50%"}
::: {.fragment}
which *looks* different than the covariance matrix we'd get from an LME:

$$\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}$$

[but these are actually the same if $\sigma^2 = \sigma^2_b + \sigma^2_\epsilon$ and $\sigma^2\alpha = \sigma^2_b$]{.fragment} [🤯]{.fragment}
:::
:::
::::



---

## Marginal model components example: continuous outcomes

If we combined 2.1 and 2.2, we get the covariance between two outcomes in the same cluster:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]\\
&= \sqrt{\sigma^2}~ \alpha ~\sqrt{\sigma^2}\\
&=\sigma^2\alpha\end{align*}$$

- Outcomes for individuals in **different** clusters would still have a covariance of 0 though because we're assuming that [clusters are independent of one another]{.alert}

---

## Marginal model components example: continuous outcomes

If we combined 2.1 and 2.2, we get the covariance between two outcomes in the same cluster:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]\\
&= \sqrt{\sigma^2}~ \alpha ~\sqrt{\sigma^2}\\
&=\sigma^2\alpha\end{align*}$$

:::: {.columns}
::: {.column width="50%"}
which creates us a [covariance matrix]{.alert} for cluster $i$ that looks like:

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha & \dots & \sigma^2\alpha \\
\sigma^2\alpha & \ddots & \dots & \sigma^2\alpha \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha & \dots & \sigma^2\alpha & \sigma^2
\end{bmatrix}$$
:::

::: {.column width="50%"}
which *looks* different than the covariance matrix we'd get from an LME:

$$\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}$$
:::
::::


[Can we assume other types of correlation/covariance structures?]{.alert} [Yes!]{.fragment}

------------------------------------------------------------------------

## Correlation structures: independence

The most basic (and the one we have the most prior experience with) is the [independence]{.alert} correlation structure

:::: {.columns}
::: {.column width="50%"}
- Assumes all observations in a cluster are completely uncorrelated with each other

$$\begin{bmatrix}
1 & 0 & \dots & 0 \\
0 & \ddots & \dots & 0 \\
\vdots & \dots & \ddots & \vdots\\
0 & \dots & 0 & 1
\end{bmatrix}$$

::: {.fragment}
- We don't have to estimate anything here!
:::
:::

::: {.column width="50%"}
::: {.fragment}
Assuming $Var[Y_{ij}|\boldsymbol{X}] = \sigma^2$, creates a covariance matrix that looks like

$$V_i=\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \ddots & \dots & 0 \\
\vdots & \dots & \ddots & \vdots\\
0 & \dots & 0 & \sigma^2
\end{bmatrix}$$
:::
:::

::::
------------------------------------------------------------------------

## Correlation structures: exchangeable

Another type is the [exchangeable]{.alert} correlation structure (AKA: compound symmetry)

:::: {.columns}
::: {.column width="50%"}
- Assumes all observations in a cluster are related to each other to the same degree

$$\begin{bmatrix}
1 & \alpha & \dots & \alpha \\
\alpha & \ddots & \dots & \alpha \\
\vdots & \dots & \ddots & \vdots\\
\alpha & \dots & \alpha & 1
\end{bmatrix}$$

::: {.fragment}
- Need to estimate $\alpha$
:::
:::

::: {.column width="50%"}
::: {.fragment}
Assuming $Var[Y_{ij}|\boldsymbol{X}] = \sigma^2$, creates a covariance matrix that looks like

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha & \dots & \sigma^2\alpha \\
\sigma^2\alpha & \ddots & \dots & \sigma^2\alpha \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha & \dots & \sigma^2\alpha & \sigma^2
\end{bmatrix}$$
:::
:::

::::

------------------------------------------------------------------------

## Correlation structures: unstructured

Could you assume all observations are (potentially) related to each other in
different ways?

:::: {.columns}
::: {.column width="50%"}
- Known as an [unstructured]{.alert} correlation structure

$$\begin{bmatrix}
1 & \alpha_{12} & \dots & \alpha_{1n} \\
\alpha_{21} & \ddots & \dots & \alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\alpha_{n1} & \dots & \alpha_{n(n-1)} & 1
\end{bmatrix}$$
:::

::: {.column width="50%"}
::: {.fragment fragment-index=1}
Assuming $Var[Y_{ij}|\boldsymbol{X}] = \sigma^2$, creates a covariance matrix that looks like

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha_{12} & \dots & \sigma^2\alpha_{1n} \\
\sigma^2\alpha_{21} & \ddots & \dots & \sigma^2\alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha_{n1} & \dots & \sigma^2\alpha_{n(n-1)} & \sigma^2
\end{bmatrix}$$
:::
:::

::::

::: {.fragment fragment-index=2}
- Offers maximum flexibility because correlations can be the same, very similar, or completely different from each other
:::

::: {.fragment fragment-index=3}
- But also requires you to estimate *every* correlation parameter in this matrix separately

   - This can cause computational and efficiency issues
:::

------------------------------------------------------------------------

## Correlation structures: unstructured

Could you assume all observations are (potentially) related to each other in
different ways?

:::: {.columns}
::: {.column width="50%"}
- Known as an [unstructured]{.alert} correlation structure

$$\begin{bmatrix}
1 & \alpha_{12} & \dots & \alpha_{1n} \\
\alpha_{21} & \ddots & \dots & \alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\alpha_{n1} & \dots & \alpha_{n(n-1)} & 1
\end{bmatrix}$$
:::

::: {.column width="50%"}
Assuming $Var[Y_{ij}|\boldsymbol{X}] = \sigma^2$, creates a covariance matrix that looks like

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha_{12} & \dots & \sigma^2\alpha_{1n} \\
\sigma^2\alpha_{21} & \ddots & \dots & \sigma^2\alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha_{n1} & \dots & \sigma^2\alpha_{n(n-1)} & \sigma^2
\end{bmatrix}$$
:::

::::

- Can get away with unstructured correlation in small data sets, but usually easier to assume a simpler structure

------------------------------------------------------------------------

## Marginal model components

So we have the main components of a marginal model

1.  The marginal expectation of the outcome: $g(E[Y_{ij}| \vec{X}_{ij}]) = g(\mu_{ij}) = \vec{X}_{ij}\vec{\beta}$

2. The covariance of the outcome

   i. The marginal variance of the outcome: $Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$

   ii. The within-cluster association of the outcomes (correlation structure)

[But how do we use these components to estimate our regression coefficients?]{.alert}

. . .

- With OLS and LMEs/GLMMs, we had a likelihood-based estimating equation we could use to derive estimates for $\beta$ (see lecture 2.1)

. . .

-   What if we used something like the GLM estimating equation here?

. . .

Thus is born [generalized estimating equations]{.alert}

<!-- --- -->

<!-- ## Marginal models and estimating equations -->

<!-- We want to take the 3 components of the marginal model -->

<!-- 1. The marginal expectation of the outcome $g(E[Y_{ij}| \vec{X}_{ij}]) = g(\mu_{ij}) = \vec{X}_{ij}\vec{\beta}$ -->

<!-- 2. The marginal variance of the outcome: $Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$ -->

<!-- 3. The within-cluster association of the outcomes -->

<!-- and shove them into something that looks like the GLM estimating equation -->

<!-- . . . -->

<!-- Thus is born [generalized estimating equations]{.alert} -->

------------------------------------------------------------------------

## Generalized estimating equations

Generalized estimating equations ([GEEs]{.alert}) find the $\beta$ that solves an
estimating equation of the general form:
$$\sum_{i=1}^N \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1}[y_{ij} - \mu_{ij}(\vec{\beta})] = 0$$

-   Almost exactly the same as the one we saw for GLMs!

. . .

-   Difference here: $V_i$ is the [working covariance]{.alert} for the
    outcome

    -   Explicitly accounts for models of outcome variance *and*
        within-cluster association (components 2.1 and 2.2)
        
        $$V_i = sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]$$
        
    -   Called "working" to distinguish it from the *true* covariance matrix of $\vec{Y}_i$

------------------------------------------------------------------------

## Generalized estimating equations

Generalized estimating equations (GEEs) find the $\beta$ that solves an
estimating equation of the general form:
$$\sum_{i=1}^N \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1}[y_{ij} - \mu_{ij}(\vec{\beta})] = 0$$

Since this estimating equation [only depends on the mean $\mu_{ij}(\cdot)$ and the working (co)variance
$V_i$]{.alert}, a GEE doesn't have to correspond to a full likelihood/distribution like a GLMM does

- This means we have [fewer assumptions]{.alert} about how $Y_{ij}$ behaves

-   Also lets us "mix and match" different forms for the marginal
    model components into this one estimating equation

   -   e.g., we don't need to theoretically derive the covariance - we can
    just approximate it using the variance and the chosen correlation
    structure
    
------------------------------------------------------------------------

## Generalized estimating equations

Generalized estimating equations (GEEs) find the $\beta$ that solves an
estimating equation of the general form:
$$\sum_{i=1}^N \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1}[y_{ij} - \mu_{ij}(\vec{\beta})] = 0$$

Since this estimating equation [only depends on the mean $\mu_{ij}(\cdot)$ and the working (co)variance
$V_i$]{.alert}, a GEE doesn't have to correspond to a full likelihood/distribution like a GLMM does

[What's the effect of this?]{.alert}

. . .

- Estimation of model parameters no longer done by maximum likelihood or REML, but by an iterative algorithm (like Newton-Raphson)

. . .

- We can't answer questions about cluster-specific trajectories like we can with GLMMs

. . .

- Fewer assumptions means results will be [more robust]{.alert} to incorrect assumptions about the marginal model components (e.g., wrong correlation structure, wrong variance) 🎉

. . .

- Also means we [need more data]{.alert} (specifically, more clusters) to run this model 🙁

---

## GLMMs vs GEEs

We noted that GEEs are *marginal* models while GLMMS are *conditional*
models...

[What are some other differences between GEEs and GLMMs?]{.alert}

. . .

|                            | GEE                                                                    | GLMM |
|----------------------------|------------------------------------------------------------------------|----------------------------------------------|
| Interpretation             | "Population"-level                                                       | Cluster-level                                |
| Accounting for correlation | Empirical estimation                                                   | Specification of random effects              |
| Assumptions                | Weak assumptions; more robust (correlation structure, over-dispersion) | Strong assumptions; less robust              |
| Sample size requirements   | Moderately large ($n > 50$)                                            | Works with smaller numbers of clusters provided that the entire model is correctly specified |
| Computational burden       | Light                                                                  | Heavy                                        |
| Cluster-level prediction?  | No                                                                     | Yes                                          |
: {tbl-colwidths="[20,40,40]"}

[Big take away:]{.alert} GLMMs and GEEs aren't so much different models, as different ways of estimating models

---

## A note on "population"-level interpretation

GEEs are often touted as having "population"-level interpretations as opposed to GLMMs cluster-level ones

- This is usually easier to think about than marginal vs conditional effects

Our estimates (and our interpretations!) are only as good as our data, though

- If the data we're using to build a GEE isn't [representative of the population]{.alert} we're interested in, it's not going to magically give us results generalizable to the population

- A more accurate description would be that GEEs give us sample-population-level interpretations while GLMMs give us sample-cluster-level interpretations

---

## GEEs: radon

Let's use our old Minnesota radon data we used back in lecture 1.2

- Recall that we want to estimate how the floor we measure on affects radon measurements in Minnesota houses, which are clustered by county

```{r radon, echo=F}
radon <- read.table("~/Desktop/teaching/PHS-651/data/Gelman-data/radon/srrs2.dat",header=T,sep=",")

set.seed(081524)

radon.mn <- radon %>% 
   as.data.frame() %>% 
   mutate(log_radon = log(activity +0.1),
          county = str_trim(county)) %>% 
   dplyr::filter(state == "MN")
```

We'd assume:

- Mean model: 

- Variance: 

- Correlation structure: 

---

## GEEs: radon

Let's use our old Minnesota radon data we used back in lecture 2.1

- Recall that we want to estimate how the floor we measure on affects radon measurements in Minnesota houses, which are clustered by county

We'd assume:

- Mean model: [$E[\text{log radon}_{ij}|\vec{X}_{ij}] = \beta_0 + \beta_1(\text{floor}_{ij})$]{.alert}\

- Variance: [$Var[Y_{ij}] = \sigma^2$]{.alert}

- Correlation structure: [exchangeable (just one correlation parameter to estimate)]{.alert}

---

## GEEs: radon

Let's run it:
```{r radon gee}
library(gee)

# run a GEE with exchangeable correlation clustered on counties #
radon_gee <- gee( log_radon ~ floor,
                  id=cntyfips, # cluster ID needs to be a number, not names or categories
                  data=radon.mn, corstr="exchangeable" )
summary( radon_gee )$coef
```

- $\beta_1$: `r round(summary(radon_gee)$coef[2,1],4)`

- Ignore the "naive" and "robust" SEs for now -- we'll talk about that more on Thursday

---

## GEEs: radon

Let's compare our GEE result with what we would get from an LME:
```{r radon lme}
library(lme4)

# run a LME with random county intercepts #
radon_lme <- lmer( log_radon ~ floor + (1|county),
                  data=radon.mn )
summary( radon_lme )$coef
```

- Exchangeable GEE $\beta_1$: `r round(summary(radon_gee)$coef[2,1],4)`

- LME $\beta_1$: `r round(summary(radon_lme)$coef[2,1],4)`

   - Not that different on point estimates!
   
   - Where we'll likely see the biggest differences is in the inference/95% CIs/standard errors... more on than Thursday

---

## Take home messages

- We add random effects to GLMMs to help [model non-0 covariance]{.alert} (correlation between observations)

<br>

- Because GLMMs are derived from the assumption that the outcome follows a specific probability distribution, it's hard to get marginal interpretations of results because it's [difficult separate out the random effects]{.alert} when we have a [non-identity link function]{.underline}

<br>

- GEEs allow us to model the mean and covariance of a regression [separately]{.alert} and in a way that doesn't have distributional assumptions, so we can easily get the [population-averaged interpretation]{.alert}

<br>

- GEEs tend to be more robust to breaking assumptions, but we [need more clusters]{.alert} to pay for that flexibility