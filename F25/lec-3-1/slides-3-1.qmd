---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
      pdf-separate-fragments: true
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```

<h1>Lecture 3.1: Marginal models for correlated data</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>September 23, 2025</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=50 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/F25/lec-3-1/slides-3-1>

:::

::: {.absolute bottom=0 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/F25/lec-3-1/slides-3-1"))
```
:::

---

## Recap: linear mixed effect models

One way to account for clustering in analysis is with a [linear mixed
effects]{.alert} model
$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

-   $\boldsymbol{X}_i\vec{\beta}$ are the [fixed effects]{.alert}
    representing the mean response

-   $\vec{Z}_ib_{0i}$ are the [random effects]{.alert} accounting for
    random between-cluster variation

-   $\vec{\epsilon}_i$ is the regular between-individual variation

- We assume the outcome observations in cluster *i*, $\vec{Y}_i$, follow a [multivariate probability distribution]{.alert}

---

## Recap: linear mixed effect models

One way to account for clustering in analysis is with a [linear mixed
effects]{.alert} model
$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

The [fixed effects]{.alert} are shared across everyone in the sample,
while the [random effects]{.alert} are unique to a particular cluster

. . .

Interpretations of $\beta_1$ in an LME are [conditional]{.alert} because
they are comparing 2 individuals with the same cluster membership

-   Often referred to as having individual-specific or cluster-specific
    interpretation

-   Possible to extract marginal effects from conditional models, but
    often complicated and prone to bias

. . .

[What if we want to look at the marginal effect of an exposure -- one that's *averaged across*
all clusters?]{.alert}

------------------------------------------------------------------------

## Marginal models

Basic idea: we want to account for clustering of data, but make
inference about [population averages]{.alert} (marginal effects)

-   We're often very interested in these in public and population health!

- Not to be confused with the average treatment effect (ATE) -- this is a different concept

-   Mean response is conditional *only* on the adjustment covariates we add
    to the mean model

. . .

<br>
Rigid distributional assumptions got in the way of this in LMEs/GLMMs

- So for marginal models we want to use [fewer distributional assumptions]{.alert}

<!-- . . . -->

<!-- - How can we account for correlation among outcome observations/non-0 covariance without the rigidity of distribution assumptions? -->

---

## R-side and G-side variance

$$g(\vec{Y}_i) = \boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_i\vec{\gamma} + \vec{\epsilon}_i$$

- Recall that with LMEs and GLMMs, we modeled:
$$\vec{\gamma} \sim N(\vec{0}, \boldsymbol{G}),~~~ \vec{\epsilon}_i \sim N(\vec{0}, \boldsymbol{R}_i=\sigma_\epsilon^2\boldsymbol{I})$$

- When using random effects (like in LMEs and GLMMs), some call this modeling with [G-side variance]{.alert}

   - These random effects $\vec{\gamma}$ were difficult to marginalize out of the expectation when we used a non-identity link function
   
   - What if instead we had *no* random effects ($\boldsymbol{G}=0$), and modeled non-zero covariance using $\vec{\epsilon}_i$ and a $\boldsymbol{R}_i \ne \sigma^2_\epsilon \boldsymbol{I}$?

. . .

- When using $\boldsymbol{R}_i \ne \sigma^2_\epsilon \boldsymbol{I}$, we call this modeling with [R-side variance]{.alert}

   - [Marginal modeling]{.alert} approaches tend to exclusively model with R-side variance
   
   - Downside: this means we're not modeling the outcome using a proper probability distribution (more on this soon...)
   

<!--    - Those assumptions focused on specifying [within-cluster and between cluster variability]{.alert}, and helped show what the covariance matrix for a cluster should look like and how we could estimate model parameters -->

<!--    - What if we focused more on the [within-cluster correlation]{.alert}, and use an [empirically estimated]{.alert} covariance structure? -->

------------------------------------------------------------------------

## Marginal model components

There are 2 main components of a marginal model

1. The [marginal expectation]{.alert} of the outcome $E[Y_{ij}|\vec{X}_{ij}]=\mu_{ij}$:
- $\mu_{ij}$ should only depend on the covariates in the model through the link function $g(\cdot)$: $g(\mu_{ij}) = \vec{X}_{ij}\vec{\beta}$

2. The covariance of the outcome $Cov[Y_{ij}, Y_{ik}|\boldsymbol{X}_i]$, which can be broken down into 2 more components:

:::: {.columns}
::: {.column width="50%"}
::: {.fragment}
i. The [marginal variance]{.alert} of the outcome:

$$Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$$

- Should depend on the marginal mean through the variance function $v(\cdot)$ and a scale parameter $\varphi$
:::
:::

::: {.column width="50%"}
::: {.fragment}
ii. The [within-cluster association]{.alert} of the outcomes

   - Should depend on the means and a correlation parameter(s)
:::
:::
::::

::: {.fragment .absolute bottom=200 right=200 width="30%"}
[Component 2 is kind of like systematic variation in GLMMs!]{.alert}
:::


---

## Marginal model components example: continuous outcomes

For example, in a model with clustered continuous responses $Y_{ij}$ and only one exposure $U_{ij}$, we might have:

1.  $E[Y_{ij}|\boldsymbol{X}_i] = \mu_{ij} = \beta_0 + U_{ij}\beta_1$

- We specify this based on our domain knowledge, hypotheses we want to test, model-building strategies, etc
    
- To account for confounding, we would add additional variables to the right hand side so that we [correctly model the mean]{.alert}

. . .

2.1.  $Var[Y_{ij}|\boldsymbol{X}] = \varphi_{i} = \sigma^2$

- Clusters share a common variance, which we assume is independent of $\mu_{ij}$

- Could also assume difference variances for each cluster, $\sigma^2_i$!

. . .

2.2.  $Corr[Y_{ij}, Y_{ik}] = \alpha$

- Outcomes in same cluster are all correlated by $\alpha$

---

## Marginal model components example: continuous outcomes

For example, in a model with clustered continuous responses $Y_{ij}$ and only one exposure $U_{ij}$, we might have:

1.  $E[Y_{ij}|\boldsymbol{X}_i] = \mu_{ij} = \beta_0 + U_{ij}\beta_1$

2.1.  $Var[Y_{ij}|\boldsymbol{X}] = \varphi_{i} = \sigma^2$

2.2.  $Corr[Y_{ij}, Y_{ik}] = \alpha$

- This means we can assume a [correlation structure]{.alert} for one cluster that looks like:

$$\begin{bmatrix}
1 & \alpha & \dots & \alpha \\
\alpha & \ddots & \dots & \alpha \\
\vdots & \dots & \ddots & \vdots\\
\alpha & \dots & \alpha & 1
\end{bmatrix}$$

---

## Marginal model components example: continuous outcomes

If we combined 2.1 and 2.2, we get the covariance between two outcomes in the same cluster:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]\\
&= \sqrt{\sigma^2}~ \alpha ~\sqrt{\sigma^2}\\
&=\sigma^2\alpha\end{align*}$$

. . .

:::: {.columns}
::: {.column width="50%"}
which creates us a [covariance matrix]{.alert} for cluster $i$, $V_i$, that looks like:

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha & \dots & \sigma^2\alpha \\
\sigma^2\alpha & \ddots & \dots & \sigma^2\alpha \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha & \dots & \sigma^2\alpha & \sigma^2
\end{bmatrix}$$
:::

::: {.column width="50%"}
::: {.fragment}
which is different than the covariance matrix we'd get from an LME:

$$\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}$$
:::
:::
::::

. . .

- We call $V_i$ the ["working" covariance]{.alert} matrix

---

## Marginal model components example: continuous outcomes

If we combined 2.1 and 2.2, we get the covariance between two outcomes in the same cluster:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]\\
&= \sqrt{\sigma^2}~ \alpha ~\sqrt{\sigma^2}\\
&=\sigma^2\alpha\end{align*}$$

- Outcomes for individuals in **different** clusters would still have a covariance of 0 though because we're assuming that [clusters are independent of one another]{.alert}

---

## Marginal model components example: continuous outcomes

If we combined 2.1 and 2.2, we get the covariance between two outcomes in the same cluster:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]\\
&= \sqrt{\sigma^2}~ \alpha ~\sqrt{\sigma^2}\\
&=\sigma^2\alpha\end{align*}$$

:::: {.columns}
::: {.column width="50%"}
which creates us a [covariance matrix]{.alert} for cluster $i$ that looks like:

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha & \dots & \sigma^2\alpha \\
\sigma^2\alpha & \ddots & \dots & \sigma^2\alpha \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha & \dots & \sigma^2\alpha & \sigma^2
\end{bmatrix}$$
:::

::: {.column width="50%"}
which is different than the covariance matrix we'd get from an LME:

$$\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}$$
:::
::::


[Can we assume other types of correlation/covariance structures?]{.alert} [Yes!]{.fragment}

------------------------------------------------------------------------

## Correlation structures: independence

The most basic (and the one we have the most prior experience with) is the [independence]{.alert} correlation structure

:::: {.columns}
::: {.column width="50%"}
- Assumes all observations in a cluster are completely uncorrelated with each other

$$\begin{bmatrix}
1 & 0 & \dots & 0 \\
0 & \ddots & \dots & 0 \\
\vdots & \dots & \ddots & \vdots\\
0 & \dots & 0 & 1
\end{bmatrix}$$

::: {.fragment}
- We don't have to estimate anything here!
:::
:::

::: {.column width="50%"}
::: {.fragment}
Assuming $Var[Y_{ij}|\boldsymbol{X}] = \sigma^2$, creates a covariance matrix that looks like

$$V_i=\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \ddots & \dots & 0 \\
\vdots & \dots & \ddots & \vdots\\
0 & \dots & 0 & \sigma^2
\end{bmatrix}$$
:::
:::

::::
------------------------------------------------------------------------

## Correlation structures: exchangeable

Another type is the [exchangeable]{.alert} correlation structure (AKA: compound symmetry)

:::: {.columns}
::: {.column width="50%"}
- Assumes all observations in a cluster are related to each other to the same degree

$$\begin{bmatrix}
1 & \alpha & \dots & \alpha \\
\alpha & \ddots & \dots & \alpha \\
\vdots & \dots & \ddots & \vdots\\
\alpha & \dots & \alpha & 1
\end{bmatrix}$$

::: {.fragment}
- Need to estimate $\alpha$
:::
:::

::: {.column width="50%"}
::: {.fragment}
Assuming $Var[Y_{ij}|\boldsymbol{X}] = \sigma^2$, creates a covariance matrix that looks like

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha & \dots & \sigma^2\alpha \\
\sigma^2\alpha & \ddots & \dots & \sigma^2\alpha \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha & \dots & \sigma^2\alpha & \sigma^2
\end{bmatrix}$$
:::
:::

::::

------------------------------------------------------------------------

## Correlation structures: unstructured

Could you assume all observations are (potentially) related to each other in
different ways?

:::: {.columns}
::: {.column width="50%"}
- Known as an [unstructured]{.alert} correlation structure

$$\begin{bmatrix}
1 & \alpha_{12} & \dots & \alpha_{1n} \\
\alpha_{21} & \ddots & \dots & \alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\alpha_{n1} & \dots & \alpha_{n(n-1)} & 1
\end{bmatrix}$$
:::

::: {.column width="50%"}
::: {.fragment fragment-index=1}
Assuming $Var[Y_{ij}|\boldsymbol{X}] = \sigma^2$, creates a covariance matrix that looks like

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha_{12} & \dots & \sigma^2\alpha_{1n} \\
\sigma^2\alpha_{21} & \ddots & \dots & \sigma^2\alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha_{n1} & \dots & \sigma^2\alpha_{n(n-1)} & \sigma^2
\end{bmatrix}$$
:::
:::

::::

::: {.fragment fragment-index=2}
- Offers maximum flexibility because correlations can be the same, very similar, or completely different from each other
:::

::: {.fragment fragment-index=3}
- But also requires you to estimate *every* correlation parameter in this matrix separately

   - This can cause computational and efficiency issues
:::

------------------------------------------------------------------------

## Correlation structures: unstructured

Could you assume all observations are (potentially) related to each other in
different ways?

:::: {.columns}
::: {.column width="50%"}
- Known as an [unstructured]{.alert} correlation structure

$$\begin{bmatrix}
1 & \alpha_{12} & \dots & \alpha_{1n} \\
\alpha_{21} & \ddots & \dots & \alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\alpha_{n1} & \dots & \alpha_{n(n-1)} & 1
\end{bmatrix}$$
:::

::: {.column width="50%"}
Assuming $Var[Y_{ij}|\boldsymbol{X}] = \sigma^2$, creates a covariance matrix that looks like

$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha_{12} & \dots & \sigma^2\alpha_{1n} \\
\sigma^2\alpha_{21} & \ddots & \dots & \sigma^2\alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha_{n1} & \dots & \sigma^2\alpha_{n(n-1)} & \sigma^2
\end{bmatrix}$$
:::

::::

- Can get away with unstructured correlation in small data sets, but usually easier to assume a simpler structure

------------------------------------------------------------------------

## Marginal model components

So we have the main components of a marginal model

1.  The marginal expectation of the outcome: $g(E[Y_{ij}| \vec{X}_{ij}]) = g(\mu_{ij}) = \vec{X}_{ij}\vec{\beta}$

2. The covariance of the outcome

   i. The marginal variance of the outcome: $Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$

   ii. The within-cluster association of the outcomes (correlation structure)

[But how do we use these components to estimate our regression coefficients?]{.alert}

. . .

- With OLS and GLMMs, we had a likelihood-based estimating equation we could use to derive estimates for $\beta$

. . .

-   What if we used something like the GLM estimating equation here?

. . .

Thus is born [generalized estimating equations]{.alert}

<!-- --- -->

<!-- ## Marginal models and estimating equations -->

<!-- We want to take the 3 components of the marginal model -->

<!-- 1. The marginal expectation of the outcome $g(E[Y_{ij}| \vec{X}_{ij}]) = g(\mu_{ij}) = \vec{X}_{ij}\vec{\beta}$ -->

<!-- 2. The marginal variance of the outcome: $Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$ -->

<!-- 3. The within-cluster association of the outcomes -->

<!-- and shove them into something that looks like the GLM estimating equation -->

<!-- . . . -->

<!-- Thus is born [generalized estimating equations]{.alert} -->

------------------------------------------------------------------------

## Generalized estimating equations

Generalized estimating equations (GEEs) find the $\beta$ that solves an
estimating equation of the general form:
$$\sum_{i=1}^N \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1}[y_{ij} - \mu_{ij}(\vec{\beta})] = 0$$

-   Almost exactly the same as the one we saw for GLMs!

. . .

-   Difference here: $V_i$ is the [working covariance]{.alert} for the
    outcome

    -   Explicitly accounts for models of outcome variance *and*
        within-cluster association (components 2.1 and 2.2)
        
        $$V_i = sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]$$
        
    -   Called "working" to distinguish it from the *true* covariance matrix of $\vec{Y}_i$

------------------------------------------------------------------------

## Generalized estimating equations

Generalized estimating equations (GEEs) find the $\beta$ that solves an
estimating equation of the general form:
$$\sum_{i=1}^N \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1}[y_{ij} - \mu_{ij}(\vec{\beta})] = 0$$

Since this estimating equation [only depends on the mean $\mu_{ij}(\cdot)$ and the working (co)variance
$V_i$]{.alert}, a GEE doesn't have to correspond to a full likelihood/distribution like a GLMM does

- This means we have [fewer assumptions]{.alert} about how $Y_{ij}$ behaves

-   Also lets us "mix and match" different forms for the marginal
    model components into this one estimating equation

   -   e.g., we don't need to theoretically derive the covariance - we can
    just approximate it using the variance and the chosen correlation
    structure
    
------------------------------------------------------------------------

## Generalized estimating equations

Generalized estimating equations (GEEs) find the $\beta$ that solves an
estimating equation of the general form:
$$\sum_{i=1}^N \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1}[y_{ij} - \mu_{ij}(\vec{\beta})] = 0$$

Since this estimating equation [only depends on the mean $\mu_{ij}(\cdot)$ and the working (co)variance
$V_i$]{.alert}, a GEE doesn't have to correspond to a full likelihood/distribution like a GLMM does

[What's the effect of this?]{.alert}

. . .

- Need to estimate mean model parameters $\beta$ and variance/covariance/correlation parameters $\varphi$, $\vec{\alpha}$

- Estimation of model parameters no longer done by maximum likelihood, but by an iterative algorithm (like Newton-Raphson)

. . .

- Fewer assumptions means results will be [more robust]{.alert} to incorrect assumptions about the marginal model components (e.g., wrong correlation structure, wrong variance) 🎉

. . .

- Also means we [need more data]{.alert} to run this model 🙁

---

## GLMMs vs GEEs

We noted that GEEs are *marginal* models while GLMMS are *conditional*
models...

. . .

[What are some other differences between GEEs and GLMMs?]{.alert}

. . .

|                            | GEE                                                                    | GLMM |
|----------------------------|------------------------------------------------------------------------|----------------------------------------------|
| Interpretation             | "Population"-level                                                       | Cluster-level                                |
| Accounting for correlation | Empirical estimation                                                   | Specification of random effects              |
| Assumptions                | Weak assumptions; more robust (correlation structure, over-dispersion) | Strong assumptions; less robust              |
| Sample size requirements   | Moderately large ($n > 50$)                                            | Works with smaller sample sizes provided that the entire model is correctly specified |
| Computational burden       | Light                                                                  | Heavy                                        |
| Cluster-level prediction?  | No                                                                     | Yes                                          |
: {tbl-colwidths="[20,40,40]"}

---

## A note on "population"-level interpretation

GEEs are often touted as having "population"-level interpretations as opposed to GLMMs cluster-level ones

- This is usually easier to think about than marginal vs conditional effects

Our estimates (and our interpretations!) are only as good as our data, though

- If the data we're using to build a GEE isn't [representative of the population]{.alert} we're interested in, it's not going to magically give us results generalizable to the population

- A more accurate description would be that GEEs give us sample-population-level interpretations while GLMMs give us sample-cluster-level interpretations