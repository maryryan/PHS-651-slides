---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```

<h1>Lecture 3.1: Inference for GEEs</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>September 25, 2025</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=50 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/F25/lec-3-2/slides-3-2>

:::

::: {.absolute bottom=0 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/F25/lec-3-2/slides-3-2"))
```
:::

---
## Inference for $\widehat{\beta}$

We also need to be able to get variance estimates for our model parameters in order to perform inference

- The "model-based" covariance matrix for $\widehat{\beta}$ looks like:

$$Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}$$

   - $\left(\frac{\partial \mu_i}{\partial \beta}\right)^T$ deals with the transformation of $Y_{ij}$ to $g(\mu_{ij})$, and links in the $\beta$s
   
   - $V_i^{-1}$ incorporates the marginal outcome variance and the outcome correlation
   
. . .

The individual variances for each $\beta$ will be found along the diagonal of the covariance matrix:

- $Var[\widehat{\beta}] = \text{diag}\left(Cov[\widehat{\beta}]\right)$
   
------------------------------------------------------------------------

## Inference for $\widehat{\beta}$

We also need to be able to get variance estimates for our model parameters in order to perform inference

- The "model-based" covariance matrix for $\widehat{\beta}$ looks like:

$$Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}$$

   - $\left(\frac{\partial \mu_i}{\partial \beta}\right)^T$ deals with the transformation of $Y_{ij}$ to $g(\mu_{ij})$, and links in the $\beta$s
   
   - $V_i^{-1}$ incorporates the marginal outcome variance and the outcome correlation
   
[What would this look like for OLS?]{.alert}

. . .

$$\begin{align*}&\frac{\partial \mu_i}{\partial \beta} = \frac{\partial \boldsymbol{X}\vec{\beta}}{\partial \beta} = \boldsymbol{X}\\
&V_i = \sigma^2\\
&Cov[\widehat{\beta}] = [\boldsymbol{X}^T (\sigma^2)^{-1} \boldsymbol{X}]^{-1} = \sigma^2 (\boldsymbol{X}^T\boldsymbol{X})^{-1}\end{align*}$$

---

## Inference for $\widehat{\beta}$

We also need to be able to get variance estimates for our model parameters in order to perform inference

- The "model-based" covariance matrix for $\widehat{\beta}$ looks like:

$$Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}$$

The confidence intervals for $\vec{\beta}$ will take the form:

$$\begin{align*}&\widehat{\beta} \pm t_{\alpha/2; DoF=n-1} \text{diag}\left(\left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}\right)\\
&\rightarrow \widehat{\beta} \pm t_{\alpha/2; DoF=n-1} \text{diag}\left(Cov[\widehat{\beta}]\right)\end{align*}$$

---
   
## Cluster-robust standard errors

Another way to account for non-independent/clustered observations in inference is to use what we call a [robust or sandwich variance]{.alert} estimator

- We call this a [post-hoc]{.alert} correction, because it happens *after* we model our data

- We can use this correction, technically, on the SEs of any type of model

The sandwich variance estimator on an OLS model would look like:

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$

where
$$\begin{align*}&B^{-1} = \left[\text{model-based covariance of }\beta\right]^{-1}= \left[\sigma^2\boldsymbol{X}^T\boldsymbol{X}\right]^{-1}\\
&A = \left[\boldsymbol{X}^T (\vec{Y} - \hat{\vec{\mu}})(\vec{Y} - \hat{\vec{\mu}})^T \boldsymbol{X}\right]\end{align*}$$

- $(\vec{Y} - \hat{\vec{\mu}})(\vec{Y} - \hat{\vec{\mu}})^T$ is the [empirical]{.alert} covariance of the outcome

---

## Cluster-robust standard errors

The sandwich variance estimator on an OLS model would look like:

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$

where
$$\begin{align*}&B^{-1} = \left[\text{model-based covariance of }\beta\right]^{-1}= \left[\sigma^2\boldsymbol{X}^T\boldsymbol{X}\right]^{-1}\\
&A = \left[\boldsymbol{X}^T (\vec{Y} - \hat{\vec{\mu}})(\vec{Y} - \hat{\vec{\mu}})^T \boldsymbol{X}\right]\end{align*}$$

If there is no clustering in the outcomes, the covariance between observations, $A \approx B$ and:
$$\begin{align*}Cov^*[\widehat{\beta}] &= B^{-1} A B^{-1}\\
&\approx B^{-1} B B^{-1}\\
& = B^{-1} = Cov[\widehat{\beta}]\end{align*}$$
the robust variance is the same as the model-based variance!

::: {.fragment}
![](clustering-SE-meme){.absolute top=75 right=-50 width="25%"}
:::


---

## Robust inference for $\widehat{\beta}$

We can also use a sandwich variance estimator to perform inference on $\widehat{\beta}$ from a GEE model

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$
where
$$\begin{align*}&B^{-1} = Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}\\
&A = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]\end{align*}$$
   
. . .

This will give us (approximately) correct variance estimates for $\widehat{\beta}$ if we're [wrong]{.alert} about the correlation structure or marginal variance (model components 2.1 and 2.2)

---

## Robust inference for $\widehat{\beta}$

We can also use a sandwich variance estimator to perform inference on $\widehat{\beta}$ from a GEE model

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$
where
$$\begin{align*}&B^{-1} = Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}\\
&A = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]\end{align*}$$
   
If we're [right]{.alert} about model components 2.1 and 2.2, $A \approx B$ and:
$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1} \approx B^{-1} B B^{-1} = B^{-1} = Cov[\widehat{\beta}]$$
the robust variance is the same as the model-based variance!

---

## Robust inference for $\widehat{\beta}$

We can also use a sandwich variance estimator to perform inference on $\widehat{\beta}$ from a GEE model

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$
where
$$\begin{align*}&B^{-1} = Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}\\
&A = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]\end{align*}$$
   
The confidence intervals for $\vec{\beta}$ will take about the same form, just using $Cov^*[\widehat{\beta}]$ instead of $Cov[\widehat{\beta}]$:

$$\widehat{\beta} \pm t_{\alpha/2; DoF=n-1} \text{diag}\left(Cov^*[\widehat{\beta}]\right)$$

---

## Robust inference for $\widehat{\beta}$

We can also use [robust or sandwich variance]{.alert} estimators to perform inference on $\widehat{\beta}$

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$
where
$$\begin{align*}&B^{-1} = Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}\\
&A = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]\end{align*}$$

Most people will use the sandwich variance estimator over the model-based variance by default to cover their bases

- Should note that robustness of the sandwich variance isn't guaranteed in scenarios when # clusters is small, cluster sizes are highly variable (see *Applied Longitudinal Analysis* pg. 359-361)

---

## GEEs: radon

Let's see how we'd fit a GEE using our Minnesota radon data

- Recall that we want to estimate how the floor we measure on affects radon measurements in Minnesota houses, which are clustered by county

```{r radon, echo=F}
radon <- read.table("~/Desktop/teaching/PHS-651/data/Gelman-data/radon/srrs2.dat",header=T,sep=",")

set.seed(081524)

radon.mn <- radon %>% 
   as.data.frame() %>% 
   mutate(log_radon = log(activity +0.1),
          county = str_trim(county)) %>% 
   dplyr::filter(state == "MN")
```

We'd assume:

- Mean model: 

- Variance: 

- Correlation structure: 

---

## GEEs: radon

Let's see how we'd fit a GEE using our Minnesota radon data

- Recall that we want to estimate how the floor we measure on affects radon measurements in Minnesota houses, which are clustered by county

We'd assume:

- Mean model: $E[\text{log radon}_{ij}|\vec{X}_{ij}] = \beta_0 + \beta_1(\text{floor}_{ij})$\

- Variance: $Var[Y_{ij}] = \sigma^2$

- Correlation structure: exchangeable (just one correlation parameter to estimate)

. . .

Let's run it!

---

## GEEs: radon

Let's run it:
```{r radon gee}
library(gee)

# run a GEE with exchangeable correlation clustered on counties #
radon_gee <- gee( log_radon ~ floor,
                  id=cntyfips, # cluster ID needs to be a number, not names or categories
                  data=radon.mn, corstr="exchangeable" )
summary( radon_gee )$coef
```

- $\beta_1$: `r round(summary(radon_gee)$coef[2,1],4)`

. . .

[What about our inference?]{.alert}

::: {.fragment}
Two SEs: model-based/naive; robust
:::

::: {.fragment}
- Naive 95% CI: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,2]*qnorm(0.975), 4)`)

- Robust 95% CI: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,4]*qnorm(0.975), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,4]*qnorm(0.975), 4)`)
:::

---

## A note on "population"-level interpretation

GEEs are often touted as having "population"-level interpretations as opposed to GLMMs cluster-level ones

- This is usually easier to think about than marginal vs conditional effects

Our estimates (and our interpretations!) are only as good as our data, though

- If the data we're using to build a GEE isn't [representative of the population]{.alert} we're interested in, it's not going to magically give us results generalizable to the population

- A more accurate description would be that GEEs give us sample-population-level interpretations while GLMMs give us sample-cluster-level interpretations
