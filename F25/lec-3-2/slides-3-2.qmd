---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1200
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
      pdf-separate-fragments: true
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```

<h1>Lecture 3.2: Inference for GEEs</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>September 25, 2025</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=225 left=300 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/F25/lec-3-2/slides-3-2>

:::

::: {.absolute bottom=120 left=50 width=250}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/F25/lec-3-2/slides-3-2"))
```
:::

---

## Roadmap

1. Recap of marginal models/GEEs

2. Model-based inference for regression coefficients from GEEs (i.e., confidence intervals)

3. Example

4. Robust inference for regression coefficients from GEEs (intro to robust/sandwich variance)

5. Example

---

## Recap: marginal models

Last class we introduced the idea of [marginal models]{.alert}

- Models where interpretation of regression coefficients $\beta$ depend [only]{.underline} on the other covariates/predictors in the model

   - $E[Y_{ij}|\vec{X}_{ij}]$

   - Interpretation does not depend on random effects
   
   - Comparing outcomes of 2 people with different exposures [regardless of cluster]{.alert}
   
   - This is [different]{.underline} than conditional models like LMEs/GLMMs, where you are comparing outcomes of 2 people [in the same cluster]{.alert} (or in clusters that have the exact same $b_{0i}$ value)
      
      - $E[Y_{ij}| \vec{X}_{ij}, b_{0i}]$
   
<br>

- This is important because when we transform the outcome (like in logistic or log/Poisson regression), the [conditional and marginal effects/interpretations are not the same]{.alert}

   - $E[Y_{ij}|\vec{X}_{ij}] \ne E[Y_{ij}| \vec{X}_{ij}, b_{0i}]$

---

## Recap: marginal models

Marginal models have 2 components (or 3, depending on how you count) that we think of as specifying somewhat separately

1. The marginal expectation of the outcome (AKA: the mean model): $g(E[Y_{ij}| \vec{X}_{ij}]) = g(\mu_{ij}) = \beta_0 + U_{ij}\beta_1$

   - Need:
      
      - (1) link function $g(\cdot)$
      
      - (2) specify covariates/predictors and how you want to model them (linear, splines, etc.)

2. The covariance of the outcome

   i. The marginal variance of the outcome: $Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$

   ii. The within-cluster association of the outcomes (how we want to represent the correlation structure)
   
- We use these to estimate our regression coefficients using [generalized estimating equations]{.alert}

------------------------------------------------------------------------

## Recap: types of correlation structures

Last class we talked about 3 main types of correlation structures

- Independence: assumes all observations in a cluster are completely uncorrelated with each other

:::: {.columns}

::: {.column width="50%"}
$$\text{Correlation structure: }\begin{bmatrix}
1 & 0 & \dots & 0 \\
0 & \ddots & \dots & 0 \\
\vdots & \dots & \ddots & \vdots\\
0 & \dots & 0 & 1
\end{bmatrix}$$
:::

::: {.column width="50%"}
$$\text{Working covariance matrix }V_i=\begin{bmatrix}
\sigma^2 & 0 & \dots & 0 \\
0 & \ddots & \dots & 0 \\
\vdots & \dots & \ddots & \vdots\\
0 & \dots & 0 & \sigma^2
\end{bmatrix}$$
:::

::::

------------------------------------------------------------------------

## Recap: types of correlation structures

Last class we talked about 3 main types of correlation structures

- Exchangeable: assumes all observations in a cluster are related/correlated to each other to the same degree ($\alpha$)

:::: {.columns}

::: {.column width="50%"}
$$\begin{bmatrix}
1 & \alpha & \dots & \alpha \\
\alpha & \ddots & \dots & \alpha \\
\vdots & \dots & \ddots & \vdots\\
\alpha & \dots & \alpha & 1
\end{bmatrix}$$
:::

::: {.column width="50%"}
$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha & \dots & \sigma^2\alpha \\
\sigma^2\alpha & \ddots & \dots & \sigma^2\alpha \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha & \dots & \sigma^2\alpha & \sigma^2
\end{bmatrix}$$
:::

::::

------------------------------------------------------------------------

## Recap: types of correlation structures

Last class we talked about 3 main types of correlation structures

- Unstructured: no assumptions about correlation of 2 outcomes in same cluster; max flexibility, but inefficient since need to estimate many correlations

:::: {.columns}

::: {.column width="50%"}
$$\begin{bmatrix}
1 & \alpha_{12} & \dots & \alpha_{1n} \\
\alpha_{21} & \ddots & \dots & \alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\alpha_{n1} & \dots & \alpha_{n(n-1)} & 1
\end{bmatrix}$$
:::

::: {.column width="50%"}
$$V_i=\begin{bmatrix}
\sigma^2 & \sigma^2\alpha_{12} & \dots & \sigma^2\alpha_{1n} \\
\sigma^2\alpha_{21} & \ddots & \dots & \sigma^2\alpha_{2n} \\
\vdots & \dots & \ddots & \vdots\\
\sigma^2\alpha_{n1} & \dots & \sigma^2\alpha_{n(n-1)} & \sigma^2
\end{bmatrix}$$
:::

::::

---

## Recap: generalized estimating equations

Generalized estimating equations (GEEs) find the $\beta$ that solves an estimating equation of the general form:
$$\sum_{i=1}^N \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1}[y_{ij} - \mu_{ij}(\vec{\beta})] = 0$$

$V_i = sd[Y_{ij}|\boldsymbol{X}] ~Corr[Y_{ij}, Y_{ik}] ~sd[Y_{ij}|\boldsymbol{X}]$ is the [working covariance]{.alert} for the outcome

<br>
Since this estimating equation [only depends on the mean $\mu_{ij}(\cdot)$ and the working (co)variance
$V_i$]{.alert}, a GEE doesn't have to correspond to a full likelihood/distribution like a GLMM does

- This means we have [fewer assumptions]{.alert} about how $Y_{ij}$ behaves 
   
   - Fewer assumptions $=$ less opportunity for model misspecification (violating assumptions)

- We don't need to theoretically derive the covariance - we can just approximate it using the variance and the chosen correlation structure

---

## Recap: generalized estimating equations

Generalized estimating equations (GEEs) find the $\beta$ that solves an estimating equation of the general form:
$$\sum_{i=1}^N \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1}[y_{ij} - \mu_{ij}(\vec{\beta})] = 0$$

Okay great we can use GEEs to get population-level interpretations of regression coefficients while accounting for non-0 covariance... [but how do we perform inference?]{.alert}


---

## Inference for $\widehat{\beta}$

The "model-based" covariance matrix for $\widehat{\beta}$ looks like:

$$Cov[\widehat{\vec{\beta}}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}$$

   - $\left(\frac{\partial \mu_i}{\partial \beta}\right)^T$ deals with the transformation of $Y_{ij}$ to $g(\mu_{ij})$, and links in the $\beta$s
   
   - $V_i^{-1}$ incorporates the marginal outcome variance and the outcome correlation
   
. . .

The individual variances for each $\beta$ will be found along the diagonal of the covariance matrix:

- $Var[\widehat{\vec{\beta}}] = \text{diag}\left(Cov[\widehat{\vec{\beta}}]\right)$


For a model with one exposure, $E[Y_{ij}] = \beta_0 + U_{ij}\beta_1$, the covariance matrix looks like:

$$\begin{bmatrix}Var[\widehat{\beta_0}] & Cov[\widehat{\beta_0},\widehat{\beta_1}]\\
Cov[\widehat{\beta_0},\widehat{\beta_1}] & Var[\widehat{\beta_1}]\end{bmatrix}$$
   
------------------------------------------------------------------------

## Inference for $\widehat{\beta}$

The "model-based" covariance matrix for $\widehat{\beta}$ looks like:

$$Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}$$

   - $\left(\frac{\partial \mu_i}{\partial \beta}\right)^T$ deals with the transformation of $Y_{ij}$ to $g(\mu_{ij})$, and links in the $\beta$s
   
   - $V_i^{-1}$ incorporates the marginal outcome variance and the outcome correlation
   
[What would this look like for OLS?]{.alert}

. . .

$$\begin{align*}&\frac{\partial \mu_i}{\partial \beta} = \frac{\partial \boldsymbol{X}\vec{\beta}}{\partial \beta} = \boldsymbol{X}\\
&V_i = \sigma^2\\
&Cov[\widehat{\beta}] = [\boldsymbol{X}^T (\sigma^2)^{-1} \boldsymbol{X}]^{-1} = \sigma^2 (\boldsymbol{X}^T\boldsymbol{X})^{-1}\end{align*}$$

---

## Inference for $\widehat{\beta}$

The confidence interval for a single $\widehat{\beta}$ (like $\widehat\beta_1$) will take the form:

$$\begin{align*}&\widehat{\beta} \pm Z_{1-\alpha/2} \text{diag}\left(\left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \beta}\right)\right]^{-1}\right)\\
&\rightarrow \widehat{\beta} \pm Z_{1-\alpha/2} \text{diag}\left(Cov[\widehat{\beta}]\right)\\
&\rightarrow \widehat\beta \pm Z_{1-\alpha/2} Var[\widehat\beta]\end{align*}$$

. . .

- If we'd like to use a $t$-distribution critical value instead of a standard Normal $Z$ critical value, approximate the [degrees of freedom as $n-p$]{.alert}, or $\text{(the number of clusters)} - \text{(the number of parameters you're estimating})$ (for CIs, usually $p=1$)

   - We [do not]{.alert} use the Satterthwaite or Kenward-Roger degrees of freedom approximation -- these were developed for LMEs/GLMMs and do not translate over to GEEs

. . .

Let's see this in an example

---

## GEEs: radon

Let's use our old Minnesota radon data we used back in lecture 1.2

- Recall that we want to estimate how the floor we measure on ($U_{ij}$) affects log radon measurements ($Y_{ij}$) in Minnesota houses, which are clustered by county

```{r radon, echo=F}
radon <- read.table("~/Desktop/teaching/PHS-651/data/Gelman-data/radon/srrs2.dat",header=T,sep=",")

set.seed(081524)

radon.mn <- radon %>% 
   as.data.frame() %>% 
   mutate(log_radon = log(activity +0.1),
          county = str_trim(county)) %>% 
   dplyr::filter(state == "MN")
```

Let's assume:

- Mean model: 

- Variance: 

- Correlation structure: 

---

## GEEs: radon

Let's use our old Minnesota radon data we used back in lecture 2.1

- Recall that we want to estimate how the floor we measure on ($U_{ij}$) affects log radon measurements ($Y_{ij}$) in Minnesota houses, which are clustered by county

Let's assume:

- Mean model: [$E[\text{log radon}_{ij}|\vec{X}_{ij}] = \beta_0 + \beta_1(\text{floor}_{ij})$]{.alert} (multiple houses $j$ observed in county $i$)

- Variance: [$Var[Y_{ij}] = \sigma^2$]{.alert}

- Correlation structure: [exchangeable (just one correlation parameter to estimate)]{.alert}

. . .

Let's run it!

---

## GEEs: radon

Let's run it:
```{r radon gee}
library(gee)

# run a GEE with exchangeable correlation clustered on counties #
radon_gee <- gee( log_radon ~ floor,
                  id=cntyfips, # cluster ID needs to be a number, not names or categories
                  data=radon.mn, corstr="exchangeable" )
summary( radon_gee )$coef
```

- $\beta_1$: `r round(summary(radon_gee)$coef[2,1],4)`

. . .

[What about our inference?]{.alert}

::: {.fragment}
- Model-based/"naive" 95% CI with a standard Normal $Z$ critical value: $\beta_{floor} \pm Z_{0.975}\times SE_{floor}=$ (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,2]*qnorm(0.975), 4)`)
:::

::: {.fragment}
- Model-based/"naive" 95% CI with a $t$ critical value and $df=85-1=84$: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,2]*qt(0.975,84), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,2]*qt(0.975,84), 4)`)
:::



# [But what if we're wrong about our choice of working correlation structure?]{.alert} {.center}

---
   
## Cluster-robust standard errors

Another way to account for non-independent/clustered observations in inference is to use what we call a [robust or sandwich variance]{.alert} estimator

- We call this a [post-hoc]{.alert} correction, because it happens *after* we model our data

- We can use this correction, technically, on the SEs of any type of model

. . .

The sandwich variance estimator on an OLS model would look like:

$$Cov^*[\widehat{\vec{\beta}}] = B^{-1} A B^{-1}$$

where
$$\begin{align*}&B^{-1} = \left[\text{model-based covariance of }\vec{\beta}\right]= \left[\sigma^2\boldsymbol{X}^T\boldsymbol{X}\right]^{-1}\\
&A = \left[\boldsymbol{X}^T (\vec{Y} - \hat{\vec{\mu}})(\vec{Y} - \hat{\vec{\mu}})^T \boldsymbol{X}\right]\end{align*}$$

- $(\vec{Y} - \hat{\vec{\mu}})(\vec{Y} - \hat{\vec{\mu}})^T$ is the [empirical]{.alert} covariance of the outcome (squared residuals)

---

## Cluster-robust standard errors

The sandwich variance estimator on an OLS model would look like:

$$Cov^*[\widehat{\vec{\beta}}] = B^{-1} A B^{-1}$$

where
$$\begin{align*}&B^{-1} = \left[\text{model-based covariance of }\vec{\beta}\right]= \left[\sigma^2\boldsymbol{X}^T\boldsymbol{X}\right]^{-1}\\
&A = \left[\boldsymbol{X}^T (\vec{Y} - \hat{\vec{\mu}})(\vec{Y} - \hat{\vec{\mu}})^T \boldsymbol{X}\right]\end{align*}$$

If there is [no clustering]{.alert} in the outcomes, the covariance between observations, $A \approx B$ and:
$$\begin{align*}\text{(Robust covariance) }Cov^*[\widehat{\vec{\beta}}] &= B^{-1} A B^{-1}\\
&\approx B^{-1} B B^{-1}\\
& = B^{-1} = Cov[\widehat{\vec{\beta}}] \text{(Model-based covariance)}\end{align*}$$
the robust covariance is the same as the model-based covariance!

---

## Cluster-robust standard errors

The sandwich variance estimator on an OLS model would look like:

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$

where
$$\begin{align*}&B^{-1} = \left[\text{model-based covariance of }\vec{\beta}\right]= \left[\sigma^2\boldsymbol{X}^T\boldsymbol{X}\right]^{-1}\\
&A = \left[\boldsymbol{X}^T (\vec{Y} - \hat{\vec{\mu}})(\vec{Y} - \hat{\vec{\mu}})^T \boldsymbol{X}\right]\end{align*}$$

If there is clustering, then the robust variance will generally be [larger]{.alert} than the "naive" variance

![](clustering-SE-meme){.absolute top=150 right=0 width="20%"}



---

## Robust inference for $\widehat{\beta}$

We can also use a [robust/sandwich variance]{.alert} estimator to perform inference on $\widehat{\beta}$ from a GEE model

$$Cov^*[\widehat{\vec{\beta}}] = B^{-1} A B^{-1}$$
where
$$\begin{align*}&B^{-1} = \left[\text{model-based covariance of } \vec{\beta}\right] = Cov[\widehat{\vec{\beta}}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \vec{\beta}}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \vec{\beta}}\right)\right]^{-1}\\
&A = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \vec{\beta}}\right)^T V_i^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \vec{\beta}}\right)\right]\end{align*}$$
   
. . .

This will give us (approximately) correct variance estimates for $\widehat{\vec{\beta}}$ if we're [wrong]{.alert} about the correlation structure or marginal variance (model components 2.1 and 2.2)

---

## Robust inference for $\widehat{\beta}$

We can also use a [robust/sandwich variance]{.alert} estimator to perform inference on $\widehat{\beta}$ from a GEE model

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$
where
$$\begin{align*}&B^{-1} = \left[\text{model-based covariance of } \vec{\beta}\right] = Cov[\widehat{\vec{\beta}}] = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \vec{\beta}}\right)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \vec{\beta}}\right)\right]^{-1}\\
&A = \left[\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \vec{\beta}}\right)^T V_i^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T V_i^{-1} \left(\frac{\partial \mu_i}{\partial \vec{\beta}}\right)\right]\end{align*}$$
   
If we're [right]{.alert} about model components 2.1 and 2.2, $A \approx B$ and:
$$\text{(Robust covariance) } Cov^*[\widehat{\beta}] = B^{-1} A B^{-1} \approx B^{-1} B B^{-1} = B^{-1} = Cov[\widehat{\beta}] \text{ (Model-based covariance)}$$
the robust covariance is the same as the model-based covariance!

---

## Robust inference for $\widehat{\beta}$

We can also use a [robust/sandwich variance]{.alert} estimator to perform inference on $\widehat{\beta}$ from a GEE model

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$
   
The confidence intervals for $\vec{\beta}$ will take about the same form as the model-based ones we did before, but just using the robust (co)variance $Cov^*[\widehat{\vec{\beta}}]$ instead of the model-based (co)variance $Cov[\widehat{\beta}]$

- For a single $\widehat\beta$ (like $\widehat\beta_1$):

$$\widehat{\beta}_1 \pm Z_{1-\alpha/2}Var^*[\widehat{\beta}_1]\ ~~~\text{ or }~~~ \widehat{\beta_1} \pm t_{1-\alpha/2; DoF=n-1} Var^*[\widehat{\beta}_1]$$

---

## Robust inference for $\widehat{\beta}$

We can also use a [robust/sandwich variance]{.alert} estimators to perform inference on $\widehat{\beta}$

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$

Most people will use the sandwich variance estimator over the model-based variance (in GEEs) by default to cover their bases

- It can be used with LMEs/GLMMs, but by convention this generally isn't common

- Note that robustness of the sandwich variance isn't guaranteed in scenarios when # clusters is small, cluster sizes are highly variable (see *Applied Longitudinal Analysis* pg. 359-361)

---

## GEE: radon

Let's go back to our Minnesota radon example and see how using cluster-robust variance methods change the results

<br>
First, let's intentionally misspecify our correlation structure and try a GEE model with an independence correlation structure

```{r gee indep}
# run a GEE with independence correlation clustered on counties #
radon_gee_indep <- gee( log_radon ~ floor,
                  id=cntyfips,
                  data=radon.mn, corstr="independence" )
summary( radon_gee_indep )$coef
```

:::: {.columns}
::: {.column width="45%"}
::: {.fragment}
[Model-based CIs]{.alert}

- 95% CI with a standard Normal $Z$ critical value: (`r round(summary(radon_gee_indep)$coef[2,1] - summary(radon_gee_indep)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_gee_indep)$coef[2,1] + summary(radon_gee_indep)$coef[2,2]*qnorm(0.975), 4)`)


- 95% CI with a $t$ critical value and $df=84$: (`r round(summary(radon_gee_indep)$coef[2,1] - summary(radon_gee_indep)$coef[2,2]*qt(0.975,84), 4)`, `r round(summary(radon_gee_indep)$coef[2,1] + summary(radon_gee_indep)$coef[2,2]*qt(0.975,84), 4)`)
:::
:::

::: {.column width="5%"}
:::

::: {.column width="50%"}
::: {.fragment}
[Robust-based CIs]{.alert}

- 95% CI with a standard Normal $Z$ critical value: (`r round(summary(radon_gee_indep)$coef[2,1] - summary(radon_gee_indep)$coef[2,4]*qnorm(0.975), 4)`, `r round(summary(radon_gee_indep)$coef[2,1] + summary(radon_gee_indep)$coef[2,4]*qnorm(0.975), 4)`)


- 95% CI with a $t$ critical value with $df=84$: (`r round(summary(radon_gee_indep)$coef[2,1] - summary(radon_gee_indep)$coef[2,4]*qt(0.975,84), 4)`, `r round(summary(radon_gee_indep)$coef[2,1] + summary(radon_gee_indep)$coef[2,4]*qt(0.975,84), 4)`)
:::
:::
::::

---

## GEEs: radon

Now let's try robust variance estimates with our exchangeble GEE and compare

- Model-based results
```{r radon gee2}
summary( radon_gee )$coef
```

:::: {.columns}
::: {.column width="45%"}

[Model-based exchangeable CIs]{.alert}

- 95% CI with a standard Normal $Z$ critical value: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,2]*qnorm(0.975), 4)`)


- 95% CI with a $t$ critical value and $df=84$: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,2]*qt(0.975,84), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,2]*qt(0.975,84), 4)`)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
[Robust-based exchangeable CIs]{.alert}

- 95% CI with a standard Normal $Z$ critical value: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,4]*qnorm(0.975), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,4]*qnorm(0.975), 4)`)


- 95% CI with a $t$ critical value with $df=84$: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,4]*qt(0.975,84), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,4]*qt(0.975,84), 4)`)
:::
::::

---

## GEEs: radon

How do these confidence intervals differ between model-based vs robust and independence vs exchangeable?

:::: {.columns}
::: {.column width="45%"}
[Exchangeable GEE ($\beta=$`r round(summary(radon_gee)$coef[2,1],4)`)]{.alert}

[Model-based exchangeable CIs]{.underline}

- 95% CI with a standard Normal $Z$ critical value: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,2]*qnorm(0.975), 4)`)


- 95% CI with a $t$ critical value and $df=84$: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,2]*qt(0.975,84), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,2]*qt(0.975,84), 4)`)
:::

::: {.column width="5%"}
:::

::: {.column width="50%"}
<br>
[Robust-based exchangeable CIs]{.underline}

- 95% CI with a standard Normal $Z$ critical value: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,4]*qnorm(0.975), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,4]*qnorm(0.975), 4)`)


- 95% CI with a $t$ critical value with $df=84$: (`r round(summary(radon_gee)$coef[2,1] - summary(radon_gee)$coef[2,4]*qt(0.975,84), 4)`, `r round(summary(radon_gee)$coef[2,1] + summary(radon_gee)$coef[2,4]*qt(0.975,84), 4)`)
:::
::::

:::: {.columns}
::: {.column width="45%"}

[Independence GEE ($\beta=$`r round(summary(radon_gee_indep)$coef[2,1],4)`)]{.alert}

[Model-based independence CIs]{.underline}

- 95% CI with a standard Normal $Z$ critical value: (`r round(summary(radon_gee_indep)$coef[2,1] - summary(radon_gee_indep)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_gee_indep)$coef[2,1] + summary(radon_gee_indep)$coef[2,2]*qnorm(0.975), 4)`)


- 95% CI with a $t$ critical value and $df=84$: (`r round(summary(radon_gee_indep)$coef[2,1] - summary(radon_gee_indep)$coef[2,2]*qt(0.975,84), 4)`, `r round(summary(radon_gee_indep)$coef[2,1] + summary(radon_gee_indep)$coef[2,2]*qt(0.975,84), 4)`)
:::

::: {.column width="5%"}
:::

::: {.column width="50%"}
<br>
[Robust-based independence CIs]{.underline}

- 95% CI with a standard Normal $Z$ critical value: (`r round(summary(radon_gee_indep)$coef[2,1] - summary(radon_gee_indep)$coef[2,4]*qnorm(0.975), 4)`, `r round(summary(radon_gee_indep)$coef[2,1] + summary(radon_gee_indep)$coef[2,4]*qnorm(0.975), 4)`)


- 95% CI with a $t$ critical value with $df=84$: (`r round(summary(radon_gee_indep)$coef[2,1] - summary(radon_gee_indep)$coef[2,4]*qt(0.975,84), 4)`, `r round(summary(radon_gee_indep)$coef[2,1] + summary(radon_gee_indep)$coef[2,4]*qt(0.975,84), 4)`)
:::
::::

---

## Take home messages

- Inference about $\widehat\beta$ like confidence intervals can be done using either [model-based]{.alert} variance estimates or [robust/sandwich]{.alert} variance estimates
   - Formulas and interpretations for confidence intervals are the same in either case, you're just switching out which variance estimate you're using

<br>

- Inference using robust variance estimates should fix any misspecification we have about the covariance part of our model
   - It won't fix any misspecification we have in our mean model, though!

<br>
   
- Robust variance estimates can be used with any type of model
   - Very common to use with GEEs
   - Uncommon to see it used with LMEs/GLMMs, but not wrong
   
- Confidence intervals for GEEs can use either standard Normal $Z$ critical values, or $t$ critical values
   - If using $t$ critical values, you usually use $df = \text{(# of clusters)} - \text{(# of regression coefficients you're performing inference on)}$ $\rightarrow$ when doing inference on one $\beta$, this usually means $df= \text{(# of clusters)} - 1$