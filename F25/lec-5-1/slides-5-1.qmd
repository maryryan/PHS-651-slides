---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
      pdf-separate-fragments: true
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```

<h1>Lecture 5.1: Diagnostics for GLMMs and GEEs</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>October 7, 2025</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=100 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/F25/lec-5-1/slides-5-1>

:::

::: {.absolute bottom=50 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/F25/lec-5-1/slides-5-1"))
```
:::

---

## Roadmap

- Modeling choices/assumptions for LMEs/GLMMs and GEEs

- Marginal and conditional residual analysis for LMEs/GLMMs

- Influential observations in LMEs/GLMMs/GEEs: leverage and DFBETA

- Other diagnostics for GEEs

---

## Model diagnostics

So far we've dealt with how to model correlated data

- But how do we assess whether the modeling choices we've made are sound?

. . .

There are some analyses/diagnostics we can perform to explore this

. . .

- Heavy emphasis on [explore]{.alert}

   - We know that our model likely won't perfectly reflect the true form/relationships in the data
   
   - But we should assess whether there are [major discrepancies]{.alert} between what we've assumed and what's in the data
   
   - Use these as a temperature check for whether the model needs to be adjusted, how much confidence can be placed in estimates, and whether estimates need to be presented with caveats

. . .

Also, in the non-longitudinal clustered data settings, there are not many diagnostic checks for GEE models

- This is because we aren't *assuming* much for GEEs - we're estimating most of the covariance matrix

- So we'll mostly be focusing on mixed models in this session

---

## Modeling choices

Let's review the various modeling choices we've covered so far

First, we can choose between conditional/mixed vs marginal models

- Usually this will be driven by whether we want a conditional or population-averaged interpretation of our effects

:::: {.columns}
::: {.column width="50%"}
::: {.fragment fragment-index=1}
Within conditional/mixed models, we make decisions about:

- Variable for random intercepts

- Variable(s) for fixed effects (isolating causal pathway)

- Outcome distribution

- Link function

:::
:::

::: {.column width="50%"}
::: {.fragment fragment-index=2}
Within marginal models, we make decisions about:

- How to specify the mean model/what variables to include to isolate causal pathway

- Marginal variance function

- Correlation structure

- Model-based vs robust confidence intervals

- Link function
:::
:::
::::

---

## Mixed model residual analysis

Recall the form of a LME with one exposure variable $U_{ij}$ for individual $j$ in cluster $i$:

$$\begin{align*}&\vec{Y}_i = \boldsymbol{X}_{i}\vec{\beta} + Z_ib_{0i} + \vec{\epsilon}_{i}\\
&Y_{ij} = \beta_0 + U_{ij}\beta_1 + Z_ib_{0i} + \epsilon_{ij}\end{align*}$$

[Residuals]{.alert} are the difference between the observed outcomes and the ones predicted by your model

$$r_{ij} = Y_{ij} - \vec{X}_{ij}\widehat{\vec{\beta}}$$

- In mixed models, we refer to $r_{ij}$ as the [marginal residuals]{.alert} (only subtracting the fixed effects)

. . .

In mixed models, we [also]{.underline} have [conditional residuals]{.alert} (subtracting marginal and random effects):
$$r^c_{ij} = Y_{ij} - \left(\vec{X}_{ij}\widehat{\vec{\beta}} + Z_i \hat{b}_{i0}\right)$$

---

## Mixed model residual analysis

$$\begin{align*}&\vec{Y}_i = \boldsymbol{X}_{i}\vec{\beta} + Z_ib_{0i} + \vec{\epsilon}_{i}\\
&Y_{ij} = \beta_0 + U_{ij}\beta_1 + Z_ib_{0i} + \epsilon_{ij}\end{align*}$$

$$\text{Marginal residuals: }r_{ij} = Y_{ij} - \vec{X}_{ij}\widehat{\vec{\beta}}$$

$$\text{Conditional residuals: }r^c_{ij} = Y_{ij} - \left(\vec{X}_{ij}\widehat{\vec{\beta}} + Z_i \hat{b}_{i0}\right)$$

Residuals tell us about how well the model fits our actual data

- We'd ideally like these to be small, but often our goal isn't to *exactly* predict our outcome variable (recall the Harrell reading from Week 1!)

- If the model [adequately captures]{.underline} the systematic trend of the response, then [the residuals should exhibit no systematic pattern]{.underline}

   - i.e., they should be scattered fairly randomly around zero

The marginal and conditional residuals will tell us different things about where we might be seeing misspecification/issues with model assumptions

---

## Marginal residuals vs predicted/fitted response

[Marginal]{.alert} residuals can be used to check for systematic departures from the model for the [mean response]{.alert}

- We often do this by making a scatter plot of marginal residuals vs marginal predicted values

   - Fitting a smoothed curve (e.g., lowess) through the scatter plot can help visualize any trends

![](residual-plot-example.png){width=1000}

::: {.absolute right=0 bottom=250 width="45%"}
- Usually, any (major) systematic pattern in the residual indicates inadequacy of the model in some way

   - Solution: re-fit the data with a more flexible model (e.g., interactions, quadratic terms, and perhaps more covariates)
   
   - This only works for LMEs, though! Usually very hard to check functional form of covariates for logistic or log-linear GLMMs
:::

---

## Marginal residuals vs predicted response

```{r radon data clean, echo=F}
radon <- read.table("~/Desktop/teaching/PHS-651/data/Gelman-data/radon/srrs2.dat",header=T,sep=",")

set.seed(081524)

radon.mn <- radon %>% 
   as.data.frame() %>% 
   mutate(log_radon = log(activity +0.1),
          county = str_trim(county)) %>% 
   dplyr::filter(state == "MN")

library(lme4)

# run an LME with random intercepts for county #
radon_lme <- lmer(log_radon ~ floor + (1 | county), data=radon.mn)
```

We can get the marginal residuals in R using the `hlm_diag()` function:

:::: {.columns}
::: {.column width="50%"}
```{r marg resid plot}
library(HLMdiag)

# run an LME with random intercepts for county #
radon_lme <- lmer(log_radon ~ floor + (1 | county), data=radon.mn)

# get various standardized residuals #
radon_resid <- hlm_resid(radon_lme,
                         standardize=TRUE)

colnames(radon_resid)
```
:::

::: {.column width="50%"}
```{r marg resid plot2}
radon_resid %>% 
   ggplot(aes(x=.mar.fitted, y=.chol.mar.resid))+
   geom_point() +
   geom_smooth(se=F) +
   theme(text = element_text(size = 20)) 
```
:::
::::

- Since `floor` only takes on 2 values, our marginal fitted values only take on 2 values

- But generally looks like we're getting similar spread of residuals at the 2 fitted values, so no strong indication that `floor` needs to be modeled non-linearly

---

## Conditional residuals vs predicted response
   
Conditional residuals can be used to check for overall model fit (mean response, use of random effects, or constant individual-level variance)

![](residual-plot-example.png){width=1000}

::: {.absolute right=0 bottom=380 width="45%"}
- Plots of [conditional]{.alert} residuals against [conditional]{.alert} fitted values will tell you about how well the data fits your assumptions about the random intercepts (zero mean), the constant $\epsilon_{ij}$ variance assumption, and whether you have outlying subjects/clusters

   - Want to be looking for things like fanning here, in addition to the curvature we look for in marginal plots
:::

---

## Conditional residuals vs predicted response

In R, we can plot the model object to get a conditional residuals plot:
```{r resid plot}
library(lme4)

# run an LME with random intercepts for county #
radon_lme <- lmer(log_radon ~ floor + (1 | county), data=radon.mn)

plot(radon_lme, smooth=T)
```

::: {.absolute right=0 top=450 width="45%"}
- Points fairly randomly distributed around 0

- Smoothed line not showing significant trends

- Constant marginal variance assumption probably met

- Mean-0 assumption of marginal variance and random intercepts probably also met
:::


---

## Conditional residuals vs predicted response

We can also use `hlm_resid()` to get a nicer-looking conditional residuals plot than base R provides

```{r cond resid plot3}
radon_resid %>% 
   ggplot(aes(x=.fitted, y=.std.resid))+
   geom_point() +
   geom_smooth(se=F, col="orange") +
   geom_hline(yintercept=0, col="blue", linetype="dashed")+
   theme(text = element_text(size = 20)) 
```

---

## Normality of conditional residuals

We can also create a QQ-plot of the (unstandardized) conditional residuals to look for major departures from the Normality assumption of the residuals

```{r cond resid qqplot}
radon_resid_raw <- hlm_resid(radon_lme)
qqnorm(radon_resid_raw$.resid)
qqline(radon_resid_raw$.resid)
```

---

## Normality of random effects

We can also create a QQ-plot of just the random effects using the `plot_ranef()` function from the `redres` library to look for major departures from the Normality of random effects assumption

```{r rand eff plot}
library(redres)

plot_ranef(radon_lme)
```
---

## Residuals vs predicted response

:::: {.columns}

::: {.column width="65%"}
In SAS, we can add the `plots=studentpanel` option to the end of `PROC GLIMMIX` to look at the studentized residuals

```{sas eval=F}
PROC GLIMMIX data=dance plots=studentpanel(marginal conditional);
   class village_id dance_class_bin(REF="0");
   model gaitfollowup = dance_class_bin / dist = normal solution cl;
   random intercept / subject=village_id;
run;
```

- Our residuals vs fitted plots would be the ones in the top left (conditional) or second-from-bottom left (marginal)

- `studentpanel` defaults to conditional residuals, but we can add `(marginal conditional)` to have SAS display both
:::

::: {.column width="35%"}
![](residualpanel_std.png){width=1000}
:::

::::

---

## Residuals vs covariates

We can follow a similar strategy for checking the functional form of covariates

1. Plot (marginal) residual vs covariate

2. If any systematic pattern, try adding a quadratic term for the covariate or transforming the covariate

```{r floor resid plot}
radon_resid %>% 
   ggplot(aes(x=floor, y=.chol.mar.resid))+
   geom_point() +
   geom_hline(yintercept=0, col="blue", linetype="dashed")+
   theme(text = element_text(size = 20)) 
```

---

## Leverage

We may also be interested in whether we have particularly influential observations in our data that are (overly) influencing our model estimates - we investigate this via "leverage" or Cook's distance plots

- [Leverage]{.alert} looks at how removing an observation/cluster affects predicted values/residuals -- it will tell us us about [extreme values in the predictor space]{.underline}

- [Cook's distance]{.alert} looks at how removing an observation/cluster affects regression coefficients when [combined]{.underline}

Observations are allowed to be influential!

- This breaks no assumptions

- But if your results look very different with and without the influential observations, it might bring the validity of your results into question

We can also see whether we have any particularly influential clusters as well

Unfortunately `PROC GLIMMIX` doesn't have functionality for influence metrics, but R does

---

## Leverage 

Observation-level leverage/Cook's D
```{r leverage, out.width="65%"}
library(ggpubr)
# looking at observation-level leverage #
radon_leverage_obs <- hlm_influence(radon_lme, leverage="overall")

ggarrange(
dotplot_diag(radon_leverage_obs$leverage.overall, cutoff="internal", name="leverage")+ ylab("Leverage"),
dotplot_diag(radon_leverage_obs$cooksd, cutoff="internal", name="cooks.distance")+ ylab("Cook's distance"),
ncol=2,nrow=1)
```

::: {.absolute right=0 bottom=200 width="30%"}
Looks like we have a lot of observations that are influential based on Cook's distance...
:::

---

## Leverage 

Cluster-level leverage/Cook's D
```{r leverage cluster, out.width="65%"}
# looking at observation-level leverage on the fixed effect estimates #
radon_leverage_cnty <- hlm_influence(radon_lme, leverage="overall", level="county")

ggarrange(
dotplot_diag(radon_leverage_cnty$leverage.overall, cutoff="internal", name="leverage"),
dotplot_diag(radon_leverage_cnty$cooksd, cutoff="internal", name="cooks.distance"),
ncol=2,nrow=1)
```

::: {.absolute right=0 bottom=200 width="30%"}
Just a few key counties that are influential based on Cook's distance...
:::

---

## DFBETA

A similar measure of influential observations is DFBETA - it's the change in a regression coefficient if the $ij^{th}$ observation were removed

- Most functions/procedures will plot DFBETAs for each coefficients in the regression

DFBETA is technically agnostic to model type - should work for both conditional and marginal models

- For SAS, `PROC GLIMMIX` doesn't have any influence statistics built in but `PROC GENMOD` does

- For R, I've had issues getting the `lmer()`, `glmer()`, and `gee()` functions to play nicely with the `dfbeta()` function

---

## DFBETA in `PROC GENMOD`

In SAS, we use the `PLOTS(UNPACK)=DFBETA` option:
```{sas DFBETA, eval=F}
proc genmod data=dance PLOTS(UNPACK)=DFBETA;
        class village_id dance_class_bin(REF="0");
        model gaitfollowup = dance_class_bin / dist = normal;
        repeated subject=village_id / type=exch;
run;
```

:::: {.columns}
::: {.column widht="60%"}
- We're looking for observations that are clearly apart from the "herd"

   - This plot looks fairly random and scattered, though

- We could also look at the standardized DFBETA by using PLOTS(UNPACK)=DFBETAS
:::
::::

::: {.absolute right=-100 bottom=50}
![](DFBETA.png){width=900}
:::

---

## Other diagnostics for GEEs

While we don't do a lot of stand-alone model testing for GEEs, we can compare the empirical estimates of the covariance structure to the model-based estimates

- For this we can compare the model-based and robust standard errors or confidence intervals of regression coefficients

- If there is a large discrepancy, our robust variance estimator is probably doing the heavy lifting of correcting this for us

   - What's a "large" difference? Hard to say -- it can be helpful to form a ratio $\frac{\text{robust SE}}{\text{model-based SE}}$ to see what percentage larger/smaller the robust standard error is compared to the model-based one

- In this setting (non-longitudinal clustered data) though, we aren't likely to see much discrepancy

---

## Take away messages

- More diagnostics available for assessing LMEs/GLMMs because we make more assumptions in those models

   - Residual plots can be useful to assess whether modeled variance/covariance correctly (constant variance, random intercepts), and sometimes to assess functional form of fixed effect covariates
   
   - I've found this tutorial of the `redres` library in R to be a helpful walkthrough of the different types of residuals and what they are useful for: <https://goodekat.github.io/redres/articles/redres-vignette.html#resids>
   
- Influential observation analyses, like looking at leverage or DFBETA, can be useful for either LMEs/GLMMs or GEEs to see if "outlier" observations are strongly influencing the fit of our model

- We can assess how well we've modeled the covarinace in GEEs by comparing the model-based standard errors to the robust standard errors

- What if we want to compare 2 different models? We'll get to this after the midterm
