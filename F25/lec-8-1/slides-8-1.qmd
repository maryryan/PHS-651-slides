---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
      pdf-separate-fragments: true
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(gridExtra)
library(qrcode)
library(corrr)
```

<h1>Lecture 8.1: Longitudinal GLMMs</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>October 27, 2025</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=200 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/F25/lec-8-1/slides-8-1>

:::

::: {.absolute bottom=150 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/F25/lec-8-1/slides-8-1"))
```
:::

---

## Roadmap

- Recap: non-longitudinal GLMMs

- Interpretation of GLMM $\beta$s when adding time as a fixed effect

- Adding random slopes to a GLMM

   - How this changes the outcome covariance matrix (math optional)
   
- Data example

   - Interpretations of $\beta$s under GLMM with random intercepts *and* slopes
   
   - Calculating simple slopes/linear constrasts
   
   - How variance/covariance changes with time
   
- Appendices

   - Other ways to account for longitudinal correlation in mixed effect-type models


---

## Concepts to practice

The last few weeks we've talked about how longitudinal data impacts our [correlation/covariance matrix]{.alert}, and the types of correlation structures that may be appropriate

- Exchangeable, Toeplitz, autoregressive, exponential, unstructured...

**But how do we apply correlation/covariance structures to a modeling framework?**

- Two ways:

   1. Adding (additional) [random effects]{.underline} to a mixed model to [induce a correlation/covariance structure -- the focus of this week]{.alert}
   
   2. [Directly applying the correlation structure to the covariance matrix in a GEE -- we covered this last week]{style="color:darkgray;"}


Extending GEEs to longitudinal data was fairly straightforward

- Few distributional assumptions means subbing in new mean model and correlation structure is easy

Due to its distributional assumptions, doing the same extension for GLMMs is slightly more involved...

---

## Recap: Non-longitudinal GLMMs

Recall the random effects model we used for non-longitudinal clustered data:
$$g(Y_{ij}) = \beta_0 + \beta_1U_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

- Where we assumed that each cluster had it's own [cluster-specific mean]{.underline} ($\beta_0 + b_{0i}$) that varied by some $b_{0i}$ around the overall sample mean $\beta_0$

- We called $b_{0i}$ the [random intercept]{.alert}

---

## Recap: Non-longitudinal GLMMs

<br>
$$g(Y_{ij}) = \beta_0 + \beta_1U_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

Including a random intercept induced an [exchangeable]{.alert} covariance matrix:
$$\begin{align*}Cov[\vec{Y}_i] &= [\text{cluster-to-cluster variation}] + [\text{within-cluster member-to-member variation}]\\
&=\begin{bmatrix}
\sigma^2_b & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b
\end{bmatrix} + \begin{bmatrix}
\sigma^2_\epsilon & 0 & \dots & 0\\
0 & \ddots & \dots & 0\\
\vdots & \dots & \ddots & \vdots\\
0& \dots & 0 & \sigma^2_\epsilon
\end{bmatrix}=\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}\end{align*}$$

- $\sigma^2_\epsilon$: within-cluster member-to-member variance

- $\sigma^2_b$: cluster-to-cluster variation

- $\sigma^2_b + \sigma^2_\epsilon$: total variance

---

## Recap: Non-longitudinal GLMMs

<br>
$$g(Y_{ij}) = \beta_0 + \beta_1U_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

We can approximate the [correlation]{.alert} between 2 outcome observations in cluster *i* via the [ICC]{.alert}: $$Corr[Y_{ij}, Y_{ik}] = \sigma^2_b/(\sigma^2_b + \sigma^2_\epsilon) = \sigma^2_b/\sigma^2= \alpha,$$ we can rewrite the covariance matrix $Cov[\vec{Y}_i]$ as:

$$[marginal variance] \times [correlation structure] = \sigma^2\begin{bmatrix}
1 & \alpha & \dots & \alpha\\
\alpha& \ddots & \dots & \alpha\\
\vdots & \dots & \ddots & \vdots\\
\alpha & \dots & \alpha & 1
\end{bmatrix},$$

which looks much more like the exchangeable covariance structure we see in GEEs!

---

## Recap: Non-longitudinal GLMMs

<br>
$$g(Y_{ij}) = \beta_0 + \beta_1U_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

:::: {.columns}
::: {.column width="50%"}
We could also express the covariance matrix using matrix notation:
$$Cov[\vec{Y}_i] = \vec{Z}_i\sigma^2_b\boldsymbol{1}\vec{Z}_i^T + \sigma^2_\epsilon \boldsymbol{I}$$

- This assumes:
$$b_{0i} \sim N(0, \sigma^2_b) ~~~~~~~ \epsilon_{ij} \sim (0, \sigma^2_{\epsilon})$$
:::

::: {.column width="50%"}
Or to write more generally:
$$Cov[\vec{Y}_i] = \vec{Z}_i\boldsymbol{G}\vec{Z}_i^T + \boldsymbol{R_i},$$
where:

- $\boldsymbol{G}$ is the covariance matrix for the random effects $b_i$

- $\boldsymbol{R}_i$ is the covariance matrix for the within-cluster individual variation $\epsilon_{ij}$

:::
::::

[With this reminder of how non-longitudinal GLMMs work, now let's try to extend them to work with longitudinal data measured over time]{.alert}

---

## Longitudinal GLMMs

Let's start by just adding a covariate for time to a model of a continuous, Normal outcome (could add exposure-by-time interaction, but focusing on main effects here for simplicity):
$$Y_{ij} = \beta_0 + \beta_1U_{i0} + \beta_2(\text{Time}_{ij}) + b_{0i}Z_i+\epsilon_{ij}$$

What is the interpretation of:

- $\beta_0$:

<br>

- $\beta_0 + b_{0i}$: 

<br>

- $\beta_1$: 

<br>

- $\beta_2$: 


---

## Longitudinal GLMMs

Let's start by just adding a covariate for time to a model of a continuous, Normal outcome (could add exposure-by-time interaction, but focusing on main effects here for simplicity):
$$Y_{ij} = \beta_0 + \beta_1U_{i0} + \beta_2(\text{Time}_{ij}) + b_{0i}Z_i+\epsilon_{ij}$$

What is the interpretation of:

- $\beta_0$: The average outcome at baseline under the unexposed/reference condition for variable $U$

- $\beta_0 + b_{0i}$: The average outcome for persion *i* at baseline under the unexposed/reference condition for variable $U$

- $\beta_1$: The average difference in outcomes between the unexposed/reference and exposed conditions, both measured at the same timepoint

- $\beta_2$: Within the same person, the average difference in outcomes that are measured one time unit apart under the same condition ([within-person rate of change]{.underline})


---

## Longitudinal GLMMs

$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2(\text{Time}_{ij}) + b_{0i}Z_i+\epsilon_{ij}$$

:::: {.columns}
::: {.column width="50%"}
The random intercept still induces an [exchangeable]{.alert} covariance matrix:
$$Var[\vec{Y}_i] =\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}$$
:::

::: {.column width="50%"}
This assumes that every member of cluster *i* has the [same relationship]{.underline} to one another ([same correlation]{.alert}): $\sigma^2_b/(\sigma^2_b + \sigma^2_\epsilon)$

<br>

- In longitudinal data, though, cluster *i* is a single person and the "members" of the cluster are observations collected at [separate time points]{.alert}

- An observation collected at time 1 may relate to observations at times 2 and 3 [differently]{.alert}
:::
::::

---

## Longitudinal GLMMs

$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2(\text{Time}_{ij}) + b_{0i}Z_i+\epsilon_{ij}$$

Inclusion of only [random intercepts]{.alert} assumes that, though everyone may randomly vary around some global baseline, everyone has the [same outcome trajectory]{.alert} over time

:::: {.columns}
::: {.column width="50%"}
![](rand-intercept.png){width="80%"}
:::

::: {.column width="50%"}
- Different issue from exposure-by-time interaction

One solution would be to allow for each cluster (person) to have [variation in its slope]{.alert} as well

- [Random slopes and intercepts!]{.alert}
:::
::::

---

## Random slope and intercept model

Consider a model with intercepts [and slopes]{.alert} that vary randomly among individuals:

$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2(\text{Time}_{ij}) + b_{0i}Z_{i0} + \color{green}{b_{1i}(\text{Time}_{ij})} + \epsilon_{ij}$$

:::: {.columns}
::: {.column width="40%"}
![](rand-slope.png)
:::

::: {.column width="60%"}
- This model suggests that individuals vary not only in their baseline level of response (when $t_{i1} = 0$), but also in terms of their changes in the response over time

<br>

- This doesn't change the interpretation of any of the $\beta$'s (should still be subject/cluster-specific), just allows for variance to come from an additional source (here, time)
:::
::::

---

## Random slope and intercept model

Consider a model with intercepts [and slopes]{.alert} that vary randomly among individuals:

$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2(\text{Time}_{ij}) + b_{0i}Z_{i0} + \color{green}{b_{1i}(\text{Time}_{ij})} + \epsilon_{ij}$$

Here we assume:

$$b_{0i} \sim N(0, \sigma^2_{b0}) ~~~~~ \color{green}{b_{1i} \sim N(0, \sigma^2_{b1})} ~~~~~ \epsilon_{ij} \sim (0, \sigma^2_\epsilon)$$


We also assume that the random intercepts and slopes covary:
$$Cov[b_{0i}, b_{1i}] = \sigma_{b0,b1}$$

- The random slopes and intercepts (may) not be independent!

---

## Random slope and intercept model: matrix notation

$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2(\text{Time}_{ij}) + b_{0i}Z_{i0} + \color{green}{b_{1i}(\text{Time}_{ij})} + \epsilon_{ij}$$

Often, we can rewrite this model more compactly in matrix notation:

$$g(\vec{Y}_{i}) = \boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_i\vec{b}_i + \vec\epsilon_{i},$$
where

$$\boldsymbol{X}_i = \begin{bmatrix}1 & U_{i0}\\
1 & U_{i1}\\
\vdots & \\
1 & U_{in}\end{bmatrix}~~~ \vec\beta = \begin{bmatrix}\beta_0\\
\beta_1\end{bmatrix} ~~~ \boldsymbol{Z}_i = \begin{bmatrix}1 & \text{Time}_{i1}\\
1 & \text{Time}_{i1}\\
\vdots \\
1 & \text{Time}_{in}\end{bmatrix} ~~~ \vec b_i = \begin{bmatrix}b_{i0}\\
b_{i1}\end{bmatrix} ~~~ \vec\epsilon_i = \begin{bmatrix}\epsilon_{i1}\\
\vdots\\
\epsilon_{in}\end{bmatrix}$$


---

## Variance/covariance of random effects

The vector of random effects, $\vec{b}_i$, are assumed to be distributed Normally with mean zero and covariance matrix $\boldsymbol{G}$ 

$$\vec{b}_i \sim N(\vec{0}, \boldsymbol{G})$$

<br>

For example, in the random intercepts and slopes model we considered earlier, we might write the covariance matrix between the random intercepts and slopes to be:
$$\boldsymbol{G}=Cov[b_{0i}, b_{1i}]=\begin{bmatrix}
\sigma^2_{b0} & \sigma_{b0,b1}\\
\sigma_{b0,b1} &  \sigma^2_{b1}\end{bmatrix}$$


---

## Variance/covariance of the outcome

What does this mean for the covariance matrix of the [outcome]{.underline}?

If we assume the within-individual errors are independent, then the [variance of a single outcome measure]{.alert} is:
$$\begin{align*}Var[Y_{ij}] &= Var[\boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_i\vec{b}_{i}+\epsilon_{ij}]\\
&= Var[\boldsymbol{Z}_i \vec{b}_{i}+\epsilon_{ij}]\\
&= Var[\vec{Z}_{1i} b_{0i} + (\text{Time}_{ij}) b_{1i} +\epsilon_{ij}]\\
&= Var[\vec{Z}_{1i} b_{0i}] + Var[(\text{Time}_{ij}) b_{1i}] + 2Cov[\vec{Z}_{1i} b_{0i}, (\text{Time}_{ij}) b_{1i}]+Var[\epsilon_{ij}]\\
&= \sigma^2_{b0} + [\text{Time}_{ij})^2\sigma^2_{b1}+ 2(\text{Time}_{ij}]\sigma_{b0,b1} + \sigma^2_{\epsilon}\end{align*}$$

[The variance of our outcome can change with time!]{.alert}

---

## Variance/covariance of the outcome

What does this mean for the covariance matrix of the [outcome]{.underline}?

If we assume the within-individual errors are independent, then the variance of a single outcome measure is:
$$Var[Y_{ij}] = \sigma^2_{b0} + (\text{Time}_{ij})^2\sigma^2_{b1}+ 2(\text{Time}_{ij})\sigma_{b0,b1} + \sigma^2_{\epsilon}$$


And the covariance between 2 outcome measures in the same cluster is:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= Cov[\boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_i\vec{b}_{i}+\epsilon_{ij}, \boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_i\vec{b}_{i}+\epsilon_{ik}]\\
&= Cov[\vec{Z}_{1i}b_{0i} +(\text{Time}_{ij})b_{1i}+\epsilon_{ij}, \vec{Z}_{1i}b_{0i}+(\text{Time}_{ik})b_{1i}+\epsilon_{ik}]\\
&\dots\\
&= \sigma^2_{b0} + [(\text{Time}_{ij}) + (\text{Time}_{ik})]\sigma_{b0,b1} + (\text{Time}_{ij})(\text{Time}_{ik})\sigma^2_{b1}\end{align*}$$

[The covariance between outcomes in a cluster also changes with time!]{.alert}

- Whether it [increases]{.underline} over time depends on whether $\sigma_{b0,b1}>0$

---

## Variance/covariance of the outcome

What does this mean for the covariance matrix of the outcome?

If we assume the within-individual errors are independent, then the variance of a single outcome measure is:
$$Var[Y_{ij}] = \sigma^2_{b0} + (\text{Time}_{ij})^2\sigma^2_{b1}+ 2(\text{Time}_{ij})\sigma_{b0,b1} + \sigma^2_{\epsilon}$$

And the covariance between 2 outcome measures in the same cluster is:
$$Cov[Y_{ij}, Y_{ik}] = \sigma^2_{b0} + ((\text{Time}_{ij}) + (\text{Time}_{ik}))\sigma_{b0,b1} + (\text{Time}_{ij})(\text{Time}_{ik})\sigma^2_{b1}$$


[The variance and covariance are both functions of time!]{.alert}

- If $\sigma_{b0,b1}=0$, then we know $Var[Y_{ij}]$ increases with time

In general, [any slope $\beta$ from our fixed effects]{.alert} can be allowed to vary randomly by simply including the corresponding covariate in random effects design matrix $\vec{Z}_{ij}$

---

## Random effects & correlation structures

When we just had random intercepts it was very clear how this created an exchangeable correlation/covariance matrix

<br>

[Do we get a similar result when we add random slopes?]{.alert}

- Kind of

<br> 

Adding random slopes induces something like an [exponential]{.alert} correlation structure

- Correlation between observations within an individual gets smaller the further apart in time the observations are

Let's see this in action with an example

---

## Example: Exercise therapy trial

Recall the exercise therapy trial from last week:

- Subjects were assigned to one of two weightlifting programs to increase muscle strength

   - Treatment 1: number of repetitions of the exercises was increased as subjects became stronger

   - Treatment 2: number of repetitions was held constant but amount of weight was increased as subjects became stronger

- Measurements of body strength were taken at baseline (0) and on days 2, 4, 6, 8, 10, and 12

[What would our GLMM model look like?]{.alert}

. . .

$$\text{body strength}_{ij} = \beta_0 + \beta_1(\text{Program}_i) + \beta_2(\text{Time}_{ij}) + \beta_3(\text{Program}_i \times \text{Time}_{ij}) + b_{0i}(\text{ID}_i) + b_{1i}(\text{Time}_{ij}) + \epsilon_{ij}$$

$$\vec{b}_i \sim N(\vec{0}, \boldsymbol{G})$$
$$\vec{\epsilon}_i \sim (\vec{0}, \boldsymbol{R}_i=\sigma^2_\epsilon\boldsymbol{I})$$

---

## Example: Exercise therapy trial

:::: {.columns}
::: {.column width="55%"}
If we run the mixed model...
```{css, echo=F}
.reveal code {
  max-height: 100% !important;
}
```
```{r exercise, echo=F}
exercise <- read.table("https://content.sph.harvard.edu/fitzmaur/ala2e/exercise-data.txt", sep="", na.strings=".")
colnames(exercise) <- c("ID", "PROGRAM", "day0","day2", "day4", "day6", "day8", "day10", "day12")

exercise_long <- exercise %>% 
   pivot_longer(!(c(ID, PROGRAM)), names_to="time", values_to="strength") %>% 
      mutate(time = case_when(time=="day0" ~ 0,
                          time=="day2" ~ 2,
                          time=="day4" ~ 4,
                          time=="day6" ~ 6,
                          time=="day8" ~ 8,
                          time=="day10" ~ 10,
                          time=="day12" ~ 12
                          ),
             time_cat=as.factor(time),
             PROGRAM = as.factor(PROGRAM),
          ID = factor(ID))

exercise_long_complete <- exercise_long %>% 
   filter(complete.cases(.))
```

```{r exercise lme, eval=F}
library(lme4)

exercise_lme <- lmer(strength~PROGRAM*time + (time | ID),
                      data = exercise_long)
summary(exercise_lme)
```
```{r exercise lme2, echo=F}
library(lme4)

exercise_lme <- lmer( strength ~ PROGRAM*time + (time | ID),
                      data = exercise_long )
exercise_lme_ci <-confint(exercise_lme)
b23_L <- c(0,0,1,1)
b13_L <- c(0,1,0,1)

b23_ci_l <- b23_L %*%summary(exercise_lme)$coef[,1]  - qnorm(0.975)*t(b23_L)%*%summary(exercise_lme)$vcov%*%b23_L
b23_ci_u <- b23_L %*%summary(exercise_lme)$coef[,1] + qnorm(0.975)*t(b23_L)%*%summary(exercise_lme)$vcov%*%b23_L

b13_ci_l <- b13_L %*%summary(exercise_lme)$coef[,1] - qnorm(0.975)*t(b13_L)%*%summary(exercise_lme)$vcov%*%b13_L
b13_ci_u <- b13_L %*%summary(exercise_lme)$coef[,1] + qnorm(0.975)*t(b13_L)%*%summary(exercise_lme)$vcov%*%b13_L
```


What's the interpretation of:

- $\beta_1$ (Program): 

<br>

- $\beta_2$ (Time): 

<br>

- $\beta_3$ (interaction): 
:::

::: {.column width="45%"}
```{r exercise lme3, echo=F}
summary(exercise_lme)
```
:::
::::

---

## Example: Exercise therapy trial

:::: {.columns}
::: {.column width="55%"}

What's the interpretation of:

- $\beta_1$ (Program): The average difference in strength between programs 2 and 1 at baseline is `r round(summary(exercise_lme)$coef[2,1],3)`. We are 95% confident the true difference is between $\widehat\beta_1 \pm 1.96\times$ `r round(summary(exercise_lme)$coef[2,2],3)` = (`r round(exercise_lme_ci[6,1], 3)`, `r round(exercise_lme_ci[6,2], 3)`).

- $\beta_2$ (Time): Within the same person in Program 1 (reference group), the average difference in strength measured 1 day apart is `r round(summary(exercise_lme)$coef[3,1],3)`. We are 95% confident the true difference is between $\widehat\beta_2 \pm 1.96\times$ `r round(summary(exercise_lme)$coef[3,2],3)` = (`r round(exercise_lme_ci[7,1], 3)`, `r round(exercise_lme_ci[7,2], 3)`).

- $\beta_3$ (interaction): Within the same person, the average difference in strength rates of change between Program 2 and Program 1 is `r round(summary(exercise_lme)$coef[4,1],3)`. We are 95% confident the true difference is between $\widehat\beta_2 \pm 1.96\times$ `r round(summary(exercise_lme)$coef[4,2],3)` = (`r round(exercise_lme_ci[8,1], 3)`, `r round(exercise_lme_ci[8,2], 3)`).
:::

::: {.column width="45%"}
```{r exercise lme4, echo=F}
summary(exercise_lme)
```
:::
::::

---

## Example: Exercise therapy trial

:::: {.columns}
::: {.column width="55%"}
What about the combinations of Program/Time and the interaction?

- $(\beta_2 + \beta_3)$: [Within the same person in Program 2, the average difference in strength measured 1 day apart is `r round(summary(exercise_lme)$coef[3,1] + summary(exercise_lme)$coef[4,1],3)`. We are 95% confident the true difference is between $\widehat\beta_2 + \widehat\beta_3 \pm 1.96\times$ `r round(t(b23_L)%*%summary(exercise_lme)$vcov%*%b23_L, 3)` = (`r round(b23_ci_l, 3)`, `r round(b23_ci_u, 3)`).]{.fragment fragment-index=1}

<br>

- $(\beta_1 + \beta_3)$: [Within the same person, the average difference in strength in Programs 2 and 1, measured on the same day is `r round(summary(exercise_lme)$coef[2,1] + summary(exercise_lme)$coef[4,1],3)`. We are 95% confident the true difference is between $\widehat\beta_2 + \widehat\beta_3 \pm 1.96\times$ `r round(t(b13_L)%*%summary(exercise_lme)$vcov%*%b13_L, 3)` = (`r round(b13_ci_l, 3)`, `r round(b13_ci_u, 3)`).]{.fragment fragment-index=1}

[Where are we getting the standard errors from? Linear contrasts!]{.alert .fragment fragment-index=2}
:::


::: {.column width="45%"}
```{r exercise lme5, echo=F}
summary(exercise_lme)
```
:::
::::

---

## Example: Exercise therapy trial

Standard error for $\widehat\beta_2 + \widehat\beta_3$:

1. Create contrast vector: $L=(0,0,1,1)$

2. Get the coefficient variance-covariance matrix:
```{r vcov}
summary(exercise_lme)$vcov
```


3. Multiply the coefficient variance-covariance matrix `summary(exercise_lme)$vcov` by the contrast vector
```{r vcov2}
b23_L <- c(0,0,1,1)

b23_SE <- t(b23_L) %*% summary(exercise_lme)$vcov %*% b23_L
b23_SE
```

---

## Example: Exercise therapy trial

We can also get estimates of the random effects covariance matrix:
```{r exercise rand cov}
VarCorr(exercise_lme)[["ID"]]
```

And use these to estimate the variance of an outcome at a particular time:
$$\begin{align*}Var[Y_{ij}] &= \sigma^2_{b0} + (\text{Time}_{ij})^2\sigma^2_{b1}+ 2(\text{Time}_{ij})\sigma_{b0,b1} + \sigma^2_{\epsilon}\\
&= 9.953 + (\text{Time}_{ij})^20.034 + 2(\text{Time}_{ij})(-0.017) + \sigma^2_\epsilon\end{align*}$$

---

## Example: Exercise therapy trial

We can also get estimates of the random effects covariance matrix:
```{r exercise rand cov2}
VarCorr(exercise_lme)[["ID"]]
```

As well as the covariance between outcomes at different times:

:::: {.columns}
::: {.column width="50%}
Between day 0 and day 2:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= \sigma^2_{b0} + ((\text{Time}_{ij}) + (\text{Time}_{ik}))\sigma_{b0,b1}\\
&~~~~~~~~+ (\text{Time}_{ij})(\text{Time}_{ik})\sigma^2_{b1}\\
&= 9.953+(0+2)(-0.017) + (0)(2)0.034\\
&= 9.919\end{align*}$$
:::

::: {.column width="50%"}
Between day 0 and day 8:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= \sigma^2_{b0} + ((\text{Time}_{ij}) + (\text{Time}_{ik}))\sigma_{b0,b1}\\
&~~~~~~~~+ (\text{Time}_{ij})(\text{Time}_{ik})\sigma^2_{b1}\\
&= 9.953+(0+8)(-0.017) + (0)(8)0.034\\
&= 9.817\end{align*}$$
:::
::::

::: {.absolute top=250 right=100 width="45%"}
[The covariance is decreasing with increased distance between time points!]{.alert}
:::

---

## Take home messages

<br>

- We can account for non-exchangeable types of correlation in a GLMM by adding [random slopes]{.alert} for time

<br>

- Adding random slopes allows for total variance and covariance to change with time (unlike GEEs)

<br>

- Interpretation of model cofficients continues to be conditional "within-cluster" interpretation

   - Fixed effects time slope interpreted as a [person-specific]{.underline} outcome trajectory

---

# Appendices

---

## Structures on $\boldsymbol{R_i}$ in GLMMs?

Up until now we've assumed that the $\epsilon_{ij}$ errors are independent within an individual: $\sigma^2_\epsilon \boldsymbol{I}$

- Allows us to interpret $\epsilon_{ij}$ as [sampling/measurement error]{.alert}

- This assumes [constant measurement error]{.alert} across time

- This is the most common type of structure to assume with mixed models

<br>

[Can we assume a non-independent structure for $\boldsymbol{R_i}$?]{.alert}

- Technically, yes...

We tend not to for 3 reasons:

1. In doing so, we lose the interpretation of $\epsilon_{ij}$ as sampling/measurement error

2. You might run into estimation issues if you try to estimate a full $\boldsymbol{G}$ (intercepts and slopes) and non-diagonal $\boldsymbol{R_i}$ without enough information in the data

3. Often full random effects (intercepts and slopes) and regular, diagonal measurement error ($\boldsymbol{R_i} = \sigma^2_\epsilon \boldsymbol{I}$) together take care of any time-variation in correlation

---

## Structures on $\boldsymbol{R_i}$ in GLMMs?

If we *do* want to use a non-independent $\boldsymbol{R_i}$, we should [only fit random intercepts]{.alert}

- This is sometimes called a [hybrid random effects]{.alert} model

<br>

Let $C_i(\text{Time}_{ij})$ be our [serial correlation]{.alert} parameter. Then we can specify the model as:

$$Y_{ij} = \beta_0 + \beta_1 U_{i0} + \beta_3 (\text{Time}_{ij}) + b_{0i}Z_i + C_i(\text{Time}_{ij}) + \epsilon_{ij}$$

- Random intercepts: $b_{0i} \sim N(0, \sigma^2_0)$

- Serial correlation: $C_i(\text{Time}_{ij}) \sim N(0, \sigma^2_c); ~~ Corr[C_i(\text{Time}_{ij}), C_i((\text{Time}_{ik}))] = \alpha((\text{Time}_{ij}), (\text{Time}_{ik}))$

   - Usually we use an exponential correlation function: $\alpha((\text{Time}_{ij}), (\text{Time}_{ik})) = \exp\{-|(\text{Time}_{ij}) - (\text{Time}_{ik})|\}$

- Measurement error: $\epsilon_{ij} \sim (0, \sigma^2_\epsilon)$

---

## Example: Exercise therapy trial

Let's try this with the exercise trial data

```{r exercise hybrid, output=F}
library(nlme)
exercise_hybrid <- lme( strength ~ PROGRAM*time,
                        random = ~1 | ID,
                        correlation = corExp( form = ~ time | ID, nugget=TRUE),
                      data = exercise_long_complete )
summary(exercise_hybrid)
```
---

## Example: Exercise therapy trial
:::: {.columns}
::: {.column width="50%"}
![](hybrid-out.png){width=80%}
:::

::: {.column width="50%"}
Interpreting the output:

- `(random) SD`: square-root of variance associated with random intercept ($\sqrt{\sigma^2_0}$)

- `Residual SD`: square root of $\sigma^2_0 + \sigma^2_\epsilon$

- `range`: correlation between observations one time unit apart

- `nugget`: if this is 0, there's no measurement error
:::
::::

---

## Example: Exercise therapy trial

Comparing hybrid and random slopes models:

:::: {.columns}
::: {.column width="50%"}
```{r hybrid 3}
summary(exercise_hybrid)$tTable[,1:4]
```
:::

::: {.column width="50%"}
```{r slopes 3}
summary(exercise_lme)$coef
```
:::
::::

$$E[\text{body strength}_{ij} | \boldsymbol{X}] = \beta_0 + \beta_1(\text{Program}_i) + \beta_2(\text{Time}_{ij}) + \beta_3(\text{Program}_i \times \text{Time}_{ij})$$

Within the same person...

- $\beta_0$: Average strength of Program 1 at baseline

- $\beta_1$: Difference in avg. strength between Program 2 and Program 1 at baseline

- $\beta_2$: Rate of strength change in Program 1 between 2 adjacent timepoints

- $(\beta_2 + \beta_3)$: Rate of strength change in Program 2 between 2 adjacent timepoints

- $(\beta_1 + \beta_3)$: Difference in strength between Program 2 and Program 1 at the same timepoint

- $\beta_3$: Difference in Program 1's strength change rate and Program 2's strength change rate