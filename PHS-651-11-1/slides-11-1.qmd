---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
library(corrr)
library(lme4)
```

<h1>Lecture 11.1: Proportional hazard regression & the Cox model</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>November 19, 2024</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=50 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/PHS-651-11-1/slides-11-1>

:::

::: {.absolute bottom=0 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/PHS-651-11-1/slides-11-1"))
```
:::

---

## Recap: Estimating survival functions

Last week we discussed how to estimate the survival function $S(t)$

- Could assume $T$ follows a probability distribution [... but there's a lot of room for us to be wrong about the distribution]{.alert}

- Could use a non-parametric method like [Kaplan-Meier]{.alert}

. . .

While KM estimates of $S(t)$ also allow us to estimate the cumulative hazard $\Lambda(t)$, this estimate doesn't perform well when your sample size is small

- An alternative approach is the estimate $\Lambda(t)$ using the [Nelson-Aalen]{.alert} method

- Typically we report $\hat S_{KM}(t)$ and $\tilde \Lambda_{NA}(t)$

To visualize how survival changes, we can plot $\hat S_{KM}(t)$ over time to create a [survival curve]{.alert}

---

## Recap: Comparing survival curves

We also discussed how to compare 2 (or more) survival curves

- The (unweighted) logrank test is most powerful under [proportional hazards]{.alert}

- When we have evidence of non-proportional hazards, it might be of interest to give different weight to events that happen early vs late

   - Several types of logrank test weights, including the Fleming-Harrington family of weights
   
. . .

These methods work well when we're looking at separate distinct groups. But what happens when we want to look at the impact of multiple covariates?

- Much like in other types of statistical analysis, stratified hypothesis tests can only get us so far

- Sounds like we need a regression method...

---

## Parametric proportional hazards regression

Models for survival times are specified in terms of the (log) hazard function

- A parametric regression model for the (log) hazard based on an exponential distribution for the survival times gets us:

$$\log\left[\lambda_i(t)\right] = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik}$$

$\beta_0$ is called the [baseline hazard]{.alert}, or the hazard when all covariates are 0

- We also denote this as $\lambda_0(t)$

---

## Parametric proportional hazards regression

$$\log\left[\lambda_i(t)\right] = \beta_0 + \beta_1 x_{i1} + \dots + \beta_k x_{ik}$$

We call this a [proportional hazards]{.alert} regression since the hazard for participant $i$ is a constant multiple of the baseline hazard for all times $t$: $\lambda_i(t) = \lambda_0(t)e^{\beta_1 x_{i1} + \dots \beta_k x_{ik}}$

- The [hazard ratio]{.alert} for participant $i$ with respect to participant $j$ is:
$$\frac{\lambda_i(t)}{\lambda_j(t)} = e^{\beta_1(x_{i1} - x_{j1}) + \dots + \beta_k (x_{ik} - x_{jk})}$$


---

## Distributions for parametric regression

Under an [exponential]{.alert} model, the baseline hazard is equal to: $h_0(t) = e^{\beta_0}$

There are other distributions we could assume:

- Weibull: $h_0(t) = \alpha \lambda t^{\alpha-1}$

- Gompertz: $h_0(t) = \theta e^{\alpha t}$

- Pareto: $h_0(t) = \theta/t$

. . .

But again, to get valid inference from a parametric regression model is highly dependent on choosing the correct probability distribution

- Is there a less parametric way we can construct a regression?

---

## Cox proportional hazards model

The [Cox proportional hazards model]{.alert} leaves the baseline hazard function $\lambda_0(t)$ unspecified:
$$\log\left[\lambda_i(t)\right] = \beta_0(t) + \beta_1 x_{i1} + \dots + \beta_k x_{ik}$$

$$\lambda_i(t) = \lambda_0(t)e^{\beta_1 x_{i1} + \dots + \beta_k x_{ik}}$$

- We call this a [semi-parametric]{.alert} approach because we are only assuming a parametric form for the covariate effects

- The traditional regression "intercept" is absorbed into the baseline hazard

---

## Cox proportional hazards model

$$\log\left[\lambda_i(t)\right] = \beta_0(t) + \beta_1 x_{i1} + \dots + \beta_k x_{ik}$$
$$\lambda_i(t) = \lambda_0(t)e^{\beta_1 x_{i1} + \dots + \beta_k x_{ik}}$$

Cox models are another example of a [proportional hazards]{.alert} regression since the hazard for participant $i$ is a constant multiple of the baseline hazard for all times $t$

- This means that any $\beta_k$ can be interpreted as the [log hazard ratio]{.alert} comparing an individual $i$ with $x_{ik}$ that is 1 unit larger than an individual $j$, all else held constant
$$\log\left[\frac{\lambda_i(t)}{\lambda_j(t)}\right] = \beta_k(x_{i1} - x_{j1}) = \beta_k$$

. . .

- The hazard ratios from Cox models are sometimes interpreted as risk ratios/relative risks

   - This is only valid when the baseline risk for the time period is small

---

## Cox model partial likelihood

Because Cox models are semi-parametric, we estimate model parameters by maximizing the [partial likelihood]{.alert}

$$L_p(\beta_1, \dots, \beta_k) = \prod_{i=1}^N \left[\frac{\exp(\beta_1 x_{i1} + \beta_k x_{ik})}{\sum_{j\in R(t_i)}\exp(\beta_1 x_{j1} + \beta_k x_{jk})}\right]^{c_i}$$

- $c_i$ denotes a censoring indicator function

- $R(t_i)$ denotes [risk set]{.alert}, or the set of subjects who were at risk for an event at time $t_i$

   - This means that censored observations are only included in the likelihood through the risk set (the denominator)
   
   - Once someone has their event (or is censored), they no longer contribute to the risk set in the denominator
   
---

## Cox model partial likelihood

The [conditional probability]{.alert} that participant $i$ with covariate values $x_{i1}, \dots, x_{ik}$ experiences an event at time $t_i$ given that exactly one person in the risk set experiences an event at that time is:
$$\frac{\exp(\beta_1 x_{i1} + \beta_k x_{ik})}{\sum_{j\in R(t_i)}\exp(\beta_1 x_{j1} + \beta_k x_{jk})}$$

- The partial likelihood is just multiplying up conditional probabilities over all (observed) events!

- This is kind of like how the KM estimator of the survival function worked last week, but now we're just adjusting for additional covariates

---

## Tied event times

Most of the intuitive theory for Cox models (but also KM survival and NA cumulative hazard estimators) are build off of an assumption that we don't have any ["tied"]{.alert} event times

- i.e., no two people have their event at the exact same moment

While this makes sense theoretically, we usually end up with at least some tied times in practice because we can't accurately observe everyone continuously

- Sometimes we get interval censoring!

. . .

Two main types of methods are used to account for ties

:::: {.columns}
::: {.column width="40%"}
- [Exact]{.alert} method: calculates conditional probability that all tied event times occur before all other observations in risk set

   - This is computationally intensive
:::

::: {.column width="60%"}
- A less computationally intensive method is the [Efron]{.alert} approximation

   - This allows for the multiple people who experience an event at time $t_i$ to contribute partially to the risk set at time $t_i$
   
   - [This is the recommended method]{.alert} - it's the default in R, but you need to specify `TIES=EFRON` in SAS
:::
::::

---

## Assumptions of the Cox model

The Cox model doesn't make many assumptions but it's main ones are:

- Correct functional form of the covariates (correct "mean" model)

- Observations from each participant are independent (no "clustering")

- Non-informative censoring

- Proportional hazards

The main assumption we tend to focus on is the proportional hazards assumption

- We can assess this by plotting KM curves, or by calculating and plotting Schoenfeld residuals

---

## Example: Larynx

We have data on 90 men diagnosed with cancer of the larynx between 1970-1978. Men entered the study when they began their first treatment. We are interested how disease stage impacts time to death, conditional on age at diagnosis
```{r larynx, echo=F}
library(KMsurv)
data(larynx)
larynx <- larynx %>% 
   mutate(age_grp = case_when(age >= 40 & age <50 ~ "[40-50)",
                              age >= 50 & age <60 ~ "[50-60)",
                              age >= 60 & age <70 ~ "[60-70)",
                              age >= 70 ~ "70+"))

```

First let's plot the unconditional KM curves:

```{r KM curves, output=F}
library(survival)
KM.age <- survfit( Surv(time, delta) ~ factor(stage), data=larynx )
plot(KM.age, mark.time=T, xlab="Months since 1st treatment", ylab="Survival", col=1:4)
legend( 10, .9, col=1:4, 
        legend=c("Stage I", "Stage II","Stage III","Stage IV"), bty="n" )

```

---

## Example: Larynx

```{r KM curves2, echo=F, out.width="200%"}
library(survival)
KM.age <- survfit( Surv(time, delta) ~ factor(stage), data=larynx )
plot(KM.age, mark.time=T, xlab="Months since 1st treatment", ylab="Survival", col=1:4)
legend( 9, .94, col=1:4, lty=1,
        legend=c("Stage I", "Stage II","Stage III","Stage IV"), bty="n" )

```

---

## Example: Larynx

```{r cox}
cox.model <- coxph( Surv(time, delta) ~ age + factor(stage), data=larynx )
summary(cox.model)
```

. . .

- $e^{\beta_2(\text{Stage II})} = 1.15$: Compared to those with Stage I cancer who were the same age at diagnosis, patients with Stage II cancer are expected to have a 15% higher hazard

- $e^{\beta_2(\text{Stage III})} = 1.90$: Compared to those with Stage I cancer who were the same age at diagnosis, patients with Stage III cancer are expected to have a 90% higher hazard

- $e^{\beta_2(\text{Stage IV})} = 5.51$: Patients with Stage IV cancer are expected to have a hazard that is 5.51 times higher compared to those with Stage I cancer who were the same age at diagnosis

---

## Cox model coefficient inference

Just like any regression model, it's not enough to just get esimates of the regression coefficients

- We need a way to perform [inference]{.alert} to understand how much uncertainty we have around that estimate

A $(1-\alpha/2)\times 100$% confidence interval for a regression estimate takes the usual form:
$$\hat{\beta} \pm Z_{1-\alpha/2} \times SE(\hat \beta)$$

. . .

Much like with logistic regression, though, it is more interpretable to perform inference on the hazard ratio $e^\beta$ than the log hazard ratio
 
 - This just means we need to exponentiate the upper and lower limits of the log hazard ratio confidence interval:
 
 $$\left(\exp\left\{\hat{\beta} - Z_{1-\alpha/2} \times SE(\hat \beta)\right\}, \exp\left\{\hat{\beta} + Z_{1-\alpha/2} \times SE(\hat \beta)\right\}\right)$$
 
 ---

## Cox model standard errors

$$\hat{\beta} \pm Z_{1-\alpha/2} \times SE(\hat \beta)$$

How do we get $SE(\hat \beta)$ though?

We can get the variance of $\hat \beta$ by taking the negative second-derivative of the log-partial likelihood:
$$Var(\hat \beta) = \frac{- \partial^2 \log(L_p)}{\partial\beta^2}$$

- The standard error is then: $SE(\hat \beta) = \sqrt{Var(\hat \beta)}$

The standard error is also reported in any Cox model output

---

## Example: Larynx

```{r cox2}
cox.model <- coxph( Surv(time, delta) ~ age + factor(stage), data=larynx )
summary(cox.model)

se.cox <- summary(cox.model)$coef[,3]

CI.lower <- exp( summary(cox.model)$coef[,1] - qnorm(0.975)*se.cox )
CI.upper <- exp( summary(cox.model)$coef[,1] + qnorm(0.975)*se.cox )
```

- 95% CI for $e^{\beta_2}$: We are 95% confident that the true hazard ratio comparing patients with Stage II cancer to similarly-aged patients with Stage I cancer is between `r round(CI.lower[2], 4)` and `r round(CI.upper[2], 4)`.

- 95% CI for $e^{\beta_3}$: (`r round(CI.lower[3], 4)`, `r round(CI.upper[3], 4)`)

- 95% CI for $e^{\beta_4}$: (`r round(CI.lower[4], 4)`, `r round(CI.upper[4], 4)`)

---

## Cox model hypothesis tests

There are also multiple ways we can perform a hypothesis test for Cox model regression coefficients

- A regular [Wald/Z-statistic test]{.alert} is convenient for testing whether a single parameter estimate $\hat \beta =0$

$$H_0: \beta = 0$$
$$H_A: \beta \ne 0$$

$$Z = \frac{\hat \beta}{SE(\hat \beta)} \sim N(0,1)$$

---

## Cox model hypothesis tests

There are also multiple ways we can perform a hypothesis test for Cox model regression coefficients

- A [partial likelihood ratio test]{.alert} allows us to test whether multiple parameter estimates are 0

   - Full model: $\lambda(t | x_1, \dots, x_p) = \lambda_0(t)e^{\beta_1 x_1 + \dots + \beta_p x_p}$
   
   - Reduced model: $\lambda(t | x_1, \dots, x_k) = \lambda_0(t)e^{\beta_1 x_1 + \dots + \beta_p x_k}$
   
$$H_0: \beta_{k+1} = \dots = \beta_p = 0$$
$$H_A: \text{at least one of }\beta_{k+1}, \dots, \beta_p \ne 0$$

$$T = 2\times \left\{\log(L_{p,full}) - \log(L_{p,reduced})\right\} \sim \chi^2_{p-k}$$

---

## Example: Larynx

What if we were interested in the effect of age on time to death, conditional on disease stage?

Let's compare the model that adjusts for cancer stage against a model that only adjusts for age:

$$H_0: \beta_2 = \beta_3 = \beta_4 = 0$$
$$H_A: \text{at least one of }\beta_2, \beta_3, \beta_4 \ne 0$$

```{r lr test}
#| output-location: column
cox.red <- coxph( Surv(time, delta) ~ age,
                  data=larynx)

anova(cox.red, cox.model)
```

. . .

$T = 15.68$, p-value=0.001... Looks like at least one of the cancer stage coefficients is statistically-significantly non-zero

---

## Estimating the baseline hazard and survival functions

We can also use the a Cox model to estimate the survival and hazard functions of a group of individuals with specific covariate values

- Let $t_1 < \dots< t_D$ denote the event times and $d_i$ denote the number of events at time $t_i$. Then:

$$\hat \Lambda_0(t) = \sum_{t_i \le t} \frac{d_i}{\sum_{j\in R(t_i)}\exp(\hat\beta_1 x_{j1} + \hat\beta_k x_{jk})}$$

is the [Breslow estimator]{.alert} of the baseline cumulative hazard

- Reduces to the Nelson-Aalen estimator of the cumulative hazard function when no covariates are present

. . .

We can then transform the Breslow estimator to get an estimate of the baseline survival function:

$$\hat S_0(t) = \exp\left[-\hat \Lambda_0(t)\right]$$

---

## Estimating non-baseline hazard and survival functions

Once we've estimated the baseline hazard and survival functions, we can use those to estimate cumulative hazard and survival functions for individuals with particular covariate values:

$$\hat \Lambda_*(t) = e^{(\hat \beta_1 x_1^* + \hat \beta_k x_k^*)}\hat \Lambda_0(t)$$

$$\hat S_*(t) = \left[\hat S_0(t)\right]^{\exp(\hat \beta_1 x_1^* + \hat \beta_k x_k^*)}$$

---

## Example: Larynx

Let's see what the survival curve would look like for someone with stage III cancer who was 45 years old at diagnosis, and compare that to baseline (a 0-year-old with stage I cancer);

```{r est S, out.width="80%"}
estSurv <- survfit( cox.model, newdata=data.frame(stage=c(3,1), age=c(45,0)))
plot(estSurv, lty=1:2, col=1:2)
legend( 1, .4, lty=1:2, col=1:2,
        legend=c("Stage I, 45yo", "Baseline"), bty="n" )

```