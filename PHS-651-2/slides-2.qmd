---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      auto-stretch: false
execute: 
   eval: true
   echo: true
---
```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```
<h1> Lecture 2: Conditional models for correlated data </h1>

<h2> PHS 651: Advanced regression methods </h2>

<hr>

<h3> Mary Ryan Baumann, PhD </h3>

<h3> September 17, 2024 </h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=50 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/PHS-651-2/slides-2>

:::

::: {.absolute bottom=0 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/PHS-651-2/slides-2"))
```
:::

---

## Recap: correlated/clustered data

What do we mean by correlated data or data that is clustered?

- [Independent data]{.underline}: data where each observation of a variable are not meaningfully correlated with other observations of the same variable

- [Clustered/Correlated data]{.underline}: data where each observation of a variable has some kind of relationship to another set of observations of that variable

   - AKA: nested/multilevel/hierarchical data
   
   - Might expect measurements on [units within a cluster are more similar]{.alert} than measurements on units in different clusters

---

## Recap: correlated/clustered data


[Is it a clustering variable or is it an exposure variable?]{.alert}

- Clusters tend to be physical/structural (geography, time, a body/being, etc) or hierarchical (social/family hierarchy, institutional hierarchy, etc)

   - Often no obvious reference group for cluster

   - Often many, many clusters in population (though we may not have data on all of them)

- Exposure variables tend to have a [limited]{.alert} number of groups

   - e.g., diagnosis groups

. . .

The ICC is a way to measure the "clustered-ness" of the outcome

$$\text{ICC} = \frac{\text{variation in cluster means}}{\text{total variation}} = \frac{\text{variation in cluster means}}{\text{variation in cluster means } + \text{individual-level variation}}$$

---

## Recap: correlated/clustered data

Clustering may impact analysis in several ways

- Groups/clusters may not be the same size

- May affect variance/break the homoskedasticity assumption of OLS

- May impact average outcome/exposure

- Reduces the amount of information we get from each individual data point

:::: {.columns}

::: {.column width="70%"}
::: {.fragment}
[How do we account for this clustering in analysis?]{.alert}
:::

::: {.fragment}
- Could fit [separate regressions]{.alert} for each group... no "global" estimate of exposure effect

- Could [add group as covariate]{.alert} in model... violates parameters vs data size assumptions

- Could do [cluster-level model]{.alert}... we lose a lot of individual-level information

- ... Something else?
:::
:::

::: {.column width="30%"}
::: {.fragment}
[Some new approaches]{.alert}

Two main "classes":

- "Conditional" models (today's focus)

- "Marginal" models (we'll get to these later)
:::
:::

::::

---

## Conditional models

Basic idea: some clusters will [randomly]{.alert} having higher/lower means than other

- This means we might assume to have one population-level mean $\beta_0$

- And that each cluster $i$ will have a [cluster-specific mean]{.alert} a little bit above/below this: $\beta_0 \pm b_{0i}$

   - Can think of $b_{0i}$ as the cluster-specific deviation -- we'll add this to our regression model

. . .

Model is [conditional]{.alert} because the interpretation of $\beta_1$ assumes we're comparing individuals *in the same cluster* (conditional on group membership/that cluster-specific bump)

- Just like how interpretation of $\beta_1$ assumes we're comparing individuals who have the same value for all other covariates

- We'll dig into this more later

. . .

We tried this (somewhat informally) last week by adding cluster membership as a covariate

- Ran into issues around # parameters increasing with data size

. . .

We can fix some of those issues by using a [linear mixed effects]{.alert} model framework

---

## The linear mixed effects model

Say we have $m$ clusters that each have $n_i$ individuals, which can vary from cluster to cluster

. . .

:::: {.columns}

::: {.column width="55%"}
The linear mixed effects (LME) model takes the form:

$$Y_{ij} = \beta_0 + U_{ij}\beta_1 + Z_ib_{0i}+\epsilon_{ij}$$

- $Y_{ij}$ is the response/outcome observation for individual $j$ in cluster $i$

- $\beta_0$ is the population-level ([fixed effect]{.alert}) intercept

- $U_{ij}$ is the exposure variable for individual $j$ in cluster $i$

- $\beta_1$ is the population-averaged (fixed) effect of exposure on the outcome

- $Z_i$ is the random effects indicator for cluster $i$

- $b_{0i}$ is the [random intercept]{.alert} for cluster $i$

- $\epsilon_{ij}$ is the random error term for individual $j$ in cluster $i$
:::

::: {.column width="45%"}
::: {.fragment}
Or in matrix/vector notation:

$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

- $\vec{Y}_i$ is the response vector for cluster $i$ (dimensions: $n_i \text{ individuals} \times 1 \text{ variable}$)

- $\boldsymbol{X}_i$ is the [fixed effects]{.alert} design matrix for cluster $i$ (dimensions: $n_i \times p$)

- $\vec{\beta}$ is the fixed effects parameter vector (dimensions: $p \times 1$)

- $\vec{Z}_i$ is the random effects vector for cluster $i$ (dimensions: $n_i \times 1$)

- $\vec{\epsilon}_i$ is the vector of error terms (dimensions: $n_i \times 1$)
:::
:::

::::

---

## Interpreting LMEs

Consider a simple LME with 1 exposure variable $U$:
$$Y_{ij} = \beta_0 + U_{ij}\beta_1 + Z_ib_{0i} + \epsilon_{ij}$$

- $\beta_0$: the (sample) population-level mean in $Y$ when $u=0$

- $\beta_0 + b_{0i}$: the mean in $Y$ for cluster $i$ when $u=0$

- $\beta_1$: the change in $Y$ for increasing $U$ by one unit, comparing 2 individuals [in the same cluster]{.alert}

. . .

We can get (potentially) 2 types of interpretations out of this model

:::: {.columns}

::: {.column width="50%"}
::: {.fragment}
Conditional mean:
$$E[Y_{ij}|\boldsymbol{X}_i, b_{0i}] = \boldsymbol{X}_i\vec{\beta} + b_{0i}$$

- Mean outcome for an individual cluster
:::
:::

::: {.column width="50%"}
::: {.fragment}
Marginal mean:
$$E[Y_{ij}|\boldsymbol{X}_i] = \boldsymbol{X}_i\vec{\beta}$$

- Mean outcome averaged over the entire sample
:::
:::

::::

---

## Breaking down LMEs

Can also represent an LME model as:

$$\vec{Y}_i = \color{green}{\boldsymbol{X}_i\vec{\beta}} + \color{blue}{\vec{\xi}_i}$$

- Allows us to break down an LME into 2 main components:

:::: {.columns}

::: {.column width="50%"}
::: {.fragment}
   1. Mean response (as function of [fixed]{.alert} covariates)
   
   $$\color{green}{\boldsymbol{X}_i\vec{\beta}}$$
   
   - Think of this as a function of the variables we need to isolate the causal effect of our exposure of interest
:::
:::

::: {.column width="50%"}
::: {.fragment}
   2. Systematic variation (of the response/outcome)
   $$\color{blue}{\vec{\xi}_i} = \color{red}{\vec{Z}_ib_{0i}}+\color{orchid}{\vec{\epsilon}_i}$$
   made up of:
         i. Random effects
         
            - Random between-cluster variation: $\color{red}{\vec{Z}_i b_{0i}}$
            
         ii. Within-cluster variation: $\color{orchid}{\vec{\epsilon}_i}$
:::
:::

::::

---

## Systematic variation {.smaller}

<!-- $$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$ -->

$$\vec{\xi}_i = \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

:::: {.columns}

::: {.column width="50%"}
We typically assume random intercepts follow a distribution with a common variance:

$$b_{0i} \sim N(0, \sigma^2_b)$$

- Instead of estimating all the cluster-level effects separately, we assume they come from a common distribution and borrow information between clusters to come up with variance estimates
:::

::: {.column width="50%"}
::: {.fragment}
This is similar to how we've assumed individual-level errors follow a distribution:

$$\vec{\epsilon}_i \sim N(\vec{0}, \sigma^2_\epsilon \boldsymbol{I})$$

- $\sigma^2_\epsilon \boldsymbol{I}$ is the covariance matrix of the individual-level errors: $Cov[\epsilon_{ij}, \epsilon_{ik}] = \sigma^2_\epsilon \boldsymbol{I}$

   - Assumes all observations in cluster $i$ are *conditionally independent* given $b_{0i}$
   
      - Given cluster membership, all members of cluster are independent
      
   - Also known as an [independence model]{.alert} of within-cluster variation

   - Let's call this $\boldsymbol{R}_i$ $\rightarrow$ we'll look at other forms of $\boldsymbol{R}_i$ later in the term
   
:::
:::

::::

::: {.fragment .absolute top=50 right=100 width="30%"}
Also assume (for now) that $b_{0i}$ and $\vec{\epsilon}_i$ are *independent* ($b_{0i}\perp \vec{\epsilon}_i$)
:::

---

## The linear mixed effects model (cont.)

What's "mixed" about this model?

$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

- Named because it uses [fixed effects]{.alert} (effects for covariates $\vec{\beta}$) as well as [random effects]{.alert} (cluster-specific intercepts $b_{0i}$)

. . .

How is this different from adding groups as covariates?

- Allows clusters with fewer observations to be more influenced by the rest of the data ("borrow" information), and clusters with more observations to be less influenced

. . .

- "... we can learn about one cluster's coefficients by understanding the variability in coefficients across the population. [When there is little variability, we should rely on the population average coefficients to estimate those for a cluster. When there's substantial variation, we must rely more heavily on the data from each cluster in estimating its own coefficients]{.alert}." (Analysis of Longitudinal Data [Diggle et al] pg. 173)

<!-- What's "conditional" about this model? -->

<!-- - Because the mean of the outcome variable is [conditional]{.alert} on both the fixed effects (always true in regressions) *and* the random effects: -->
<!-- $$E[\vec{Y}_i|\boldsymbol{X}_i,\vec{b}_{0i}] = \boldsymbol{X}_i\vec{\beta}$$ -->

---

## Breaking down systematic variation {.smaller}

Random effects:
$$b_{0i} \sim N(0, \sigma^2_b)$$

- Assumes that [clusters]{.alert} are *randomly* varying around the population mean response: $E[b_{0i}]=0$

- The typical cluster-to-cluster deviation in the overall level of the response is $\sqrt{\sigma^2_b}$

. . .

Within-cluster variation:
$$\vec{\epsilon}_i \sim N(0, \boldsymbol{R}_i)$$

- Assumes that [cluster members]{.alert} are *randomly* varying around the cluster-specific mean response: $E[\vec{\epsilon}_i]=0$

-  The typical member-to-member deviation in the cluster-specific level of the response is $\sqrt{\sigma^2_\epsilon}$

---

## Variance/covariance of the outcome

$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

Then the variance of a single outcome measure is:
$$\begin{align*}Var[Y_{ij}] &= Var[\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\epsilon_{ij}]\\
&= Var[\vec{Z}_ib_{0i}+\epsilon_{ij}]\\
&= \sigma^2_b + \sigma^2_{\epsilon}\end{align*}$$ 

::: {.alert .fragment .absolute top=450 right=450}
(total variation!)
:::

. . .

And the covariance between 2 outcome measures in the same cluster is:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= Cov[\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\epsilon_{ij}, \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\epsilon_{ik}]\\
&= Cov[\vec{Z}_ib_{0i}+\epsilon_{ij}, \vec{Z}_ib_{0i}+\epsilon_{ik}]\\
&= Cov[\vec{Z}_ib_{0i}, \vec{Z}_ib_{0i}] + Cov[\epsilon_{ij}, \vec{Z}_ib_{0i}] + Cov[\vec{Z}_ib_{0i}, \epsilon_{ik}] + Cov[\epsilon_{ij}, \epsilon_{ik}]\\
&= \sigma^2_b + 0 + 0 + 0\\
&= \sigma^2_b\end{align*}$$

::: {.alert .fragment .absolute bottom=75 left=650}
In 552, we were used to seeing $Cov[Y_{ij}, Y_{ik}]=0$!
:::

---

## What does this covariance matrix look like?

$$\begin{align*}Cov[\vec{Y}_i] &= \vec{Z}_i\sigma^2_b\boldsymbol{1}\vec{Z}_i^T + \sigma^2_\epsilon \boldsymbol{I}\\
&= [\text{cluster-to-cluster variation}] + [\text{within-cluster member-to-member variation}]\\
&=\begin{bmatrix}
\sigma^2_b & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b
\end{bmatrix} + \begin{bmatrix}
\sigma^2_\epsilon & 0 & \dots & 0\\
0 & \ddots & \dots & 0\\
\vdots & \dots & \ddots & \vdots\\
0& \dots & 0 & \sigma^2_\epsilon
\end{bmatrix}\\
&=\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}\end{align*}$$

- Known as a [block exchangeable]{.alert} matrix or [compound symmetry]{.alert}

---

## Inference for $\widehat{\beta}$

Estimation of $\beta$ is done by (restricted) maximum likelihood estimation -- the formula looks a lot like it did for weighted least squares (WLS)

$$\widehat{\beta} = \left(\sum_{i=1}^n \boldsymbol{X}^T \left\{Cov[\vec{Y}_i]\right\}^{-1}\boldsymbol{X}_i\right)^{-1}\sum_{i=1}^n \boldsymbol{X}^T \left\{Cov[\vec{Y}_i]\right\}^{-1}\vec{Y}_i$$

::: {.fragment}
- In OLS and WLS, we used [maximum likelihood]{.alert} to estimate $\beta$'s and variance components 
   
- In LMEs, we use [restricted maximum likelihood (REML)]{.alert} estimation because MLE gives us biased estimates of the covariance components in small samples

- This means that we [cannot]{.alert} perform likelihood ratio test-based inference for $\vec{\beta}$ when using REML!

   - Wald-based inference (z/t-tests and confidence intervals) are still fine, though
:::

---

## Inference for $\widehat{\beta}$

Estimation of $\beta$ is done by (restricted) maximum likelihood estimation -- the formula looks a lot like it did for weighted least squares (WLS)

$$\widehat{\beta} = \left(\sum_{i=1}^n \boldsymbol{X}^T \left\{Cov[\vec{Y}_i]\right\}^{-1}\boldsymbol{X}_i\right)^{-1}\sum_{i=1}^n \boldsymbol{X}^T \left\{Cov[\vec{Y}_i]\right\}^{-1}\vec{Y}_i$$

The "model-based" covariance matrix for $\widehat{\beta}$ is
$$Cov[\widehat{\beta}]=\left(\sum_{i=1}^n \boldsymbol{X}_i^T\left\{Cov[\vec{Y}_i]\right\}^{-1}\boldsymbol{X}_i^T\right)^{-1}$$

::: {.fragment}
The confidence intervals for $\vec{\beta}$ will take the form:

$$\widehat{\beta} \pm t_{\alpha/2; DoF=n-1} \text{diag}\left(\sum_{i=1}^n \boldsymbol{X}_i^T\left\{Cov[\vec{Y}_i]\right\}^{-1}\boldsymbol{X}_i^T\right)^{-1}$$
:::

---

## Robust inference for $\widehat{\beta}$

We can also use [robust or sandwich variance]{.alert} estimators to perform inference on $\widehat{\beta}$

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$

where
$$\begin{align*}&B^{-1} = Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \boldsymbol{X}_i^T \left\{Cov[\vec{Y}_i]\right\}^{-1} \boldsymbol{X}_i\right]^{-1}\\
&A = \left[\sum_{i=1}^n \boldsymbol{X}_i^T \left\{Cov[\vec{Y}_i]\right\}^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T \left\{Cov[\vec{Y}_i]\right\}^{-1} \boldsymbol{X}_i\right]\end{align*}$$

::: {.fragment}
This will give us (approximately) correct variance estimates for $\widehat{\beta}$ if we're [wrong]{.alert} about the random effects or the variance of $Y$
:::

---

## Robust inference for $\widehat{\beta}$

We can also use [robust or sandwich variance]{.alert} estimators to perform inference on $\widehat{\beta}$

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$

where
$$\begin{align*}&B^{-1} = Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \boldsymbol{X}_i^T \left\{Cov[\vec{Y}_i]\right\}^{-1} \boldsymbol{X}_i\right]^{-1}\\
&A = \left[\sum_{i=1}^n \boldsymbol{X}_i^T \left\{Cov[\vec{Y}_i]\right\}^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T \left\{Cov[\vec{Y}_i]\right\}^{-1} \boldsymbol{X}_i\right]\end{align*}$$

If our random intercepts [correctly]{.alert} model the covariance between observations, $A \approx B$ and:
$$\begin{align*}Cov^*[\widehat{\beta}] &= B^{-1} A B^{-1}\\
&\approx B^{-1} B B^{-1}\\
& = B^{-1} = Cov[\widehat{\beta}]\end{align*}$$
the robust variance is the same as the model-based variance!

::: {.fragment}
![](clustering-SE-meme){.absolute top=75 right=-50 width="25%"}
:::

---

## ICC in LMEs

LMEs also make it very intuitive to calculate the ICC

$$\vec{\xi}_i = \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

$$b_{i0} \sim N(0, \sigma^2_b)$$

$$\epsilon_{ij} \sim N(0, \sigma^2_\epsilon)$$

$$\begin{align*}\text{ICC} &= \frac{\text{variation in cluster means}}{\text{variation in cluster means } + \text{individual-level variation}}\\
&= \frac{\sigma^2_b}{\sigma^2_b + \sigma^2_{\epsilon}}\end{align*}$$

<!-- --- -->

<!-- ## Distributional assumptions -->

<!-- LMEs assume that the outcome follows a [multivariate normal]{.alert} distribution -->

---

## Confounding by cluster membership in LMEs

The chief purpose of LMEs is to account for correlations between observations

- They assume cluster membership adds an additional level of variation to the outcome that isn't accounted for by OLS modeling strategies

. . .

Also assumes the random effects are independent of the fixed effect variables

- i.e., Cluster membership is affecting the outcome distribution, but not (meaningfully) impacting the exposure distribution

. . .

This means that [adding random intercepts does not account for confounding by cluster membership]{.alert}

- To account for confounding, we need to add additional (cluster-level) fixed effect variables to the mean model

---

## Example: radon measurements in Minnesota

Let's fit an LME model for the effect of measurement floor on log-radon measurements in Minnesota houses. What would our model equation look like?

. . .

$$Y_{ij} = \beta_0 + \beta_1 (\text{floor}_{ij}) + b_{0i} + \epsilon_{ij}$$

   - $Y_{ij}$ is the log radon for house $j$ in county $i$
   
   - $\text{floor}_{ij}$ is whether the lowest floor of house $j$ in county $i$ was the basement (0) or the ground floor (1)
   
   - $b_{0i}$ is the county-level random intercept
   
---

## Example: radon measurements in Minnesota

What would the output/estimates of this model look like?

. . .

```{r radon cleaning, echo=F}
radon <- read.table("~/Desktop/teaching/PHS-651/data/Gelman-data/radon/srrs2.dat",header=T,sep=",")

set.seed(081524)

radon.mn <- radon %>% 
   as.data.frame() %>% 
   mutate(log_radon = log(activity +0.1),
          county = str_trim(county)) %>% 
   dplyr::filter(state == "MN")
```

```{r radon lme}
library(lme4)

# run an LME with random intercepts for county #
radon_lme <- lmer(log_radon ~ floor + (1 | county), data=radon.mn)
summary(radon_lme)
```

- LME $\beta_1$: `r round(summary(radon_lme)$coef[2,1],4)`

   - Model-based 95% CI: (`r round(summary(radon_lme)$coef[2,1] - summary(radon_lme)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_lme)$coef[2,1] + summary(radon_lme)$coef[2,2]*qnorm(0.975), 4)`)

   - How would we interpret this?

---

## Example: radon measurements in Minnesota

Let's compare the model-based confidence intervals

 - Model-based 95% CI: (`r round(summary(radon_lme)$coef[2,1] - summary(radon_lme)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_lme)$coef[2,1] + summary(radon_lme)$coef[2,2]*qnorm(0.975), 4)`)

to the robust CIs:

```{r radon lme robust}
library(clubSandwich)

# get the sandwich-based covariance matrix #
var_sand <- vcovCR(radon_lme, type="CR0")

# get the sandwich-based confidence interval #
paste0( "(",round(conf_int(radon_lme, vcov="CR0")$CI_L, 4), ", "
        , round(conf_int(radon_lme, vcov="CR0")$CI_U, 4),")")
```

. . .

:::: {.columns}

::: {.column width="50%"}
Model-based SEs:
```{r radon lme model-based}
summary(radon_lme)$coef[,2]
```
:::

::: {.column width="50%"}
Robust SEs:
```{r radon lm robust-var}
sqrt(diag(var_sand))
```
:::

::::

::: {.fragment}
![](clustering-SE-meme){.absolute top=0 right=100 width="25%"}
:::

---

## Example: radon measurements in Minnesota

Let's compare the LME results to adding county as a regression coefficient

. . .

```{r radon lm cty}
radon_cty_lm <- lm(log_radon ~ floor + factor(county) - 1, data=radon.mn)
summary(radon_cty_lm)
```

:::: {.columns}

::: {.column width="60%"}

::: {.fragment}
- OLS $\beta_1$: `r round(summary(radon_cty_lm)$coef[1,1],4)`

   - 95% CI: (`r round(summary(radon_cty_lm)$coef[1,1] - summary(radon_cty_lm)$coef[1,2]*qnorm(0.975), 4)`, `r round(summary(radon_cty_lm)$coef[1,1] + summary(radon_cty_lm)$coef[1,2]*qnorm(0.975), 4)`)
:::

::: {.fragment}
What differences can you see compared to the previous LME?

- LME $\beta_1$: `r round(summary(radon_lme)$coef[2,1],4)`

   - Model-based 95% CI: (`r round(summary(radon_lme)$coef[2,1] - summary(radon_lme)$coef[2,2]*qnorm(0.975), 4)`, `r round(summary(radon_lme)$coef[2,1] + summary(radon_lme)$coef[2,2]*qnorm(0.975), 4)`)
   
   - Robust 95% CI: (`r round(conf_int(radon_lme, vcov="CR0")$CI_L, 4)`, `r round(conf_int(radon_lme, vcov="CR0")$CI_U, 4)`)
:::
:::

::: {.column width="40%"}
::: {.fragment}
```{r radon CI plot, echo=F}
ci_ols <- rbind(c(2, round(summary(radon_cty_lm)$coef[1,1] - summary(radon_cty_lm)$coef[1,2]*qnorm(0.975), 4)), c(2,round(summary(radon_cty_lm)$coef[1,1] + summary(radon_cty_lm)$coef[1,2]*qnorm(0.975), 4)))

ci_lme <- rbind(c(1.5, round(summary(radon_lme)$coef[2,1] - summary(radon_lme)$coef[2,2]*qnorm(0.975), 4)), c(1.5,round(summary(radon_lme)$coef[2,1] + summary(radon_lme)$coef[2,2]*qnorm(0.975), 4)))

ci_robust <- rbind(c(1,round(conf_int(radon_lme, vcov="CR0")$CI_L, 4)), c(1,round(conf_int(radon_lme, vcov="CR0")$CI_U, 4)))

plot(NA, xlim=c(-0.9,-0.35), ylim=c(0.5,2.2), xlab="", ylab="", yaxt='n', cex.axis=1.5)
segments(ci_ols[1,2], ci_ols[1,1],ci_ols[2,2], ci_ols[2,1], col="red", lwd=2)
text(ci_ols[2,2]+0.03, 2, "OLS CI", col="red", cex=1.5)
segments(ci_lme[1,2], ci_lme[1,1],ci_lme[2,2], ci_lme[2,1], col="purple", lwd=2)
text(ci_lme[2,2]+0.08, 1.5, "LME model-based CI", col="purple", cex=1.5)
segments(ci_robust[1,2], ci_robust[1,1],ci_robust[2,2], ci_robust[2,1], col="blue", lwd=2)
text(ci_robust[2,2]+0.06, 1, "LME robust CI", col="blue", cex=1.5)
```
:::
:::

::::

::: {.fragment .absolute top=400 right=50 width="30%"}
[If county affects both % houses with basements and radon levels, what variables could we include to control for this confounding?]{.alert}
:::

---

## Marginal effects in LMEs

The default interpretation of LMEs is [conditional]{.alert}... but we can sometimes extract [marginal]{.alert} effects out of them

. . .

- For continuous outcomes (like we've been talking about), or outcomes that we **do not** transform, [the conditional and marginal effects will be identical]{.alert}

$$\begin{align*}E[\vec{Y}_i] &= E\left\{E[\vec{Y}_i|\boldsymbol{X}_i, b_{0i}]\right\}\\
&=E[\boldsymbol{X}_i\vec{\beta} + \vec{Z}_i b_{0i}]\\
&=\boldsymbol{X}_i\vec{\beta} + \vec{Z}_i E[b_{0i}]\\
&=\boldsymbol{X}_i\vec{\beta}\end{align*}$$

. . .

- For non-continuous outcomes or outcomes we do need to transform, the above will not hold and the marginal and conditional effects with be different... more on this next week

---

## A note on terminology

Throughout this lecture we've referred to $\vec{\beta}$ as "fixed effects" and $b_{0i}$ as "random effects"

- This is standard terminology within statistics

. . .

In parts of the social sciences (\*cough\* economics \*cough\*), "fixed effects" refer to adjusting for cluster membership as covariates

- These types of models are referred to in econ as "fixed effect models"

- This is done to adjust for the (potentially) confounding effect of cluster membership, not so much to deal with correlated data

. . .

This terminology has lead to many an argument between researchers until everyone realizes there was a miscommunication

- Just an FYI in case you're ever reading about random/fixed effects online