---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      auto-stretch: false
execute: 
   eval: true
   echo: true
---
```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```
<h1> Lecture 2: Conditional models for correlated data </h1>

<h2> PHS 651: Advanced regression methods </h2>

<hr>

<h3> Mary Ryan Baumann, PhD </h3>

<h3> September 17, 2024 </h3>

::: {.absolute bottom=-100 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/PHS-651-2/slides-2>

:::

::: {.absolute bottom=-150 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/PHS-651-2/slides-2"))
```
:::

---

## Zoom recording disclaimer

<br>

This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.

<br>

If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.

<br>

The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.

---

## Recap: correlated/clustered data

What do we mean by correlated data or data that is clustered?

- [Independent data]{.underline}: data where each observation of a variable are not meaningfully correlated with other observations of the same variable

- [Correlated data]{.underline}: data where each observation of a variable is not entirely independent of other observations of the same variable
   - AKA: multilevel/hierarchical data
   
. . .

The ICC is a way to measure the "clustered-ness" of the outcome

$$\text{ICC} = \frac{\text{variation in cluster means}}{\text{total variation}} = \frac{\text{variation in cluster means}}{\text{variation in cluster means } + \text{individual-level variation}}$$

---

## Recap: correlated/clustered data

Clustering may impact analysis in several ways

- Groups/clusters may not be the same size

- May affect variance/break the homoskedasticity assumption of OLS

- May impact average outcome/exposure

- Reduces the amount of information we get from each individual data point

[How do we account for this clustering in analysis?]{.alert}

. . .

- Could fit separate regressions for each group... no "global" estimate of exposure effect

- Could add group as covariate in model... violates parameters vs data size assumptions

- Could do cluster-level model... we lose a lot of individual-level information

- ... Something else?

. . .

Two main "classes" of modeling approaches:

- "Conditional" models (today's focus)

- "Marginal" models (we'll get to these later)

---

## Conditional models

Basic idea: some clusters will [randomly]{.alert} having higher/lower means than other

- This means we might assume to have one population-level mean $\beta_0$

- And that each cluster $i$ will have a [cluster-specific mean]{.alert} a little bit above/below this: $\beta_0 \pm b_{0i}$

   - Can think of $b_{0i}$ as the cluster-specific deviation -- we'll add this to our regression model

. . .

Model is [conditional]{.alert} because the interpretation of $\beta_1$ assumes we're comparing individuals *in the same cluster* (conditional on group membership/that cluster-specific bump)

- Just like how interpretation of $\beta_1$ assumes we're comparing individuals who have the same value for all other covariates

- We'll dig into this more later

. . .

We can put this into practice with a [linear mixed effects]{.alert} model

---

## The linear mixed effects model

The linear mixed effects (LME) model takes the form:

$$Y_{ij} = \beta_0 + U_{ij}\beta_1 + Z_ib_{0i}+\epsilon_{ij}$$

- $Z_i$ is the random effects indicator for cluster $i$

- $b_{0i}$ is the [random intercept]{.alert} for cluster $i$ (dimensions: $1\times 1$)

. . .

Or in matrix/vector notation:

$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

- $\vec{Y}_i$ is the response vector for cluster $i$ (dimensions: $n_i \text{ individuals} \times 1 \text{ variable}$)

- $\boldsymbol{X}_i$ is the [fixed effects]{.alert} design matrix for cluster $i$ (dimensions: $n_i \times p$)

- $\vec{\beta}$ is the fixed effects parameter vector (dimensions: $p \times 1$)

- $\vec{Z}_i$ is the random effects vector for cluster $i$ (dimensions: $n_i \times 1$)

- $\vec{\epsilon}_i$ is the vector of error terms (dimensions: $n_i \times 1$)

---

## Interpreting LMEs

Consider a simple LME with 1 exposure variable $W$:
$$\vec{Y}_i = \beta_0 + \beta_1 \vec{U}_{i} + \vec{Z}_ib_{0i} + \vec{\epsilon}_i$$

- $\beta_0$: the (sample) population-level mean in $Y$ when $w=0$

- $\beta_0 + b_{0i}$: the mean in $Y$ for cluster $i$ when $w=0$

- $\beta_1$: the change in $Y$ for increasing $W$ by one unit, comparing 2 individuals *in the same cluster*

. . .

Conditional mean:
$$E[Y_{ij}|\boldsymbol{X}_i, b_{0i}] = \boldsymbol{X}_i\vec{\beta} + b_{0i}$$

- Mean outcome for an individual cluster

Marginal mean:
$$E[Y_{ij}|\boldsymbol{X}_i] = \boldsymbol{X}_i\vec{\beta}$$

- Mean outcome averaged over the entire sample

---

## Breaking down LMEs

Can also represent an LME model as:

$$\vec{Y}_i = \color{green}{\boldsymbol{X}_i\vec{\beta}} + \color{blue}{\vec{\xi}_i}$$

- Allows us to break down an LME into 2 main components:

   1. Mean response (as function of [fixed]{.alert} covariates)
   
   $$\color{green}{\boldsymbol{X}_i\vec{\beta}}$$
   
   2. Systematic variation (of the response/outcome)
   $$\color{blue}{\vec{\xi}_i} = \color{red}{\vec{Z}_ib_{0i}}+\color{purple}{\vec{\epsilon}_i}$$
   made up of:
         i. Random effects
         
            - Random between-cluster variation: $\color{red}{\vec{Z}_i b_{0i}}$
            
         ii. Within-cluster variation: $\color{purple}{\vec{\epsilon}_i}$

---

## The linear mixed effects model (cont.)

<!-- $$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$ -->

$$\vec{\xi}_i = \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

We typically assume
$$b_{0i} \sim N(0, \sigma^2_b)$$
$$\vec{\epsilon}_i \sim N(\vec{0}, \boldsymbol{R}_i)$$

- $\boldsymbol{R}_i$ is the covariance matrix of the individual-level errors: $Cov[\epsilon_{ij}, \epsilon_{ik}] = \boldsymbol{R}_i$

- Also assume $b_{0i}$ and $\vec{\epsilon}_i$ are *independent* ($b_{0i}\perp \vec{\epsilon}_i$)
   
   - What happens when this is violated? Stay tuned for later this semester :grinning:

---

## The linear mixed effects model (cont.)

What's "mixed" about this model?

$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

- Named because it uses [fixed effects]{.alert} (effects for covariates $\vec{\beta}$) as well as [random effects]{.alert} (cluster-specific intercepts $b_{0i}$)

. . .

How is this different from adding groups as covariates?

- Allows groups with fewer observations to be more influenced by the rest of the data, and groups with more observations to be less

- "... we can learn about one cluster's coefficients by understanding the variability in coefficients across the population. [When there is little variability, we should rely on the population average coefficients to estimate those for a cluster When there's substantial variation, we must rely more heavily on the data from each cluster in estimating its own coefficients]{.alert}." (Analysis of Longitudinal Data, pg. 173)

<!-- What's "conditional" about this model? -->

<!-- - Because the mean of the outcome variable is [conditional]{.alert} on both the fixed effects (always true in regressions) *and* the random effects: -->
<!-- $$E[\vec{Y}_i|\boldsymbol{X}_i,\vec{b}_{0i}] = \boldsymbol{X}_i\vec{\beta}$$ -->

---

## Breaking down systematic variation

Random effects:
$$b_{0i} \sim N(0, \sigma^2_b)$$

- Assumes that [clusters]{.alert} are *randomly* varying around the population mean response: $E[b_{0i}]=0$

- The typical cluster-to-cluster deviation in the overall level of the response is $\sqrt{\sigma^2_b}$

. . .

Within-cluster variation:
$$\vec{\epsilon}_i \sim N(0, \boldsymbol{R}_i)$$

- Assumes that [cluster members]{.alert} are *randomly* varying around the cluster-specific mean response: $E[\vec{\epsilon}_i]=0$
- For now, assume $\boldsymbol{R}_i = \sigma^2_\epsilon \boldsymbol{I}$
   - Assumes all observations in cluster $i$ are *conditionally independent* given $b_{0i}$
      - Given cluster membership, all members of cluster are independent
   - Also known as an [independence model]{.alert} of within-cluster variation
-  The typical member-to-member deviation in the cluster-specific level of the response is $\sqrt{\sigma^2_\epsilon}$

---

## Variance/covariance of the outcome

$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

Then the variance of a single outcome measure is:
$$\begin{align*}Var[Y_{ij}] &= Var[\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\epsilon_{ij}]\\
&= Var[\vec{Z}_ib_{0i}+\epsilon_{ij}]\\
&= \sigma^2_b + \sigma^2_{\epsilon}\end{align*}$$

. . .

And the covariance between 2 outcome measures in the same cluster is:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= Cov[\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\epsilon_{ij}, \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\epsilon_{ik}]\\
&= Cov[\vec{Z}_ib_{0i}+\epsilon_{ij}, \vec{Z}_ib_{0i}+\epsilon_{ik}]\\
&= Cov[\vec{Z}_ib_{0i}, \vec{Z}_ib_{0i}] + Cov[\epsilon_{ij}, \vec{Z}_ib_{0i}] + Cov[\vec{Z}_ib_{0i}, \epsilon_{ik}] + Cov[\epsilon_{ij}, \epsilon_{ik}]\\
&= \sigma^2_b + 0 + 0 + 0\\
&= \sigma^2_b\end{align*}$$

. . .

[In 552, we were used to seeing $Cov[Y_{ij}, Y_{ik}]=0$!]{.alert}

---

## What does this covariance matrix look like?

$$\begin{align*}Cov[\vec{Y}_i] &= \vec{Z}_i\sigma^2_b\boldsymbol{1}\vec{Z}_i^T + \boldsymbol{R}_i\\
&= [\text{cluster-to-cluster variation}] + [\text{within-cluster member-to-member variation}]\\
&=\begin{bmatrix}
\sigma^2_b & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b
\end{bmatrix} + \begin{bmatrix}
\sigma^2_\epsilon & 0 & \dots & 0\\
0 & \ddots & \dots & 0\\
\vdots & \dots & \ddots & \vdots\\
0& \dots & 0 & \sigma^2_\epsilon
\end{bmatrix}\\
&=\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}\end{align*}$$

- Known as a [block exchangeable]{.alert} matrix or [compound symmetry]{.alert}

---

## Inference for $\widehat{\beta}$

Estimation of $\beta$ is done by (restricted) maximum likelihood estimation -- the formula looks a lot like it did for weighted least squares (WLS)

$$\widehat{\beta} = \left(\sum_{i=1}^n \boldsymbol{X}^T \left\{Cov[\vec{Y}_i]\right\}^{-1}\boldsymbol{X}_i\right)^{-1}\sum_{i=1}^n \boldsymbol{X}^T \left\{Cov[\vec{Y}_i]\right\}^{-1}\vec{Y}_i$$

::: {.r-stack}

::: {.fragment .fade-in-then-out}
- In OLS and WLS, we used [maximum likelihood]{.alert} to estimate $\beta$'s and variance components 
   
- In LMEs, we use [restricted maximum likelihood (REML)]{.alert} estimation because MLE gives us biased estimates of the covariance components in small samples

- This means that we [cannot]{.alert} perform likelihood ratio test-based inference for $\vec{\beta}$ when using REML!

   - Wald-based inference (z/t-tests and confidence intervals) are still fine, though
:::

::: {.fragment}
The "model-based" covariance matrix for $\widehat{\beta}$ is
$$Cov[\widehat{\beta}]=\left(\sum_{i=1}^n \boldsymbol{X}_i^T\left\{Cov[\vec{Y}_i]\right\}^{-1}\boldsymbol{X}_i^T\right)^{-1}$$
:::

:::

::: {.fragment}
The confidence intervals for $\vec{\beta}$ will take the form:

$$\widehat{\beta} \pm t_{\alpha/2; DoF=n-1} \text{diag}\left(\sum_{i=1}^n \boldsymbol{X}_i^T\left\{Cov[\vec{Y}_i]\right\}^{-1}\boldsymbol{X}_i^T\right)^{-1}$$
:::

---

## Robust inference for $\widehat{\beta}$

We can also use [robust or sandwich variance]{.alert} estimators to perform inference on $\widehat{\beta}$

$$Cov^*[\widehat{\beta}] = B^{-1} A B^{-1}$$

where
$$\begin{align*}&B^{-1} = Cov[\widehat{\beta}] = \left[\sum_{i=1}^n \boldsymbol{X}_i^T \left\{Cov[\vec{Y}_i]\right\}^{-1} \boldsymbol{X}_i\right]^{-1}\\
&A = \left[\sum_{i=1}^n \boldsymbol{X}_i^T \left\{Cov[\vec{Y}_i]\right\}^{-1} (Y_i - \hat{\mu}_i)(Y_i - \hat{\mu}_i)^T \left\{Cov[\vec{Y}_i]\right\}^{-1} \boldsymbol{X}_i\right]\end{align*}$$

::: {.r-stack}

::: {.fragment .fade-in-then-out}
This will give us (approximately) correct variance estimates for $\widehat{\beta}$ if we're [wrong]{.alert} about the random effects or the variance of $Y$
:::

::: {.fragment}
If our random intercepts [correctly]{.alert} model the covariance between observations, $A \approx B$ and:
$$\begin{align*}Cov^*[\widehat{\beta}] &= B^{-1} A B^{-1}\\
&\approx B^{-1} B B^{-1}\\
& = B^{-1} = Cov[\widehat{\beta}]\end{align*}$$
the robust variance is the same as the model-based variance!
:::

::: {.fragment}
![](clustering-SE-meme){.absolute top=150 right=100 width="20%"}
:::

:::

---

## ICC in LMEs

It is also very intuitive to calculate the ICC from an LME:
$$\begin{align*}\text{ICC} &= \frac{\text{variation in cluster means}}{\text{variation in cluster means } + \text{individual-level variation}}\\
&= \frac{\sigma^2_b}{\sigma^2_b + \sigma^2_{\epsilon}}\end{align*}$$

<!-- --- -->

<!-- ## Distributional assumptions -->

<!-- LMEs assume that the outcome follows a [multivariate normal]{.alert} distribution -->

---

## Example: radon measurements in Minnesota

Let's fit an LME model for our Minnesota log-radon measurements:
$$Y_{ij} = \beta_0 + \beta_1 (\text{floor}_{ij}) + b_{0i} + \epsilon_{ij}$$

   - $Y_{ij}$ is the log radon for house $j$ in county $i$
   
   - $\text{floor}_{ij}$ is whether the lowest floor of house $j$ in county $i$ was the basement (0) or the ground floor (1)
   
   - $b_{0i}$ is the county-level random intercept
   
---

## Example: radon measurements in Minnesota

Let's fit an LME model for our Minnesota log-radon measurements:
```{r radon cleaning, echo=F}
radon <- read.table("~/Desktop/teaching/PHS-651/data/Gelman-data/radon/srrs2.dat",header=T,sep=",")

set.seed(081524)

radon.mn <- radon %>% 
   as.data.frame() %>% 
   mutate(log_radon = log(activity +0.1),
          county = str_trim(county)) %>% 
   dplyr::filter(state == "MN")
```

```{r radon lme}
library(lme4)

# run an LME with random intercepts for county #
radon_lme <- lmer(log_radon ~ floor + (1 | county), data=radon.mn)
summary(radon_lme)
```

- LME $\beta_1$: `r round(summary(radon_lme)$coef[2,1],4)`

   - Model-based 95% CI: (`r round(summary(radon_lme)$coef[2,1] - summary(radon_lme)$coef[2,2]*pnorm(0.975), 4)`, `r round(summary(radon_lme)$coef[2,1] + summary(radon_lme)$coef[2,2]*pnorm(0.975), 4)`)

   - How would we interpret this?

---

## Example: radon measurements in Minnesota

Let's compare the model-based confidence intervals

 - Model-based 95% CI: (`r round(summary(radon_lme)$coef[2,1] - summary(radon_lme)$coef[2,2]*pnorm(0.975), 4)`, `r round(summary(radon_lme)$coef[2,1] + summary(radon_lme)$coef[2,2]*pnorm(0.975), 4)`)

to the robust CIs:

```{r radon lme robust}
library(clubSandwich)

# get the sandwich-based covariance matrix #
var_sand <- vcovCR(radon_lme, type="CR0")

paste0( "(",round(summary(radon_lme)$coef[2,1] - sqrt(var_sand[2,2])*pnorm(0.975), 4), ", "
        , round(summary(radon_lme)$coef[2,1] + sqrt(var_sand[2,2])*pnorm(0.975), 4),")")
```

. . .

:::: {.columns}

::: {.column width="50%"}
Model-based SEs:
```{r radon lme model-based}
summary(radon_lme)$coef[,2]
```
:::

::: {.column width="50%"}
Robust SEs:
```{r radon lm robust-var}
sqrt(diag(var_sand))
```
:::

::::

::: {.fragment}
![](clustering-SE-meme){.absolute bottom=-150 right=800 width="20%"}
:::

---

## Example: radon measurements in Minnesota

Let's compare the LME results to adding county as a regression coefficient

. . .

```{r radon lm cty}
radon_cty_lm <- lm(log_radon ~ floor + factor(county) - 1, data=radon.mn)
summary(radon_cty_lm)
```

- OLS $\beta_1$: `r round(summary(radon_cty_lm)$coef[1,1],4)`

   - 95% CI: (`r round(summary(radon_cty_lm)$coef[1,1] - summary(radon_cty_lm)$coef[1,2]*pnorm(0.975), 4)`, `r round(summary(radon_cty_lm)$coef[1,1] + summary(radon_cty_lm)$coef[1,2]*pnorm(0.975), 4)`)
   
. . .

:::: {.columns}

::: {.column width="60%"}
What differences can you see compared to the previous LME?

- LME $\beta_1$: `r round(summary(radon_lme)$coef[2,1],4)`

   - Model-based 95% CI: (`r round(summary(radon_lme)$coef[2,1] - summary(radon_lme)$coef[2,2]*pnorm(0.975), 4)`, `r round(summary(radon_lme)$coef[2,1] + summary(radon_lme)$coef[2,2]*pnorm(0.975), 4)`)
   
   - Robust 95% CI: (`r round(summary(radon_lme)$coef[2,1] - sqrt(var_sand[2,2])*pnorm(0.975), 4)`, `r round(summary(radon_lme)$coef[2,1] + sqrt(var_sand[2,2])*pnorm(0.975), 4)`)
:::

::: {.column width="40%"}
```{r radon CI plot, echo=F}
ci_ols <- rbind(c(2, round(summary(radon_cty_lm)$coef[1,1] - summary(radon_cty_lm)$coef[1,2]*pnorm(0.975), 4)), c(2,round(summary(radon_cty_lm)$coef[1,1] + summary(radon_cty_lm)$coef[1,2]*pnorm(0.975), 4)))

ci_lme <- rbind(c(1.5, round(summary(radon_lme)$coef[2,1] - summary(radon_lme)$coef[2,2]*pnorm(0.975), 4)), c(1.5,round(summary(radon_lme)$coef[2,1] + summary(radon_lme)$coef[2,2]*pnorm(0.975), 4)))

ci_robust <- rbind(c(1,round(summary(radon_lme)$coef[2,1] - sqrt(var_sand[2,2])*pnorm(0.975), 4)), c(1,round(summary(radon_lme)$coef[2,1] + sqrt(var_sand[2,2])*pnorm(0.975), 4)))

plot(NA, xlim=c(-0.8,-0.45), ylim=c(0.5,2.2), xlab="", ylab="", yaxt='n')
segments(ci_ols[1,2], ci_ols[1,1],ci_ols[2,2], ci_ols[2,1], col="red", lwd=2)
text(ci_ols[2,2]+0.03, 2, "OLS CI", col="red")
segments(ci_lme[1,2], ci_lme[1,1],ci_lme[2,2], ci_lme[2,1], col="purple", lwd=2)
text(ci_lme[2,2]+0.05, 1.5, "LME model-based CI", col="purple")
segments(ci_robust[1,2], ci_robust[1,1],ci_robust[2,2], ci_robust[2,1], col="blue", lwd=2)
text(ci_robust[2,2]+0.03, 1, "LME robust CI", col="blue")
```
:::

::::

---

## Marginal effects in LMEs

The default interpretation of LMEs is [conditional]{.alert}... but we can sometimes extract [marginal]{.alert} effects out of them

. . .

- For continuous outcomes (like we've been talking about), or outcomes that we **do not** transform, [the conditional and marginal effects will be identical]{.alert}

$$\begin{align*}E[\vec{Y}_i] &= E\left\{E[\vec{Y}_i|\boldsymbol{X}_i, b_{0i}]\right\}\\
&=E[\boldsymbol{X}_i\vec{\beta} + \vec{Z}_i b_{0i}]\\
&=\boldsymbol{X}_i\vec{\beta} + \vec{Z}_i E[b_{0i}]\\
&=\boldsymbol{X}_i\vec{\beta}\end{align*}$$

. . .

- For non-continuous outcomes or outcomes we do need to transform, the above will not hold and the marginal and conditional effects with be different... more on this next week

---

## A note on terminology

Throughout this lecture we've referred to $\vec{\beta}$ as "fixed effects" and $b_{0i}$ as "random effects"

- This is standard terminology within statistics

. . .

In parts of the social sciences (\*cough\* economics \*cough\*), "fixed effects" refer to adjusting for cluster membership as covariates (think back to the last lecture)

- These types of models are referred to in econ as "fixed effect models"

- This is done to adjust for the (potentially) confounding effect of cluster membership, not so much to deal with correlated data

. . .

This terminology has lead to many an argument between researchers until everyone realizes there was a miscommunication

- Just an FYI in case you're ever reading about random/fixed effects online