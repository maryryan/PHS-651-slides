---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
execute: 
   eval: true
   echo: true
---
```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
```
<h1> Lecture 3: Non-normal outcomes, correlated data, and conditional models </h1>

<h2> PHS 651: Advanced regression methods </h2>

<hr>

<h3> Mary Ryan, PhD </h3>

<h3> September 24, 2024 </h3>

---

## Review: linear mixed effect models

One way to account for clustering in analysis is with a [linear mixed effects]{.alert} model
$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

- $\boldsymbol{X}_i\vec{\beta}$ are the [fixed effects]{.alert} representing the mean response

- $\vec{Z}_ib_{0i}$ are the [random effects]{.alert} accounting for random between-cluster variation

- $\vec{\epsilon}_i$ is the regular between-individual variation

The [fixed effects]{.alert} are shared across everyone in the sample, while the [random effects]{.alert} are unique to a particular cluster

Interpretations of $\beta_1$ in an LME are [conditional]{.alert} because they are comparing 2 individuals with the same cluster membership

- Often referred to as having individual-specific or cluster-specific interpretation

. . .

[We can also extend this model to non-continuous/non-Normal outcome data]{.alert}

---

## Recap: logistic regression

If our outcome $Y_{i}$ is [binary]{.alert}, we can model it using a logistic regression:
$$\log\left(\frac{p_i}{1-p_i}\right) = \boldsymbol{X}_i\vec{\beta} = \beta_0 + W_i\beta_1$$

- We assume $Y_i \sim \text{Binomial}(p_i)$

   - $E[Y_i] = p_i$
   
- The logit function $\log\left(\frac{x}{1-x}\right)$ acts as the [link]{.alert} between $E[Y_i]$ and the linear model $\boldsymbol{X}_i\vec{\beta}$

   - $\left(\frac{x}{1-x}\right)$ are known as the [odds]{.alert} of $w=1$ compared to $w=0$

- $\beta_1$ tells us about the [log odds]{.alert}:

$$\beta_1 = \log\left(\frac{p(w=1)}{1-p(w-1)}\right) - \log\left(\frac{p(w=0)}{1-p(w=0)}\right)$$

- $e^{\beta_1}$ tells us about the [odds ratio]{.alert}:

$$e^{\beta_1} = \frac{\text{odds}(w=1)}{\text{odds}(w=0)}=\frac{\frac{p(w=1)}{1-p(w=1)}}{\frac{p(w=0)}{1-p(w=0)}}$$

---

## Review: logistic regression

Logistic regression is solving 2 problems for us:

1. Using our Binomial distribution assumptions to handle non-constant variance in binary data

   - Remember, variance of binary data is a function of its mean!

2. Transforms the outcome so we don't have to worry about it staying between 0 and 1

. . .

These are common problems when analyzing non-Normal data

- Logistic regression is a specific example of a broader class of [generalized linear models]{.alert}


---

## Generalized linear models

In the independent observation setting, many types of data are modeled using a common [generalized linear model]{.alert} (GLM) structure

$$g(Y_i) = \boldsymbol{X}_i\vec{\beta} + \epsilon_i$$

In GLMs, we estimate the regression coefficients $\vec{\beta}$ as the solution to the [estimating equation]{.alert}:
$$\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T v_i^{-1}[Y_i - \mu_i(\beta)] = 0$$

- $E[Y_i]=\mu_i(\beta) = \boldsymbol{X}\vec{\beta}$

- $Var[Y_i] = v_i = \sigma^2$

- The estimating equation is actually just $\partial f(y_i)/\partial \beta$ (AKA: the score)

. . .
   
Look similar?

$$\sum_{i=1}^n\boldsymbol{X}^T\sigma^2[Y_i - \boldsymbol{X}\vec{\beta}]=0 $$

OLS is a type of GLM!

. . .

This estimating equation is based on the [likelihood function]{.alert} associated with the distribution we assume the outcome is coming from

   - Many types of data distributions have likelihoods with a similar form - GLMs exploit that

---

## Exponential family

Many common types of data distributions belong to a family called the [exponential family]{.alert}

- Called such because their likelihoods can be written in a common form:
$$f(y_i)= \exp\left\{\frac{[y_i\theta_i - \psi(\theta_i)]}{\varphi} + c(y_i,\varphi)\right\}$$
   
   - $\theta_i$ is the natural parameter
   
   - $\psi(\cdot)$ related $\theta_i$ to $E[Y_i]$ through $E[Y_i]=\partial\psi(\theta_i)/\partial \theta_i$
   
   - $\varphi$ is a variance scaling parameter
   
   - $c(y_i,\varphi)$ is a catch-all for everything else
   
---

## A caveat: exponential family (cont.)
   
Examples:

- Normal: $\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(y_i-\mu_i)^2}{2\sigma^2}\right\}=\exp\left\{\frac{y_i\mu_i - 1/2\mu_i^2}{\sigma^2} -\frac{1}{2} \log(2\pi\sigma^2)-\frac{y_i^2}{2\sigma^2}\right\}$

   - $\theta_i = \mu_i$
   
   - $\psi(\theta_i) = 1/2 \mu_i^2$
   
   - $\varphi = \sigma^2$
   
   - $c(y_i,\varphi) = -\frac{1}{2} \log(2\pi\sigma^2)-\frac{y_i^2}{2\sigma^2}$

- Binomial: ${n_i\choose n_i y_i}\pi_i^{n_i y_i})(1-\pi_i)^{n_i - n_i y_i} = \exp\left\{\frac{y_i \theta_i - \log[1+\exp(\theta_i)]}{1/n_i} + \log\left[{n_i \choose n_i y_i} \right]\right\}$

   - $\theta_i = \log\left[\frac{\pi_i}{1-\pi_i}\right]$
   
   - $\psi(\theta_i) = \log[1+\exp(\theta_i)]$
   
   - $\varphi = 1/n_i$
   
   - $c(y_i,\theta_i) = \log\left[{n_i \choose n_i y_i} \right]$

- Poisson: $\frac{\exp\{-\mu_i\}\mu_i^{y_i}}{y_i!} = \exp\{y_i\log(\mu_i) - \mu_i - \log(y_i!)\}$

   - $\theta_i = \log(\mu_i)$
   
   - $psi(\theta_i) = \exp(\theta_i)$
   
   - $\varphi = 1$
   
   - $c(y_i, \theta_i) = -\log(y_i!)$
   
---

## Generalized linear mixed models

We can also use the idea of GLMs in LMEs to model data that is both *clustered* and *non-Normal*

- We call this hybrid a [generalized linear mixed model]{.alert} (GLMMs)

$$g(\vec{Y}_i) = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

   - Assume $[\vec{Y}_i|\boldsymbol{X}_i, b_{0i}] \sim \text{exponential family}$
   
   - Use distribution assumption to pull assume link function $g(\cdot)$ and form of within-cluster variance

```{r, eval=F}
library(MASS)

## fit our GLMM ##
glmm_fit <- glmmPQL( y ~ w, 
                     random=~1,
                     family=binomial,
                     data=df )

## get model summary ##
summary(glmm_fit)
```

---

## Some notes

In order to work out the likelihood function for a GLMM, we usually need to do some approximations

- There are 2 ways to do this:

   1. Approximate maximum likelihood (penalized quasi-likelihood)

   2. Approximate the integral (Gaussian quadrature)

So you might see multiple functions online for running GLMMs in R and SAS

. . .

[We will use]{.alert}

- `glmmPQL()` in R and `PROC GLIMMIX` in SAS

- These both use method 1 above, and can be used with longitudinal models developed later in this course

. . .

Others include:

- `glmmML()` and `glmm()` in R

   - Both use method 2 but can't extend to the longitudinal models we'll get to later
   
- `PROC NLMIXED` in SAS

   - Uses method 2
   
   - Can extend to longitudinal models, but can take a while to run