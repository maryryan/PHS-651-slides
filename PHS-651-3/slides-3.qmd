---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
execute: 
   eval: true
   echo: true
---
```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
```
<h1> Lecture 3: Non-normal outcomes, correlated data, and conditional models </h1>

<h2> PHS 651: Advanced regression methods </h2>

<hr>

<h3> Mary Ryan Baumann, PhD </h3>

<h3> September 24, 2024 </h3>

---

## Recap: linear mixed effect models

One way to account for clustering in analysis is with a [linear mixed effects]{.alert} model
$$\vec{Y}_i = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

- $\boldsymbol{X}_i\vec{\beta}$ are the [fixed effects]{.alert} representing the mean response

- $\vec{Z}_ib_{0i}$ are the [random effects]{.alert} accounting for random between-cluster variation

- $\vec{\epsilon}_i$ is the regular between-individual variation

The [fixed effects]{.alert} are shared across everyone in the sample, while the [random effects]{.alert} are unique to a particular cluster

Interpretations of $\beta_1$ in an LME are [conditional]{.alert} because they are comparing 2 individuals with the same cluster membership

- Often referred to as having individual-specific or cluster-specific interpretation

. . .

[We can also extend this model to non-continuous/non-Normal outcome data]{.alert}

---

## 522 review: logistic regression

If our outcome $Y_{i}$ is [binary]{.alert}, we can model it using a logistic regression:
$$\log\left(\frac{p_i}{1-p_i}\right) = \boldsymbol{X}_i\vec{\beta} = \beta_0 + W_i\beta_1 + \epsilon_i$$

- We assume $Y_i \sim \text{Binomial}(p_i)$

   - $E[Y_i] = p_i$
   
- The logit function $\log\left(\frac{x}{1-x}\right)$ acts as the [link]{.alert} between $E[Y_i]$ and the linear model $\boldsymbol{X}_i\vec{\beta}$

   - $\left(\frac{p_i}{1-p_i}\right)$ are known as the [odds]{.alert} of $w=1$ compared to $w=0$

- $\beta_1$ tells us about the [log odds]{.alert}:

$$\beta_1 = \log\left(\frac{p(w=1)}{1-p(w-1)}\right) - \log\left(\frac{p(w=0)}{1-p(w=0)}\right)$$

- $e^{\beta_1}$ tells us about the [odds ratio]{.alert}:

$$e^{\beta_1} = \frac{\text{odds}(w=1)}{\text{odds}(w=0)}=\frac{\frac{p(w=1)}{1-p(w=1)}}{\frac{p(w=0)}{1-p(w=0)}}$$

---

## 522 review: logistic regression

Logistic regression is solving 2 problems for us:

1. Using our Binomial distribution assumptions to handle non-constant variance in binary data

   - Remember, variance of binary data is a function of its mean!

2. Transforms the outcome so we don't have to worry about predictions going above 1 or below 0

. . .

These are common problems when analyzing many types of non-Normal data

- Logistic regression is a specific example of a broader class of [generalized linear models]{.alert}


---

## Generalized linear models

In the independent observation setting, many types of data are modeled using a common [generalized linear model]{.alert} (GLM) structure

$$g(Y_i) = \boldsymbol{X}_i\vec{\beta} + \epsilon_i$$

- $g(\cdot)$ is known as the [link function]{.alert} because it *links* a linear model on the righthand side to the outcome on the lefthand side

- $\epsilon_i$ will still be Normally distributed, but its variance $\sigma^2_{\epsilon}$ will match the distribution of $Y_i$

---

## Generalized linear models

In GLMs, we estimate the regression coefficients $\vec{\beta}$ as the solution to the [estimating equation]{.alert}:
$$\sum_{i=1}^n \left(\frac{\partial \mu_i}{\partial \beta}\right)^T v_i^{-1}[Y_i - \mu_i(\beta)] = 0$$

- $E[Y_i]=\mu_i(\beta) = \boldsymbol{X}\vec{\beta}$

- $Var[Y_i] = v_i = \sigma^2$

- The estimating equation is actually just the derivative of the likelihood with respect to regression parameters $\beta$ (AKA: the score $\partial f(y_i)/\partial \beta$)

. . .
   
Look similar?

$$\sum_{i=1}^n\boldsymbol{X}^T\sigma^2[Y_i - \boldsymbol{X}\vec{\beta}]=0 $$

OLS is a type of GLM!

. . .

This estimating equation is based on the [likelihood function]{.alert} associated with the distribution we assume the outcome is coming from

   - Many types of data distributions have likelihoods with a similar form called the [exponential family form]{.alert} - GLMs exploits this

---

## Exponential family

Exponential family distributions share some common statistical properties

- Variance of $Y_i$ can be expressed in terms of
$$\text{Var}[Y_i] = \phi v(\mu_i),$$

   - $\phi$ is a scale parameter $>0$
   
   - $v(\mu_i)$ is the variance function that describes how the outcome variance $\text{Var}[Y_i]$ is related to the mean of the outcome $\mu_i$
   
---

## Exponential family and GLMs

| Distribution | Var. function $v(\mu)$ | Link function                                                                     |
|--------------|------------------------|-----------------------------------------------------------------------------------|
| Normal       | $v(\mu)=1$             | Identity: $g(\mu) = \mu = \boldsymbol{X}_i\vec{\beta}$                            |
| Bernoulli    | $v(\mu)=\mu(1-\mu)$    | Logit: $g(\mu) = \log\left(\frac{\mu}{1-\mu}\right)= \boldsymbol{X}_i\vec{\beta}$ |
| Poisson      | $v(\mu)=\mu$           | Log: $g(\mu) = \log(\mu)= \boldsymbol{X}_i\vec{\beta}$                            |

---

## Exponential family and GLMs: examples

Normal distribution

- Assume that $g(\cdot)$ is the identity function $g(\mu) = \mu$

- Model is then

$$g(E[Y_i]) = \mu_i = \boldsymbol{X}_i\vec{\beta}$$

a standard linear regression model

- Variance is $\text{Var}[Y_i] = \phi$ (independent of mean)

---

## Exponential family and GLMs: examples

Bernoulli distribution

- Assume that $g(\cdot)$ is the logit function $g(\mu) = \log\left(\frac{\mu}{1-\mu}\right)$

- Model is then

$$g(E[Y_i]) = \log\left(\frac{\mu_i}{1-\mu_i}\right) = \boldsymbol{X}_i\vec{\beta}$$

a logistic regression model

- Variance is $\text{Var}[Y_i] = \mu_i(1-\mu_i)$ (*dependent* on mean)
<!-- ## Exponential family -->

<!-- Many common types of data distributions belong to a family called the [exponential family]{.alert} -->

<!-- - Called such because their likelihoods can be written in a common form: -->
<!-- $$f(y_i)= \exp\left\{\frac{[y_i\theta_i - \psi(\theta_i)]}{\varphi} + c(y_i,\varphi)\right\}$$ -->

<!--    - $\theta_i$ is the natural parameter -->

<!--    - $\psi(\cdot)$ related $\theta_i$ to $E[Y_i]$ through $E[Y_i]=\partial\psi(\theta_i)/\partial \theta_i$ -->

<!--    - $\varphi$ is a variance scaling parameter -->

<!--    - $c(y_i,\varphi)$ is a catch-all for everything else -->

<!-- --- -->

<!-- ## Exponential family examples -->

<!-- Normal: $\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(y_i-\mu_i)^2}{2\sigma^2}\right\}=\exp\left\{\frac{y_i\mu_i - 1/2\mu_i^2}{\sigma^2} -\frac{1}{2} \log(2\pi\sigma^2)-\frac{y_i^2}{2\sigma^2}\right\}$ -->

<!-- - $\theta_i = \mu_i$ -->

<!-- - $\psi(\theta_i) = 1/2 \mu_i^2$ -->

<!-- - $\varphi = \sigma^2$ -->

<!-- - $c(y_i,\varphi) = -\frac{1}{2} \log(2\pi\sigma^2)-\frac{y_i^2}{2\sigma^2}$ -->

<!-- --- -->

<!-- ## Exponential family examples -->

<!-- Binomial: ${n_i\choose n_i y_i}\pi_i^{n_i y_i})(1-\pi_i)^{n_i - n_i y_i} = \exp\left\{\frac{y_i \theta_i - \log[1+\exp(\theta_i)]}{1/n_i} + \log\left[{n_i \choose n_i y_i} \right]\right\}$ -->

<!-- - $\theta_i = \log\left[\frac{\pi_i}{1-\pi_i}\right]$ -->

<!-- - $\psi(\theta_i) = \log[1+\exp(\theta_i)]$ -->

<!-- - $\varphi = 1/n_i$ -->

<!-- - $c(y_i,\theta_i) = \log\left[{n_i \choose n_i y_i} \right]$ -->

<!-- --- -->

<!-- ## Exponential family examples -->

<!-- Poisson: $\frac{\exp\{-\mu_i\}\mu_i^{y_i}}{y_i!} = \exp\{y_i\log(\mu_i) - \mu_i - \log(y_i!)\}$ -->

<!-- - $\theta_i = \log(\mu_i)$ -->

<!-- - $psi(\theta_i) = \exp(\theta_i)$ -->

<!-- - $\varphi = 1$ -->

<!-- - $c(y_i, \theta_i) = -\log(y_i!)$ -->
   
---

## Generalized linear mixed models

We can also use the idea of GLMs in LMEs to model data that is both *clustered* and *non-Normal*

- We call this hybrid a [generalized linear mixed model]{.alert} (GLMMs)

$$g(\vec{Y}_i) = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}+\vec{\epsilon}_i$$

   - Assume $[\vec{Y}_i|\boldsymbol{X}_i, b_{0i}] \sim \text{exponential family}$
   
   - Use distribution assumption to pull assume link function $g(\cdot)$ and form of within-cluster variance

```{r, eval=F}
library(MASS)

## fit our GLMM ##
glmm_fit <- glmmPQL( y ~ w, 
                     random=~1,
                     family=binomial,
                     data=df )

## get model summary ##
summary(glmm_fit)
```

---

## Some notes

In order to work out the likelihood function for a GLMM, we usually need to do some approximations

- There are 2 ways to do this:

   1. Approximate maximum likelihood (penalized quasi-likelihood)

   2. Approximate the integral (Gaussian quadrature)

So you might see multiple functions online for running GLMMs in R and SAS

. . .

[We will use]{.alert}

- `glmmPQL()` in R and `PROC GLIMMIX` in SAS

- These both use method 1 above, and can be used with longitudinal models developed later in this course

. . .

Others include:

- `glmmML()` and `glmm()` in R

   - Both use method 2 but can't extend to the longitudinal models we'll get to later
   
- `PROC NLMIXED` in SAS

   - Uses method 2
   
   - Can extend to longitudinal models, but can take a while to run
   
---

## Marginal effects in GLMMs

Last time we noted that marginal and conditional effects are *identical* in LMEs

- This is not the case for GLMMs

Take a logistic mixed effects model:

$$\text{logit}(\vec{Y}_i) = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i} + \vec{\epsilon}_i$$

. . .

- The conditional mean of our outcome is then:
$$logit(E[\vec{Y}_i|\boldsymbol{X}_i, b_{0i}]) = \boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i}$$

. . .

In order to get the marginal effects, we would need to inverse-transform the outcome:
$$E[\vec{Y}_i|\boldsymbol{X}_i, b_{0i}] = \frac{\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}{1+\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}$$

and then take the marginal expectation of that:
$$\begin{align*}E[\vec{Y}_i] &= E\left\{E[\vec{Y}_i|\boldsymbol{X}_i, b_{0i}]\right\}\\
&=E\left[\frac{\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}{1+\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}\right]\end{align*}$$

. . .

- Clearly, $E[\vec{Y}_i] \ne \frac{\exp(\boldsymbol{X}_i\vec{\beta})}{1+\exp(\boldsymbol{X}_i\vec{\beta})}$

---

## Marginal effects in GLMMs

So what happens when we have transformed outcomes and want marginal effects?

. . .

- You could integrate out the random effects:

$$E\left[\frac{\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}{1+\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}\right] = \int_{-\infty}^{\infty}\frac{\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})}{1+\exp(\boldsymbol{X}_i\vec{\beta} + \vec{Z}_ib_{0i})} f(b_{0i})db_{0i}$$

but this generally doesn't end up with a nice formulaic solution...

. . .

- You could use an approximation:
$$\text{logit}(\vec{Y}_i) = \frac{\boldsymbol{X}_i\vec{\beta}}{\sqrt{(1+0.346\sigma^2_b)}}$$

but this is very sensitive to any model misspecification and can be biased

. . .

Maybe what we need are specific [marginal models]{.alert} that incorporate clustering... (stay tuned for next week)