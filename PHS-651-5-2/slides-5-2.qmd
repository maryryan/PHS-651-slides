---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```

<h1>Lecture 5.2: Diagnostics</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>October 10, 2024</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=5 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/PHS-651-5-2/slides-5-2>

:::

::: {.absolute bottom=0 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/PHS-651-5-2/slides-5-2"))
```
:::

---


## Model diagnostics

So far we've dealt with how to model correlated data

- But how do we assess whether the modeling choices we've made are sound?

. . .

There are some analyses/diagnostics we can perform to explore this

. . .

- Heavy emphasis on [explore]{.alert}

   - We know that our model likely won't perfectly reflect the true form/relationships in the data
   
   - But we should assess whether there are [major discrepancies]{.alert} between what we've assumed and what's in the data
   
   - Use these as a temperature check for whether the model needs to be adjusted, how much confidence can be placed in estimates, and whether estimates need to be presented with caveats

. . .

Also, in the non-longitudinal clustered data settings, there are not many diagnostic checks for GEE models

- This is because we aren't *assuming* much for GEEs - we're estimating most of the covariance matrix

- So we'll mostly be focusing on mixed models in this session

---

## Residual analysis

[Residuals]{.alert} are the difference between the observed outcomes and the ones predicted by your model

$$r_{ij} = Y_{ij} - \boldsymbol{X}_{ij}\widehat{\vec{\beta}}$$

. . .

If the model [adequately captures]{.alert} the systematic trend of the response, then [the residuals should exhibit no systematic pattern]{.alert}

- i.e., they should be scattered fairly randomly around zero

---

## Residuals vs predicted/fitted response

Residuals can be used to check for systematic departures from the model for the mean response

- We often do this by making a scatter plot of residuals vs predicted values

   - Fitting a smoothed curve (e.g., lowess) through the scatter plot is helpful in this respect
   
. . .

![](residual-plot-example.png){width=1000}

---

## Residuals vs predicted response
   
Usually, if there is any systematic pattern in the residual indicates inadequacy of the model in some way

- Solution: re-fit the data with a more flexible model (e.g., interactions, quadratic terms, and perhaps more covariates)

![](residual-plot-example.png){width=1000}

---

## Residuals vs predicted response

In R, we can do this by plotting the model object:
```{r radon data clean, echo=F}
radon <- read.table("~/Desktop/teaching/PHS-651/data/Gelman-data/radon/srrs2.dat",header=T,sep=",")

set.seed(081524)

radon.mn <- radon %>% 
   as.data.frame() %>% 
   mutate(log_radon = log(activity +0.1),
          county = str_trim(county)) %>% 
   dplyr::filter(state == "MN")
```

```{r resid plot}
library(lme4)

# run an LME with random intercepts for county #
radon_lme <- lmer(log_radon ~ floor + (1 | county), data=radon.mn)

plot(radon_lme)
```

---

## Residuals vs predicted response


In SAS, we can add the "plots=residualpanel" option to the end of PROC GLIMMIX

```{sas eval=F}
PROC GLIMMIX data=dance plots=residualpanel;
        class county floor(REF="0");
        model log_radon = floor / dist = normal solution cl;
        random intercept / subject=county;
run;
```

- Our residuals plot would be the one in the top left

::: {.absolute right=0 bottom=0}
![](residualpanel.png){width=1000}
:::

---

## Investigating dispersion in Poisson models

Recall that Poisson GLMMs assume that the mean and variance of the data are the same

- When this assumption is broken, we say that [dispersion]{.alert} is present

- If variance $>$ mean, we have [over]{.alert}dispersion

- If variance $<$ mean, we have [under]{.alert}dispersion (less common)

. . .

If dispersion is present this is an indicator that we likely want to fit something like a Negative Binomial GLMM that allows variance $\ne$ mean

- If that does not fix the issue, it may be that the dispersion is caused by "excess" zeros - in this case, something like a zero-inflated Poisson or NB model might work better

---

## Investigating dispersion in Poisson models

To see if dispersion is present, we can calculate the dispersion parameter using the squared sum of Pearson residuals:
$$\widehat\phi = \frac{\sum[\text{Pearson Residual}_{ij}^2]}{\text{Residual degrees of freedom}}$$

- Pearson residuals correct for the unequal variance in the raw residuals by dividing by the standard deviation - this includes an estimate of the dispersion paramter (which we're extracting above)

- $\hat\phi$ should be 1 if there is absolutely no dispersion

- Will usually expect this not to be *exactly* 1, but major departures (rule of thumb: $<0.75$ or $>1.4$) indicate a NB model is likely better

---

## Investigating dispersion in Poisson models

SAS calculates and displays $\hat\phi$ in its default result output

:::: {.columns}
::: {.column width="50%"}
- For PROC GLIMMIX method=QUAD, the dispersion parameter is listed under "Pearson Chi-Square / DF" in the "fit statistics for conditional distribution" table

![](glimmix_quad.png){width=700}
:::

::: {.column width="50%"}
- For PROC GLIMMIX with PQL, the dispersion parameter is listed under "Gener. Chi-Square / DF" in the "fit statistics" table

![](glimmix_pql.png){width=700}
:::
::::

<!-- https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_glimmix_examples19.htm  -->

---

## Investigating dispersion in Poisson models

In R, we can calculate the dispersion parameter by hand:
```{r awards, echo=F}
library(foreign)
awards <- read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
awards$cid <- factor(awards$cid)
```
```{r dispersion}
library(lme4)
awards_quad <- glmer( awards ~ write + (1 | cid),
                      family = poisson(link="log"), data = awards,
                      control=glmerControl(optimizer="bobyqa") )

# estimate dispersion parameter #
# pearson weighted residual sum of squares divided by degrees of freedom #
# if not dispersion, we want this to be 1 #
summary(awards_quad)$devcomp$cmp["pwrss"] / summary(awards_quad)$AICtab["df.resid"]
# can also calculate the pearson RSS yourself #
sum(resid(awards_quad, type="pearson")^2) / summary(awards_quad)$AICtab["df.resid"]
```

. . .

This dispersion parameter is very small, so the data is likely very under-dispersed

---

## Investigating dispersion in Poisson models

We can also use the `testDispersion()` function
```{r testDispersion}
# can use the testDisperson() function from DHARMa as well #
library(DHARMa)
testDispersion(awards_quad, type="PearsonChisq")
```

. . .

- Unfortunately, can't really get a dispersion parameter estimate for a model using PQL in R

. . .

If there's dispersion in the mixed model, there will (likely) be dispersion in the marginal model

- With GEEs you can combat this with robust standard errors, even if you don't specify a NB marginal variance

---

## Leverage

We may also be interested in whether we have particularly influential observations in our data that are (overly) influencing our model estimates - we investigate this via "leverage" or Cook's distance plots

---

## Leverage 

```{r leverage, out.width="100%"}
library(performance)
plot(check_model(radon_lme))
```

---

## DFBETA for GEEs

A similar measure of influential observations for GEEs is DFBETA - it's the change in a regression coefficient if the $ij^{th}$ observation were removed

- Most functions/procedures will plot DFBETAs for all coefficients in the regression

In SAS, we use the PLOTS(UNPACK)=DFBETA option:
```{sas DFBETA, eval=F}

proc genmod data=dance PLOTS(UNPACK)=DFBETA;
        class village_id dance_class_bin(REF="0");
        model gaitfollowup = dance_class_bin / dist = normal;
        repeated subject=village_id / type=exch;
run;
```

:::: {.columns}
::: {.column widht="60%"}
- We're looking for observations that are clearly apart from the "herd"

- We could also look at the standardized DFBETA by using PLOTS(UNPACK)=DFBETAS
:::
::::

::: {.absolute right=-100 bottom=50}
![](DFBETA.png){width=900}
:::

---

## Other diagnostics for GEEs

While we don't do a lot of model testing for GEEs, we can compare the empirical estimates of the covariance structure to the model-based estimates

- For this we can compare the model-based and robust confidence intervals of regression coefficients

- If there is a large discrepancy, our robust variance estimator is probably doing the heavy lifting of correcting this for us

- In this setting (non-longitudinal clustered data) though, we aren't likely to see much discrepancy

