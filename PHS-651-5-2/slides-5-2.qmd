---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
```

<h1>Lecture 5.2: Diagnostics</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>October 10, 2024</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=5 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/PHS-651-5-2/slides-5-2>

:::

::: {.absolute bottom=0 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/PHS-651-5-2/slides-5-2"))
```
:::

---


## Model diagnostics

So far we've dealt with how to model correlated data

- But how do we assess whether the modeling choices we've made are sound?

. . .

There are some analyses/diagnostics we can perform to explore this

. . .

- Heavy emphasis on [explore]{.alert}

   - We know that our model likely won't perfectly reflect the true form/relationships in the data
   
   - But we should assess whether there are [major discrepancies]{.alert} between what we've assumed and what's in the data
   
   - Use these as a temperature check for whether the model needs to be adjusted, how much confidence can be placed in estimates, and whether estimates need to be presented with caveats

. . .

Also, in the non-longitudinal clustered data settings, there are not many diagnostic checks for GEE models

- This is because we aren't *assuming* much for GEEs - we're estimating most of the covariance matrix

- So we'll mostly be focusing on mixed models in this session

---

## Modeling choices

Let's review the various modeling choices we've covered so far

First, we can choose between conditional/mixed vs marginal models

- Usually this will be driven by whether we want a conditional or population-averaged interpretation of our effects

:::: {.columns}
::: {.column width="50%"}
::: {.fragment fragment-index=1}
Within conditional/mixed models, we make decisions about:

- Variable for random intercepts

- [Variable(s) for fixed effects (isolating causal pathway)]{.fragment .highlight-red fragment-index=3}

- [Outcome distribution]{.fragment .highlight-red fragment-index=3}

- Link function

- For GLMMs: estimation algorithm (PQL vs Gaussian quadrature)

- Model-based vs robust confidence intervals
:::
:::

::: {.column width="50%"}
::: {.fragment fragment-index=2}
Within marginal models, we make decisions about:

- [How to specify the mean model/what variables to include to isolate causal pathway]{.fragment .highlight-red fragment-index=3}

- Marginal variance function

- Correlation structure

- Model-based vs robust confidence intervals
:::
:::
::::

---

## Residual analysis

[Residuals]{.alert} are the difference between the observed outcomes and the ones predicted by your model

$$r_{ij} = Y_{ij} - \boldsymbol{X}_{ij}\widehat{\vec{\beta}}$$

If the model [adequately captures]{.alert} the systematic trend of the response, then [the residuals should exhibit no systematic pattern]{.alert}

- i.e., they should be scattered fairly randomly around zero

. . .

In mixed models, we also have conditional residuals:
$$r^c_{ij} = Y_{ij} - \boldsymbol{X}_{ij}\widehat{\vec{\beta}} - Z_i \hat{b}_{i0}$$

- $r_{ij}$ would then be known as the marginal residuals

---

## Residuals vs predicted/fitted response

Residuals can be used to check for systematic departures from the model for the [mean response]{.alert}

- We often do this by making a scatter plot of residuals vs predicted values

   - Fitting a smoothed curve (e.g., lowess) through the scatter plot is helpful in this respect

![](residual-plot-example.png){width=1000}

---

## Residuals vs predicted response
   
Usually, if there is any systematic pattern in the residual indicates inadequacy of the model in some way

- Solution: re-fit the data with a more flexible model (e.g., interactions, quadratic terms, and perhaps more covariates)

![](residual-plot-example.png){width=1000}

::: {.absolute right=0 bottom=400 width="45%"}
- Plots of [conditional]{.alert} residuals will tell you about the adequacy of the fixed effects

- Plots of the [marginal]{.alert} residuals will tell you about how well the data fits your assumptions about the random intercepts (zero mean)
:::

---

## Residuals vs predicted response

In R, we can plot the model object to get a conditional residuals plot:
```{r radon data clean, echo=F}
radon <- read.table("~/Desktop/teaching/PHS-651/data/Gelman-data/radon/srrs2.dat",header=T,sep=",")

set.seed(081524)

radon.mn <- radon %>% 
   as.data.frame() %>% 
   mutate(log_radon = log(activity +0.1),
          county = str_trim(county)) %>% 
   dplyr::filter(state == "MN")
```

```{r resid plot}
library(lme4)

# run an LME with random intercepts for county #
radon_lme <- lmer(log_radon ~ floor + (1 | county), data=radon.mn)

plot(radon_lme, smooth=T)
```

::: {.absolute right=100 top=450}
- Points fairly randomly distributed around 0

- Smoothed line not showing significant trends
:::

---

## Residuals vs predicted response

We can also get the marginal residuals:

:::: {.columns}
::: {.column width="50%"}
```{r marg resid plot}
library(HLMdiag)

# get various standardized and marginal residuals #
radon_resid <- hlm_resid(radon_lme,standardize=TRUE)

colnames(radon_resid)
```
:::

::: {.column width="50%"}
```{r marg resid plot2}
radon_resid %>% 
   ggplot(aes(x=.mar.fitted, y=.chol.mar.resid))+
   geom_point() +
   geom_smooth(se=F) +
   theme(text = element_text(size = 20)) 
```
:::
::::

- Since `floor` only takes on 2 values, our marginal fitted values only take on 2 values

---

## Residuals vs predicted response

We can also use `hlm_resid()` to get a nicer-looking conditional residuals plot than base R provides

```{r marg resid plot3}
radon_resid %>% 
   ggplot(aes(x=.fitted, y=.std.resid))+
   geom_point() +
   geom_smooth(se=F) +
   theme(text = element_text(size = 20)) 
```

---

## Residuals vs predicted response


In SAS, we can add the `plots=residualpanel` option to the end of `PROC GLIMMIX`

```{sas eval=F}
PROC GLIMMIX data=dance plots=residualpanel;
        class village_id dance_class_bin(REF="0");
        model gaitfollowup = dance_class_bin / dist = normal solution cl;
        random intercept / subject=village_id;
run;
```

::: {.absolute width="45%"}
- Our residuals vs fitted plot would be the one in the top left

- `residualspanel` defaults to conditional residuals

   - we can have it also print marginal residuals by changing the option to `plots=residualspanel(marginal conditional)`
:::

::: {.absolute right=0 bottom=0}
![](residualpanel.png){width=1000}
:::

---

## Investigating dispersion in Poisson models

Recall that Poisson GLMMs assume that the mean and variance of the data are the same

- When this assumption is broken, we say that [dispersion]{.alert} is present

- If variance $>$ mean, we have [over]{.alert}dispersion

- If variance $<$ mean, we have [under]{.alert}dispersion

. . .

If dispersion is present this is an indicator that we likely want to fit something like a Negative Binomial GLMM that allows variance $\ne$ mean

- If that does not fix the issue, it may be that the dispersion is caused by "excess" zeros - in this case, something like a zero-inflated Poisson or NB model might work better

---

## Investigating dispersion in Poisson models

To see if dispersion is present, we can calculate the dispersion parameter using the squared sum of Pearson residuals:
$$\widehat\phi = \frac{\sum[\text{Pearson Residual}_{ij}^2]}{\text{Residual degrees of freedom}}$$

- Pearson residuals correct for the unequal variance in the raw residuals by dividing by the standard deviation - this includes an estimate of the dispersion paramter (which we're extracting above)

- $\hat\phi$ should be 1 if there is absolutely no dispersion

- Will usually expect this not to be *exactly* 1, but major departures (rule of thumb: $<0.75$ or $>1.4$) indicate a NB model is likely better

---

## Investigating dispersion in Poisson models

SAS calculates and displays $\hat\phi$ in its default result output

:::: {.columns}
::: {.column width="50%"}
- For `PROC GLIMMIX method=QUAD`, the dispersion parameter is listed under "Pearson Chi-Square / DF" in the "fit statistics for conditional distribution" table

![](glimmix_quad.png){width=700}
:::

::: {.column width="50%"}
- For `PROC GLIMMIX` with PQL, the dispersion parameter is listed under "Gener. Chi-Square / DF" in the "fit statistics" table

![](glimmix_pql.png){width=700}
:::
::::

<!-- https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_glimmix_examples19.htm  -->

---

## Investigating dispersion in Poisson models

In R, we can calculate the dispersion parameter by hand:
```{r awards, echo=F}
library(foreign)
awards <- read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
awards$cid <- factor(awards$cid)
```
```{r dispersion}
library(lme4)
awards_quad <- glmer( awards ~ write + (1 | cid),
                      family = poisson(link="log"), data = awards,
                      control=glmerControl(optimizer="bobyqa") )

# estimate dispersion parameter #
# pearson weighted residual sum of squares divided by degrees of freedom #
# if not dispersion, we want this to be 1 #
summary(awards_quad)$devcomp$cmp["pwrss"] / summary(awards_quad)$AICtab["df.resid"]
# can also calculate the pearson RSS yourself #
sum(resid(awards_quad, type="pearson")^2) / summary(awards_quad)$AICtab["df.resid"]
```

. . .

This dispersion parameter is very small, so the data is likely very under-dispersed

---

## Investigating dispersion in Poisson models

We can also use the `testDispersion()` function
```{r testDispersion}
# can use the testDisperson() function from DHARMa as well #
library(DHARMa)
testDispersion(awards_quad, type="PearsonChisq")
```

- This provides a formal test for whether $\hat{\phi} \ne 0$ in addition to calculating the estimated $\phi$

- I would generally [not]{.alert} pay much attention to the p-value - focus on the estimated value for $\phi$

. . .

- Unfortunately, can't really get a dispersion parameter estimate for a model using PQL in R

. . .

If there's dispersion in the mixed model, there will (likely) be dispersion in the marginal model

- With GEEs you can combat this with robust standard errors, even if you don't specify a NB marginal variance

---

## Leverage

We may also be interested in whether we have particularly influential observations in our data that are (overly) influencing our model estimates - we investigate this via "leverage" or Cook's distance plots

- Leverage looks at how removing an observation/cluster affects predicted values/residuals

- Cook's distance looks at how removing an observation/cluster affects regression coefficients when combined

We can also see whether we have any particularly influential clusters as well

. . .

Unfortunately `PROC GLIMMIX` doesn't have functionality for influence metrics, but R does

---

## Leverage 

Observation-level leverage/Cook's D
```{r leverage, out.width="65%"}
library(ggpubr)
# looking at observation-level leverage #
radon_leverage_obs <- hlm_influence(radon_lme, leverage="overall")

ggarrange(
dotplot_diag(radon_leverage_obs$leverage.overall, cutoff="internal", name="leverage")+ ylab("Leverage"),
dotplot_diag(radon_leverage_obs$cooksd, cutoff="internal", name="cooks.distance")+ ylab("Cook's distance"),
ncol=2,nrow=1)
```

::: {.absolute right=0 bottom=200 width="30%"}
Looks like we have a lot of observations that are influential based on Cook's distance...
:::

---

## Leverage 

Cluster-level leverage/Cook's D
```{r leverage cluster, out.width="65%"}
# looking at observation-level leverage on the fixed effect estimates #
radon_leverage_cnty <- hlm_influence(radon_lme, leverage="overall", level="county")

ggarrange(
dotplot_diag(radon_leverage_cnty$leverage.overall, cutoff="internal", name="leverage"),
dotplot_diag(radon_leverage_cnty$cooksd, cutoff="internal", name="cooks.distance"),
ncol=2,nrow=1)
```

::: {.absolute right=0 bottom=200 width="30%"}
Just a few key counties that are influential based on Cook's distance...
:::

---

## DFBETA

A similar measure of influential observations is DFBETA - it's the change in a regression coefficient if the $ij^{th}$ observation were removed

- Most functions/procedures will plot DFBETAs for each coefficients in the regression

DFBETA is technically agnostic to model type - should work for both conditional and marginal models

- For SAS, `PROC GLIMMIX` doesn't have any influence statistics built in but `PROC GENMOD` does

- For R, I've had issues getting the `lmer()`, `glmer()`, and `gee()` functions to play nicely with the `dfbeta()` function

---

## DFBETA in `PROC GENMOD`

In SAS, we use the `PLOTS(UNPACK)=DFBETA` option:
```{sas DFBETA, eval=F}
proc genmod data=dance PLOTS(UNPACK)=DFBETA;
        class village_id dance_class_bin(REF="0");
        model gaitfollowup = dance_class_bin / dist = normal;
        repeated subject=village_id / type=exch;
run;
```

:::: {.columns}
::: {.column widht="60%"}
- We're looking for observations that are clearly apart from the "herd"

   - This plot looks fairly random and scattered, though

- We could also look at the standardized DFBETA by using PLOTS(UNPACK)=DFBETAS
:::
::::

::: {.absolute right=-100 bottom=50}
![](DFBETA.png){width=900}
:::

---

## Other diagnostics for GEEs

While we don't do a lot of model testing for GEEs, we can compare the empirical estimates of the covariance structure to the model-based estimates

- For this we can compare the model-based and robust confidence intervals of regression coefficients

- If there is a large discrepancy, our robust variance estimator is probably doing the heavy lifting of correcting this for us

- In this setting (non-longitudinal clustered data) though, we aren't likely to see much discrepancy

