---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
library(corrr)
```

<h1>Lecture 8: Longitudinal GEEs & GLMMs</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>October 29, 2024</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=50 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/PHS-651-8/slides-8>

:::

::: {.absolute bottom=0 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/PHS-651-8/slides-8"))
```
:::

---

## Concepts to practice

Last week we talked about how longitudinal data can impact how we formulate our [mean model]{.alert}

- Need to include [time]{.alert} as a regression coefficient

- May be interested in exposure-by-time [interaction]{.alert}

$$E[Y_{ij}] = \beta_0 + \beta_1 (U_{i0}) + \beta_2 t_{ij} + \beta_3 (U_{i0} \times t_{ij})$$

. . .

We also talked about how it impacts our [correlation/covariance matrix]{.alert}, and the types of correlation structures that may be appropriate

- Exchangeable, Toeplitz, autoregressive, exponential, unstructured...

. . .

But how do we apply correlation/covariance structures to a modeling framework?

. . .

- Two ways:

   1. Adding (additional) random effects to a mixed model to [induce a correlation/covariance structure]{.alert}
   
   2. [Directly applying the correlation structure]{.alert} to the covariance matrix in a GEE [- we'll start here]{.fragment .alert}

---

## Recap: Non-longitudinal GEEs

Recall: a GEE has 2 main components

1. Marginal expectation of the outcome ("the mean model"): $E[Y_{ij}|X_{ij}]=\mu_{ij}$

<br>

2. The covariance of the outcome $V_i$

   2.1 The marginal variance of the outcome: $Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$
   
   <br>
   
   2.2 The within-cluster association of the outcome ("correlation structure")

---

## Longitudinal GEEs

We can easily extend this framework to longitudinal data

[How would we modify each of the components for longitudinal data?]{.alert}

1. Marginal expectation of the outcome

<br>
<br>
<br>

2.1 The marginal variance of the outcome

<br>
<br>
<br>

2.2 The within-cluster association of the outcome ("correlation structure")

<br>
<br>
<br>

---

## Longitudinal GEEs

We can easily extend this framework to longitudinal data

1. Marginal expectation of the outcome

   - Simply add time (and possibly exposure-by-time interactions) to the mean model

$$g(E[Y_{ij}]) = \beta_0 + \beta_1U_{i0} + \beta_2t_{ij} + \beta_3(U_{i0} \times t_{ij})$$

. . .

2.1 The marginal variance of the outcome

   - Will probably stay the same, unless we want different variances at different time points (heterogeneous variance)

$$Var[Y_{ij}|\vec{X}_{ij}] = \varphi v(\mu_{ij})$$

. . .

2.2 The within-cluster association of the outcome ("correlation structure")

   - Can use some of the more complex correlation structures we talked about in Lecture 7!

---

## Example: Exercise therapy trial

Recall the exercise therapy trial from last week:

- Subjects were assigned to one of two weightlifting programs to increase muscle strength

   - Treatment 1: number of repetitions of the exercises was increased as subjects became stronger

   - Treatment 2: number of repetitions was held constant but amount of weight was increased as subjects became stronger

- Measurements of body strength were taken at baseline (0) and on days 2, 4, 6, 8, 10, and 12

```{r exercise, echo=F}
exercise <- read.table("https://content.sph.harvard.edu/fitzmaur/ala2e/exercise-data.txt", sep="", na.strings=".")
colnames(exercise) <- c("ID", "PROGRAM", "day0","day2", "day4", "day6", "day8", "day10", "day12")
exercise_long <- exercise %>% 
   pivot_longer(!(c("ID","PROGRAM")), names_to="time", values_to="strength") %>% 
   mutate(time=parse_number(time))

exercise_long_complete <- exercise_long %>% 
   filter(complete.cases(.))

glimpse(exercise_long)
```

---

## Example: Exercise therapy trial

:::: {.columns}
::: {.column width="40%"}
Some general summary statistics:
```{r exercise summary2, echo=F}
exercise_long %>% 
   summarize(mean=mean(strength,na.rm=T), var=var(strength, na.rm=T))
```

And by time point:

```{r exercise summary, echo=F}
exercise_long %>% 
   group_by(time) %>% 
   summarize(mean=mean(strength,na.rm=T), var=var(strength, na.rm=T))
```
:::

::: {.column width="60%"}
Taking a look at the raw correlation:

```{r correlation}
exercise %>%
   select(day0, day2, day4, day6, day8, day12) %>% 
   correlate(diagonal = 1)
```

[What correlation structure(s) might be appropriate here?]{.alert}
:::
::::

---

## Example: Exercise therapy trial

1. Marginal expectation of the outcome: $E[\text{body strength}_{ij}] = \beta_0 + \beta_1(\text{Program}_i) + \beta_2t_{ij} + \beta_3(\text{Program}_i \times t_{ij})$

2.1 The marginal variance of the outcome: $\sigma^2$

2.2 Correlation structure: autoregressive (equal spacing)

```{r gee, output=F}
library(gee)

# fitting an autoregression, distance 1 correlation structure #
exercise_gee <- gee(strength ~ PROGRAM*time, id=ID,
                    data=exercise_long, corstr="AR-M", Mv=1)
```

---

## Example: Exercise therapy trial

If we fit the GEE...
```{r gee2}
summary(exercise_gee)$coefficients

summary(exercise_gee)$working.correlation

summary(exercise_gee)$scale
```

---

## GLMMs and longitudinal data

Extending GEEs to longitudinal data was fairly straightforward

- Few distributional assumptions means subbing in new mean model and correlation structure is easy

Due to its distributional assumptions, doing the same extension for GLMMs is slightly more involved...

---

## Recap: Non-longitudinal GLMMs

Recall the random effects model we used for non-longitudinal clustered data:
$$g(Y_{ij}) = \beta_0 + \beta_1U_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

- Where we assumed that each cluster had it's own cluster-specific mean ($\beta_0 + b_{0i}$) that varied by some $b_{0i}$ around the overall sample mean $\beta_0$

- We called $b_{0i}$ the [random intercept]{.alert}

. . .

The random intercept induced an [exchangeable]{.alert} covariance matrix:
$$\begin{align*}Var[\vec{Y}_i] &= [\text{cluster-to-cluster variation}] + [\text{within-cluster member-to-member variation}]\\
&=\begin{bmatrix}
\sigma^2_b & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b
\end{bmatrix} + \begin{bmatrix}
\sigma^2_\epsilon & 0 & \dots & 0\\
0 & \ddots & \dots & 0\\
\vdots & \dots & \ddots & \vdots\\
0& \dots & 0 & \sigma^2_\epsilon
\end{bmatrix}=\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}\end{align*}$$

---

## Recap: Non-longitudinal GLMM

Recall the random effects model we used for non-longitudinal clustered data:
$$g(Y_{ij}) = \beta_0 + \beta_1U_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

- Where we assumed that each cluster had it's own cluster-specific mean ($\beta_0 + b_{0i}$) that varied by some $b_{0i}$ around the overall sample mean $\beta_0$

- We called $b_{0i}$ the [random intercept]{.alert}

If $Corr[Y_{ij}, Y_{ik}] = \sigma^2_b/(\sigma^2_b + \sigma^2_\epsilon) = \sigma^2_b/\sigma^2= \rho$, we can rewrite $Var[\vec{Y}_i]$ as:

$$\sigma^2\begin{bmatrix}
1 & \rho & \dots & \rho\\
\rho& \ddots & \dots & \rho\\
\vdots & \dots & \ddots & \vdots\\
\rho & \dots & \rho & 1
\end{bmatrix},$$

which looks much more like the exchangeable covariance structure we see in GEEs!

---

## Recap: Non-longitudinal GLMMs

Recall the random effects model we used for non-longitudinal clustered data:
$$g(Y_{ij}) = \beta_0 + \beta_1U_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

- Where we assumed that each cluster had it's own cluster-specific mean ($\beta_0 + b_{0i}$) that varied by some $b_{0i}$ around the overall sample mean $\beta_0$

- We called $b_{0i}$ the [random intercept]{.alert}

:::: {.columns}
::: {.column width="50%"}
We could also express the covariance matrix using matrix notation:
$$Cov[\vec{Y}_i] = \vec{Z}_i\sigma^2_b\boldsymbol{1}\vec{Z}_i^T + \sigma^2_\epsilon \boldsymbol{I}$$

- This assumes:
$$b_{0i} \sim N(0, \sigma^2_b) ~~~~~~~ \epsilon_{ij} \sim (0, \sigma^2_{\epsilon})$$
:::

::: {.column width="50%"}
::: {.fragment}
Or to write more generally:
$$Cov[\vec{Y}_i] = \vec{Z}_i\boldsymbol{G}\vec{Z}_i^T + \boldsymbol{R_i}$$
:::
:::
::::


---

## Longitudinal GLMMs

Extending the random effects model longitudinal data (could also include interaction):
$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2t_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

what is the interpretation of:

- $\beta_0$:

<br>

- $\beta_0 + b_{0i}$:

<br>

- $\beta_1$:

<br>

- $\beta_2$:

---

## Longitudinal GLMMs

Extending the random effects model longitudinal data:
$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2t_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

:::: {.columns}
::: {.column width="50%"}
The random intercept still induces an [exchangeable]{.alert} covariance matrix:
$$Var[\vec{Y}_i] =\begin{bmatrix}
\sigma^2_b + \sigma^2_\epsilon & \sigma^2_b & \dots & \sigma^2_b\\
\sigma^2_b & \ddots & \dots & \sigma^2_b\\
\vdots & \dots & \ddots & \vdots\\
\sigma^2_b & \dots & \sigma^2_b & \sigma^2_b + \sigma^2_\epsilon
\end{bmatrix}$$
:::

::: {.column width="50%"}
This assumes that every member of cluster $i$ has the same relationship to one another ([same correlation]{.alert}): $\sigma^2_b/(\sigma^2_b + \sigma^2_\epsilon)$

::: {.fragment}
- In longitudinal data, though, cluster $i$ is a single person and the "members" of the cluster are observations collected at [separate time points]{.alert}

- An observation collected at time 1 may relate to observations at times 2 and 3 [differently]{.alert}
:::
:::
::::

---

## Longitudinal GLMMs

Extending the random effects model longitudinal data:
$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2t_{ij} + b_{0i}Z_i+\epsilon_{ij}$$

Additionally, inclusion of only [random intercepts]{.alert} assumes that, though everyone's may randomly vary around some global baseline, everyone has the [same outcome trajectory]{.alert} over time

:::: {.columns}
::: {.column width="50%"}
![](rand-intercept.png){width="80%"}
:::

::: {.column width="50%"}
- Different issue from exposure-by-time interaction

[One solution would be to allow for each cluster (person) to have [variation in its slope]{.alert} as well]{.fragment}

[- Random slopes and intercepts!]{.alert .fragment}
:::
::::

---

## Random slope and intercept model

Consider a model with intercepts [and slopes]{.alert} that vary randomly among individuals:

$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2t_{ij} + b_{0i}Z_{i0} + \color{green}{b_{1i}t_{ij}} + e_{ij}$$

- This model suggests that individuals vary not only in their baseline level of response (when $t_{i1} = 0$), but also in terms of their changes in the response over time

![](rand-slope.png){width="40%"}

---

## Random slope and intercept model

Consider a model with intercepts [and slopes]{.alert} that vary randomly among individuals:

$$g(Y_{ij}) = \beta_0 + \beta_1U_{i0} + \beta_2t_{ij} + b_{0i}Z_{i0} + \color{green}{b_{1i}t_{ij}} + \epsilon_{ij}$$

Here we assume:

$$b_{0i} \sim N(0, \sigma^2_{b0}) ~~~~~ \color{green}{b_{1i} \sim N(0, \sigma^2_{b1})} ~~~~~ \epsilon_{ij} \sim (0, \sigma^2_\epsilon)$$

. . .

We also assume:
$$Cov[b_{0i}, b_{1i}] = \sigma_{b0,b1}$$

- The random slopes and intercepts (may) not be independent!

---

## Variance/covariance of random effects

In fact, the vector of random effects, $\vec{b}_i$, are assumed to have a [multivariate normal distribution]{.alert} with mean zero and covariance matrix $\boldsymbol{G}$

$$\vec{b}_i \sim MVN(\vec{0}, \boldsymbol{G})$$

:::: {.columns}
::: {.column width="50%"}
For example, in the random intercepts and slopes model we considered earlier
$$\boldsymbol{G}=\begin{bmatrix}
\sigma^2_{b0} & \sigma_{b0,b1}\\
\sigma_{b0,b1} &  \sigma^2_{b1}\end{bmatrix}$$
:::

::: {.column width="50%"}
::: {.fragment}
Could also re-formulate $Cov[b_{0i}, b_{1i}]$ to be $\rho\sigma^*_{b0,b1}$, which would get us:
$$\boldsymbol{G} = \begin{bmatrix}
\sigma^2_{b0} & \rho\sigma^*_{b0,b1}\\
\rho\sigma^*_{b0,b1} &  \sigma^2_{b1}\end{bmatrix}$$
:::
:::
::::

---

## Variance/covariance of the outcome

What does this mean for the covariance matrix of the outcome?

. . .

If we assume the within-individual errors are independent, then the variance of a single outcome measure is:
$$\begin{align*}Var[Y_{ij}] &= Var[\boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_i\vec{b}_{i}+\epsilon_{ij}]\\
&= Var[\boldsymbol{Z}_i \vec{b}_{i}+\epsilon_{ij}]\\
&= Var[\vec{Z}_{1i} b_{0i} + t_{ij} b_{1i} +\epsilon_{ij}]\\
&= Var[\vec{Z}_{1i} b_{0i}] + Var[t_{ij} b_{1i}] + 2Cov[\vec{Z}_{1i} b_{0i}, t_{ij} b_{1i}]+Var[\epsilon_{ij}]\\
&= \sigma^2_{b0} + t^2_{ij}\sigma^2_{b1}+ 2t_{ij}\sigma_{b0,b1} + \sigma^2_{\epsilon}\end{align*}$$

---

## Variance/covariance of the outcome

What does this mean for the covariance matrix of the outcome?

If we assume the within-individual errors are independent, then the variance of a single outcome measure is:
$$Var[Y_{ij}] = \sigma^2_{b0} + t^2_{ij}\sigma^2_{b1}+ 2t_{ij}\sigma_{b0,b1} + \sigma^2_{\epsilon}$$

. . .

And the covariance between 2 outcome measures in the same cluster is:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= Cov[\boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_i\vec{b}_{i}+\epsilon_{ij}, \boldsymbol{X}_i\vec{\beta} + \boldsymbol{Z}_i\vec{b}_{i}+\epsilon_{ik}]\\
&= Cov[\vec{Z}_{1i}b_{0i} +t_{ij}b_{1i}+\epsilon_{ij}, \vec{Z}_{1i}b_{0i}+t_{ik}b_{1i}+\epsilon_{ik}]\\
&= Cov[\vec{Z}_{1i}b_{0i}, \vec{Z}_{1i}b_{0i}] + Cov[\vec{Z}_{1i}b_{0i}, t_{ik}b_{01}] + Cov[\vec{Z}_{i1}b_{0i}, \epsilon_{ik}] + Cov[t_{ij}b_{1i}, \vec{Z}_{1i}b_{0i}]\\
&~~~~~~+ Cov[t_{ij}b_{1i}, t_{ik}b_{1i}] + Cov[t_{ij}b_{1i}, \epsilon_{ik}] + Cov[\epsilon_{ij}, \vec{Z}_{1i}b_{0i}] + Cov[\epsilon_{ij}, t_{ik}b_{1i}] + Cov[\epsilon_{ij}, \epsilon_{ik}]\\
&= \sigma^2_{b0} + t_{ik}\sigma_{b0,b1} + 0 + t_{ij}\sigma_{b0,b1} + t_{ij}t_{ik}\sigma^2_{b1} + 0 + 0 + 0 + 0\\
&= \sigma^2_{b0} + (t_{ij} + t_{ik})\sigma_{b0,b1} + t_{ij}t_{ik}\sigma^2_{b1}\end{align*}$$

---

## Variance/covariance of the outcome

What does this mean for the covariance matrix of the outcome?

If we assume the within-individual errors are independent, then the variance of a single outcome measure is:
$$Var[Y_{ij}] = \sigma^2_{b0} + t^2_{ij}\sigma^2_{b1}+ 2t_{ij}\sigma_{b0,b1} + \sigma^2_{\epsilon}$$

And the covariance between 2 outcome measures in the same cluster is:
$$Cov[Y_{ij}, Y_{ik}] = \sigma^2_{b0} + (t_{ij} + t_{ik})\sigma_{b0,b1} + t_{ij}t_{ik}\sigma^2_{b1}$$

. . .

[The variance and covariance are both functions of time!]{.alert}

- If $\sigma_{b0,b1}=0$, then we know $Var[Y_{ij}]$ increases with time

. . .

In general, [any component of $\beta$]{.alert} can be allowed to vary randomly by simply including the corresponding covariate in $\vec{Z}_{ij}$

---

## Random effects & correlation structures

When we just had random intercepts it was very clear how this created an exchangeable correlation/covariance matrix

. . .

[Do we get a similar result when we add random slopes?]{.alert}

. . .

- Kind of

. . .

Adding random slopes induces something like an [exponential]{.alert} correlation structure

- Correlation between observations within an individual gets smaller the further apart in time the observations are

Let's see this in action with an example

---

## Example: Exercise therapy trial

Recall the exercise therapy trial from last week:

- Subjects were assigned to one of two weightlifting programs to increase muscle strength

   - Treatment 1: number of repetitions of the exercises was increased as subjects became stronger

   - Treatment 2: number of repetitions was held constant but amount of weight was increased as subjects became stronger

- Measurements of body strength were taken at baseline (0) and on days 2, 4, 6, 8, 10, and 12

[What would our model look like?]{.alert}

. . .

$$\text{body strength}_{ij} = \beta_0 + \beta_1(\text{Program}_i) + \beta_2t_{ij} + \beta_3(\text{Program}_i \times t_{ij}) + b_{0i}(\text{ID}_i) + b_{1i}t_{ij} + \epsilon_{ij}$$

$$\vec{b}_i \sim MVN(\vec{0}, \boldsymbol{G})$$
$$\vec{\epsilon}_i \sim (\vec{0}, \sigma^2_\epsilon\boldsymbol{I})$$

---

## Example: Exercise therapy trial

If we run the mixed model...

```{r exercise lme}
library(lme4)

exercise_lme <- lmer( strength ~ PROGRAM*time + (time | ID),
                      data = exercise_long )
summary(exercise_lme)
```

---

## Example: Exercise therapy trial

We can also get estimates of the random effects covariance matrix:
```{r exercise rand cov}
VarCorr(exercise_lme)[["ID"]]
```

And we can estimate the variance of an outcome at a particular time:
$$\begin{align*}Var[Y_{ij}] &= \sigma^2_{b0} + t^2_{ij}\sigma^2_{b1}+ 2t_{ij}\sigma_{b0,b1} + \sigma^2_{\epsilon}\\
&= 9.953 + t^2_{ij}0.034 + 2(t_{ij})(-0.017) + \sigma^2_\epsilon\end{align*}$$

---

## Example: Exercise therapy trial

We can also get estimates of the random effects covariance matrix:
```{r exercise rand cov2}
VarCorr(exercise_lme)[["ID"]]
```

And the covariance between outcomes at different times:

:::: {.columns}
::: {.column width="50%}
Between day 0 and day 2:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= \sigma^2_{b0} + (t_{ij} + t_{ik})\sigma_{b0,b1} + t_{ij}t_{ik}\sigma^2_{b1}\\
&= 9.953+(0+2)(-0.017) + (0)(2)0.034\\
&= 9.919\end{align*}$$
:::

::: {.column width="50%"}
Between day 0 and day 8:
$$\begin{align*}Cov[Y_{ij}, Y_{ik}] &= \sigma^2_{b0} + (t_{ij} + t_{ik})\sigma_{b0,b1} + t_{ij}t_{ik}\sigma^2_{b1}\\
&= 9.953+(0+8)(-0.017) + (0)(8)0.034\\
&= 9.817\end{align*}$$
:::
::::

. . .

[Decreasing with increased distance between time points!]{.alert}

---

## Structures on $\boldsymbol{R_i}$ in GLMMs?

Up until now we've assumed that the $\epsilon_{ij}$ errors are independent within an individual: $\sigma^2_\epsilon \boldsymbol{I}$

- Allows us to interpret $\epsilon_{ij}$ as [sampling/measurement error]{.alert}

- This assumes [constant measurement error]{.alert} across time

- This is the most common type of structure to assume with mixed models

. . .

[Can we assume a non-independent structure for $\boldsymbol{R_i}$?]{.alert}

. . .

- Technically, yes...

. . .

We tend not to for 3 reasons:

1. In doing so, we lose the interpretation of $\epsilon_{ij}$ as sampling/measurement error

2. You might run into estimation issues if you try to estimate a full $\boldsymbol{G}$ (intercepts and slopes) and non-diagonal $\boldsymbol{R_i}$ without enough information in the data

3. Often full random effects (intercepts and slopes) and regular, diagonal measurement error ($\boldsymbol{R_i} = \sigma^2_\epsilon \boldsymbol{I}$) together take care of any time-variation in correlation

---

## Structures on $\boldsymbol{R_i}$ in GLMMs?

If we *do* want to use a non-independent $\boldsymbol{R_i}$, we should [only fit random intercepts]{.alert}

- This is sometimes called a [hybrid random effects]{.alert} model

. . .

Let $C_i(t_{ij})$ be our [serial correlation]{.alert} parameter. Then we can specify the model as:

$$Y_{ij} = \beta_0 + \beta_1 U_{i0} + \beta_3 t_{ij} + b_{0i}Z_i + C_i(t_{ij}) + \epsilon_{ij}$$

- Random intercepts: $b_{0i} \sim N(0, \sigma^2_0)$

- Serial correlation: $C_i(t_{ij}) \sim N(0, \sigma^2_c); ~~ Corr[C_i(t_{ij}), C_i(t_{ik})] = \rho(t_{ij}, t_{ik})$

   - Usually we use an exponential correlation function: $\rho(t_{ij}, t_{ik}) = \exp\{-|t_{ij} - t_{ik}|\}$

- Measurement error: $\epsilon_{ij} \sim (0, \sigma^2_\epsilon)$

---

## Example: Exercise therapy trial

Let's try this with the exercise trial data

```{r exercise hybrid, output=F}
library(nlme)
exercise_hybrid <- lme( strength ~ PROGRAM*time,
                        random = ~1 | ID,
                        correlation = corExp( form = ~ time | ID, nugget=TRUE),
                      data = exercise_long_complete )
summary(exercise_hybrid)
```
---

## Example: Exercise therapy trial
:::: {.columns}
::: {.column width="50%"}
![](hybrid-out.png){width=80%}
:::

::: {.column width="50%"}
Interpreting the output:

- `(random) SD`: square-root of variance associated with random intercept ($\sqrt{\sigma^2_0}$)

- `Residual SD`: square root of $\sigma^2_0 + \sigma^2_\epsilon$

- `range`: correlation between observations one time unit apart

- `nugget`: if this is 0, there's no measurement error
:::
::::

---

## Example: Exercise therapy trial

Comparing hybrid and random slopes models:

:::: {.columns}
::: {.column width="50%"}
```{r hybrid 2}
summary(exercise_hybrid)$tTable[,1:4]
```
:::

::: {.column width="50%"}
```{r slopes 2}
summary(exercise_lme)$coef
```
:::
::::

::: {.fragment fragment-index=1}
$$E[\text{body strength}_{ij} | \text{Program}_{ij}, t_{ij}] = \beta_0 + \beta_1(\text{Program}_i) + \beta_2t_{ij} + \beta_3(\text{Program}_i \times t_{ij})$$

- $\beta_0$: [Average strength of Program 1 at baseline]{.fragment fragment-index=3}

- $\beta_1$: [Difference in avg. strength between Program 2 and Program 1 at baseline]{.fragment fragment-index=3}

- $\beta_2$: [Rate of strength change in Program 1 between 2 adjacent timepoints]{.fragment fragment-index=3}

- $(\beta_2 + \beta_3)$: [Rate of strength change in Program 2 between 2 adjacent timepoints]{.fragment fragment-index=3}

- $(\beta_1 + \beta_3)$: [Difference in strength between Program 2 and Program 1 at the same timepoint]{.fragment fragment-index=3}

- $\beta_3$: [Difference in Program 1's strength change rate and Program 2's strength change rate]{.fragment fragment-index=3}
:::
