---
format: 
   revealjs:
      theme: ["../theme/q-theme.scss"]
      slide-number: c/t
      logo: POPUHEALSMPH_color-flush.png
      code-copy: true
      center-title-slide: false
      code-link: true
      code-overflow: wrap
      highlight-style: a11y
      height: 1080
      width: 1920
      chalkboard: true
      from: markdown+emoji
      fragment: true
      auto-stretch: false
execute: 
   eval: true
   echo: true
editor: 
  markdown: 
    wrap: 72
---

```{r load libraries, echo=F}
library(tidyverse)
library(ggdag)
library(dagitty)
library(gridExtra)
library(qrcode)
library(corrr)
```

<h1>Lecture 9.2: Comparing longitudinal models & other diganostics</h1>

<h2>PHS 651: Advanced regression methods</h2>

<hr>

<h3>Mary Ryan Baumann, PhD</h3>

<h3>November 7, 2024</h3>

<br>

::: {style="font-size: 75%;"}
**Recording disclosure**

*This class is being conducted in person, as well as over [Zoom]{.alert}. As the instructor, I will be [recording]{.alert} this session. I have disabled the recording feature for others so that no one else will be able to record this session. I will be posting this session to the course’s website.*

*If you have privacy concerns and do not wish to appear in the recording, you may turn video off (click “stop video”) so that Zoom does not record you.*

*The chat box is always open for discussion and questions to the entire class. You may also send messages privately to the instructor. Please note that Zoom saves all chat transcripts.*
:::

::: {.absolute bottom=50 left=260 width=1500}

Slides found at: <https://maryryan.github.io/PHS-651-slides/PHS-651-9-2/slides-9-2>

:::

::: {.absolute bottom=0 left=100 width=150}
```{r qr code, echo=F, fig.height=2, fig.width=2}
plot(qr_code("https://maryryan.github.io/PHS-651-slides/PHS-651-9-2/slides-9-2"))
```
:::

---

## Recall: Building a longitudinal model

Over the last few weeks we've also touched on how we might explore the data to make some initial modeling decisions

- Should we include a time-by-exposure interaction in the mean model?

   - First, ask if this is part of our research question
   
      - If yes, should include it regardless
   
   - Second, plot outcome over time by exposure group to look for evidence of non-parallel trends

- What correlation structures might be appropriate for a GEE?

   - Calculate a matrix of the raw correlations between outcomes and different time points and observe behavior

- If we're running a GLMM, should we include random slopes/serial correlation?

   - Calculate correlation matrix
   
   - Create "spaghetti plot" of a random sample of individuals and observe differences in trends
   
---

## Recall: Building longitudinal model(s?)

However, even after we perform these exploratory steps, we might still be left with multiple plausible model specifications

- Is there a way to compare multiple model specifications or assess the appropriateness of our modeling decisions?

. . .

[Yes!]{.alert}

. . .

There are 2 general approaches to compare GLMMs: likelihood ratios tests and AIC

- We can't use these for GEEs because GEEs don't have a likelihood

. . .

For GEEs, we can use a modification of AIC called QIC

---

## Model comparison via likelihood ratio test

One way to compare multiple models is via a [likelihood ratio test]{.alert} (LRT)

. . .

- An LRT is obtained by taking twice the difference in the respective maximized REML log-likelihoods,
$$G^2 = 2(\hat{l}_{full} - \hat{l}_{reduced})$$
and comparing statistic to a chi-squared distribution with degrees-of-freedom equal to difference between the number of covariance parameters in full and reduced models

   - "Full" here would be the most complicated covariance structure, while "reduced" would be a simpler one
   
   - Sometimes it's also written as $G^2 = -2(\hat{l}_{reduced} - \hat{l}_full)$

. . .

- This tests the hypothesis:

$$\begin{align*}&H_0: \text{There is no difference on how the full and reduced models fit the data}\\
&H_A: \text{There is a difference on how the full and reduced models fit the data}\end{align*}$$

   - This assumes that the "full" or more complicated model is always the best-fitting

---

## Model comparison via likelihood ratio test

One way to compare multiple models is via a [likelihood ratio test]{.alert} (LRT)

- An LRT is obtained by taking twice the difference in the respective maximized REML log-likelihoods,
$$G^2 = 2(\hat{l}_{full} - \hat{l}_{reduced})$$
and comparing statistic to a chi-squared distribution with degrees-of-freedom equal to difference between the number of covariance parameters in full and reduced models

   - "Full" here would be the most complicated covariance structure, while "reduced" would be a simpler one
   
   - Sometimes it's also written as $G^2 = -2(\hat{l}_{reduced} - \hat{l}_full)$

For GLMMs, this means comparing models with [the same fixed effects]{.alert} but "nested" random effects

- i.e., random intercepts vs random slopes + random intercepts

---

## Model comparison via AIC

An alternative approach is the [Akaike Information Criterion (AIC)]{.alert}

- According to the AIC, given a set of competing models for the covariance, one should select the model that [minimizes]{.alert}:

$$AIC = -2(\text{maximized log-likelihood}) + 2(\text{number of covariance parameters})$$

. . .

According to AIC, we want a model that fits the data well, but not at the price of having to estimate a ton of extra covariance parameters

. . .

Let's try this with the exercise therapy trial

---

## Example: Exercise therapy trial

We'll fit 2 models: one with random slopes + random intercepts, and one with just random intercepts

- Then we'll use the `anova()` function to both perform a LRT and calculate the AICs for each

. . .

```{r glmm compare}
exercise_lme_slopes <- lmer( strength ~ PROGRAM*time + (time | ID),
                      data = exercise_long )

exercise_lme_int <- lmer( strength ~ PROGRAM*time + (1 | ID),
                      data = exercise_long )

anova(exercise_lme_slopes, exercise_lme_int)
```

. . .

[What can we conclude?]{.alert}

---

## Model comparison via QIC

Neither LRTs nor AIC works for GEEs because GEEs do not have a likelihood that is needed to calculate those statistics

- Instead, we can use a modified AIC called the [Quasi-likelihood Information Criterion (QIC)]{.alert}

   - Its calculation is somewhat complicated, but it's interpretation is similar to the AIC: you want to minimize it

. . .

Let's test this in the exercise trial

---

## Example: Exercise therapy trial

We've been using `gee()` from `library(gee)` to fit GEEs so far

However, `QIC()` is only available for models run with the `geeglm()` function from `library(geepack)`

. . .

```{r gee compare, output=F}
library(geepack)

# fitting an unstructured correlation structure #
exercise_gee_un <- geeglm(strength ~ PROGRAM*time, id=ID,
                    data=exercise_long, corstr="unstructured")

# fitting an autoregression, distance 1 correlation structure #
exercise_gee_AR <- geeglm(strength ~ PROGRAM*time, id=ID,
                    data=exercise_long, corstr="ar1")

# fitting an exchangeable correlation structure #
exercise_gee_exch <- geeglm(strength ~ PROGRAM*time, id=ID,
                    data=exercise_long, corstr="exchangeable")

```

```{r gee compare2}
QIC(exercise_gee_un, exercise_gee_AR, exercise_gee_exch)
```

. . .


[What can we conclude?]{.alert}

---

## Example: Exercise therapy trial

---

## Model justification

We can use LRTs and AIC/QIC to help us make a decision between two competing models

- We've interally justified both to ourselves and just need to make a decision

They can also help us justify a modeling decision we've already made

- Want to show the model we've picked is just as good or better than another specification someone else might suggest

While the mechanics of both of these are the same, the motivations are different
